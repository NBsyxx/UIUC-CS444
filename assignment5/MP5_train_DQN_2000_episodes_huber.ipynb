{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning - Train a DQN Model Mean Evaluation: 7.89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
    "# %pip install -U tf-agents pyvirtualdisplay\n",
    "# %pip install -U gym[box2d,atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U gym>=0.21.0\n",
    "# !pip3 install --upgrade setuptools --user\n",
    "# !pip3 install ez_setup \n",
    "# !pip3 install -U gym[box2d,atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init agent, action size: 3\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "double_dqn = True# set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_training = False\n",
    "\n",
    "if continue_training == False:\n",
    "    rewards, episodes = [], []\n",
    "    best_eval_reward = 0\n",
    "    start = 0\n",
    "else:\n",
    "    # load training params\n",
    "#     agent = joblib.load(\"prev_agent\")\n",
    "#     params = joblib.load(\"prev_params\")\n",
    "    # load training model\n",
    "    agent.policy_net.load_state_dict(torch.load('./save_model/breakout_DQN_2021_episodes.pth', map_location=torch.device('cpu')))\n",
    "#     agent.memory = params[\"agent_memory\"]\n",
    "#     agent.epsilon = params[\"epsilon\"]\n",
    "#     rewards = params[\"rewards\"]\n",
    "#     episodes = params[\"episodes\"]\n",
    "#     best_eval_reward = params[\"best_eval_reward\"]\n",
    "#     start = params[\"episodes\"][-1]  \n",
    "    start = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 3.0   memory length: 228   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 1   score: 1.0   memory length: 380   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 2   score: 5.0   memory length: 701   epsilon: 1.0    steps: 321    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 3   score: 1.0   memory length: 871   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 4   score: 1.0   memory length: 1042   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 5   score: 0.0   memory length: 1165   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8333333333333333\n",
      "episode: 6   score: 1.0   memory length: 1334   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
      "episode: 7   score: 2.0   memory length: 1533   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 8   score: 0.0   memory length: 1656   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5555555555555556\n",
      "episode: 9   score: 3.0   memory length: 1906   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 10   score: 1.0   memory length: 2058   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 11   score: 1.0   memory length: 2230   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.5833333333333333\n",
      "episode: 12   score: 1.0   memory length: 2400   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5384615384615385\n",
      "episode: 13   score: 5.0   memory length: 2725   epsilon: 1.0    steps: 325    lr: 0.0001     evaluation reward: 1.7857142857142858\n",
      "episode: 14   score: 0.0   memory length: 2849   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 15   score: 1.0   memory length: 3020   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 16   score: 1.0   memory length: 3190   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.588235294117647\n",
      "episode: 17   score: 0.0   memory length: 3313   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 18   score: 2.0   memory length: 3532   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.5263157894736843\n",
      "episode: 19   score: 4.0   memory length: 3826   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 20   score: 1.0   memory length: 3996   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.619047619047619\n",
      "episode: 21   score: 0.0   memory length: 4120   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5454545454545454\n",
      "episode: 22   score: 4.0   memory length: 4415   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.6521739130434783\n",
      "episode: 23   score: 0.0   memory length: 4538   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5833333333333333\n",
      "episode: 24   score: 1.0   memory length: 4709   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 25   score: 3.0   memory length: 4954   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.6153846153846154\n",
      "episode: 26   score: 2.0   memory length: 5152   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6296296296296295\n",
      "episode: 27   score: 0.0   memory length: 5276   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
      "episode: 28   score: 1.0   memory length: 5446   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5517241379310345\n",
      "episode: 29   score: 0.0   memory length: 5569   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 30   score: 1.0   memory length: 5721   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.4838709677419355\n",
      "episode: 31   score: 2.0   memory length: 5920   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 32   score: 1.0   memory length: 6072   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.4848484848484849\n",
      "episode: 33   score: 1.0   memory length: 6242   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4705882352941178\n",
      "episode: 34   score: 1.0   memory length: 6413   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.457142857142857\n",
      "episode: 35   score: 0.0   memory length: 6536   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4166666666666667\n",
      "episode: 36   score: 2.0   memory length: 6718   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.4324324324324325\n",
      "episode: 37   score: 1.0   memory length: 6888   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4210526315789473\n",
      "episode: 38   score: 0.0   memory length: 7011   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3846153846153846\n",
      "episode: 39   score: 1.0   memory length: 7163   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 40   score: 2.0   memory length: 7382   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.3902439024390243\n",
      "episode: 41   score: 3.0   memory length: 7615   epsilon: 1.0    steps: 233    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 42   score: 2.0   memory length: 7814   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.441860465116279\n",
      "episode: 43   score: 3.0   memory length: 8063   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.4772727272727273\n",
      "episode: 44   score: 0.0   memory length: 8186   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
      "episode: 45   score: 0.0   memory length: 8310   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4130434782608696\n",
      "episode: 46   score: 0.0   memory length: 8433   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3829787234042554\n",
      "episode: 47   score: 3.0   memory length: 8660   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.4166666666666667\n",
      "episode: 48   score: 0.0   memory length: 8783   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3877551020408163\n",
      "episode: 49   score: 2.0   memory length: 8981   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 50   score: 2.0   memory length: 9185   epsilon: 1.0    steps: 204    lr: 0.0001     evaluation reward: 1.411764705882353\n",
      "episode: 51   score: 1.0   memory length: 9358   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.4038461538461537\n",
      "episode: 52   score: 2.0   memory length: 9557   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4150943396226414\n",
      "episode: 53   score: 2.0   memory length: 9755   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4259259259259258\n",
      "episode: 54   score: 0.0   memory length: 9879   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 55   score: 2.0   memory length: 10098   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.4107142857142858\n",
      "episode: 56   score: 1.0   memory length: 10269   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.4035087719298245\n",
      "episode: 57   score: 0.0   memory length: 10393   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3793103448275863\n",
      "episode: 58   score: 4.0   memory length: 10689   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.423728813559322\n",
      "episode: 59   score: 0.0   memory length: 10812   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 60   score: 4.0   memory length: 11101   epsilon: 1.0    steps: 289    lr: 0.0001     evaluation reward: 1.4426229508196722\n",
      "episode: 61   score: 0.0   memory length: 11225   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4193548387096775\n",
      "episode: 62   score: 2.0   memory length: 11424   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 63   score: 0.0   memory length: 11547   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.40625\n",
      "episode: 64   score: 5.0   memory length: 11892   epsilon: 1.0    steps: 345    lr: 0.0001     evaluation reward: 1.4615384615384615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 65   score: 2.0   memory length: 12090   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4696969696969697\n",
      "episode: 66   score: 0.0   memory length: 12213   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4477611940298507\n",
      "episode: 67   score: 2.0   memory length: 12414   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.4558823529411764\n",
      "episode: 68   score: 5.0   memory length: 12738   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.5072463768115942\n",
      "episode: 69   score: 2.0   memory length: 12937   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5142857142857142\n",
      "episode: 70   score: 2.0   memory length: 13138   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.5211267605633803\n",
      "episode: 71   score: 1.0   memory length: 13308   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5138888888888888\n",
      "episode: 72   score: 2.0   memory length: 13507   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5205479452054795\n",
      "episode: 73   score: 0.0   memory length: 13631   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 74   score: 3.0   memory length: 13880   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 75   score: 2.0   memory length: 14083   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.5263157894736843\n",
      "episode: 76   score: 1.0   memory length: 14234   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5194805194805194\n",
      "episode: 77   score: 4.0   memory length: 14512   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.5512820512820513\n",
      "episode: 78   score: 3.0   memory length: 14758   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.5696202531645569\n",
      "episode: 79   score: 0.0   memory length: 14882   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 80   score: 0.0   memory length: 15006   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5308641975308641\n",
      "episode: 81   score: 4.0   memory length: 15295   epsilon: 1.0    steps: 289    lr: 0.0001     evaluation reward: 1.5609756097560976\n",
      "episode: 82   score: 2.0   memory length: 15515   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.5662650602409638\n",
      "episode: 83   score: 1.0   memory length: 15685   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5595238095238095\n",
      "episode: 84   score: 2.0   memory length: 15884   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5647058823529412\n",
      "episode: 85   score: 3.0   memory length: 16151   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.5813953488372092\n",
      "episode: 86   score: 0.0   memory length: 16274   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5632183908045978\n",
      "episode: 87   score: 1.0   memory length: 16426   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5568181818181819\n",
      "episode: 88   score: 1.0   memory length: 16598   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.550561797752809\n",
      "episode: 89   score: 2.0   memory length: 16797   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5555555555555556\n",
      "episode: 90   score: 2.0   memory length: 16996   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5604395604395604\n",
      "episode: 91   score: 2.0   memory length: 17215   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.565217391304348\n",
      "episode: 92   score: 2.0   memory length: 17435   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.5698924731182795\n",
      "episode: 93   score: 3.0   memory length: 17662   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.5851063829787233\n",
      "episode: 94   score: 2.0   memory length: 17849   epsilon: 1.0    steps: 187    lr: 0.0001     evaluation reward: 1.5894736842105264\n",
      "episode: 95   score: 2.0   memory length: 18032   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.59375\n",
      "episode: 96   score: 0.0   memory length: 18156   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.577319587628866\n",
      "episode: 97   score: 4.0   memory length: 18472   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.6020408163265305\n",
      "episode: 98   score: 3.0   memory length: 18738   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.6161616161616161\n",
      "episode: 99   score: 1.0   memory length: 18911   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 100   score: 1.0   memory length: 19080   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 101   score: 1.0   memory length: 19251   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 102   score: 2.0   memory length: 19449   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 103   score: 1.0   memory length: 19601   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 104   score: 2.0   memory length: 19782   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 105   score: 1.0   memory length: 19933   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 106   score: 0.0   memory length: 20057   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 107   score: 3.0   memory length: 20269   epsilon: 1.0    steps: 212    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 108   score: 1.0   memory length: 20422   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 109   score: 0.0   memory length: 20545   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 110   score: 2.0   memory length: 20746   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 111   score: 2.0   memory length: 20945   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 112   score: 2.0   memory length: 21143   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 113   score: 2.0   memory length: 21342   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 114   score: 0.0   memory length: 21466   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 115   score: 3.0   memory length: 21713   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 116   score: 0.0   memory length: 21836   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 117   score: 2.0   memory length: 22035   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 118   score: 1.0   memory length: 22204   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 119   score: 2.0   memory length: 22402   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 120   score: 1.0   memory length: 22571   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 121   score: 1.0   memory length: 22740   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 122   score: 3.0   memory length: 22966   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 123   score: 2.0   memory length: 23147   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 124   score: 0.0   memory length: 23271   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 125   score: 2.0   memory length: 23490   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 126   score: 0.0   memory length: 23614   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 127   score: 1.0   memory length: 23784   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 128   score: 0.0   memory length: 23907   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 129   score: 0.0   memory length: 24031   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 130   score: 0.0   memory length: 24155   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 131   score: 2.0   memory length: 24376   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 132   score: 0.0   memory length: 24500   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 133   score: 0.0   memory length: 24624   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 134   score: 0.0   memory length: 24747   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 135   score: 3.0   memory length: 24976   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 136   score: 0.0   memory length: 25100   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 137   score: 1.0   memory length: 25252   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 138   score: 2.0   memory length: 25452   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 139   score: 3.0   memory length: 25699   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 140   score: 0.0   memory length: 25823   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 141   score: 0.0   memory length: 25947   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 142   score: 2.0   memory length: 26166   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 143   score: 3.0   memory length: 26395   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 144   score: 1.0   memory length: 26565   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 145   score: 2.0   memory length: 26764   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 146   score: 2.0   memory length: 26984   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 147   score: 1.0   memory length: 27138   epsilon: 1.0    steps: 154    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 148   score: 0.0   memory length: 27262   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 149   score: 2.0   memory length: 27460   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 150   score: 2.0   memory length: 27676   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 151   score: 2.0   memory length: 27892   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 152   score: 2.0   memory length: 28113   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 153   score: 0.0   memory length: 28237   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 154   score: 4.0   memory length: 28496   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 155   score: 8.0   memory length: 28853   epsilon: 1.0    steps: 357    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 156   score: 1.0   memory length: 29022   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 157   score: 2.0   memory length: 29220   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 158   score: 1.0   memory length: 29390   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 159   score: 5.0   memory length: 29713   epsilon: 1.0    steps: 323    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 160   score: 1.0   memory length: 29882   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 161   score: 1.0   memory length: 30033   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 162   score: 4.0   memory length: 30310   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 163   score: 3.0   memory length: 30537   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 164   score: 4.0   memory length: 30834   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 165   score: 3.0   memory length: 31083   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 166   score: 1.0   memory length: 31255   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 167   score: 1.0   memory length: 31428   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 168   score: 1.0   memory length: 31580   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 169   score: 0.0   memory length: 31703   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 170   score: 0.0   memory length: 31827   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 171   score: 0.0   memory length: 31950   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 172   score: 0.0   memory length: 32074   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 173   score: 1.0   memory length: 32243   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 174   score: 0.0   memory length: 32367   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 175   score: 1.0   memory length: 32518   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 176   score: 2.0   memory length: 32717   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 177   score: 0.0   memory length: 32841   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 178   score: 1.0   memory length: 33010   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 179   score: 4.0   memory length: 33330   epsilon: 1.0    steps: 320    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 180   score: 0.0   memory length: 33454   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 181   score: 1.0   memory length: 33624   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 182   score: 3.0   memory length: 33871   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 183   score: 0.0   memory length: 33994   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 184   score: 1.0   memory length: 34166   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 185   score: 1.0   memory length: 34336   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 186   score: 3.0   memory length: 34550   epsilon: 1.0    steps: 214    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 187   score: 0.0   memory length: 34674   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 188   score: 1.0   memory length: 34844   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 189   score: 1.0   memory length: 34996   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 190   score: 0.0   memory length: 35120   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 191   score: 1.0   memory length: 35272   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 192   score: 0.0   memory length: 35396   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 193   score: 2.0   memory length: 35614   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 194   score: 1.0   memory length: 35766   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 195   score: 2.0   memory length: 35984   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 196   score: 2.0   memory length: 36183   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 197   score: 2.0   memory length: 36382   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 198   score: 4.0   memory length: 36640   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 199   score: 3.0   memory length: 36909   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 200   score: 1.0   memory length: 37079   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 201   score: 0.0   memory length: 37203   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 202   score: 3.0   memory length: 37450   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 203   score: 2.0   memory length: 37669   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 204   score: 1.0   memory length: 37821   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 205   score: 1.0   memory length: 37990   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 206   score: 3.0   memory length: 38237   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 207   score: 1.0   memory length: 38407   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 208   score: 0.0   memory length: 38531   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 209   score: 2.0   memory length: 38730   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 210   score: 1.0   memory length: 38900   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 211   score: 2.0   memory length: 39120   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 212   score: 1.0   memory length: 39272   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 213   score: 2.0   memory length: 39471   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 214   score: 0.0   memory length: 39594   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 215   score: 2.0   memory length: 39813   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 216   score: 1.0   memory length: 39984   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 217   score: 1.0   memory length: 40136   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 218   score: 2.0   memory length: 40337   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 219   score: 3.0   memory length: 40585   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 220   score: 2.0   memory length: 40784   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 221   score: 1.0   memory length: 40953   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 222   score: 0.0   memory length: 41077   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 223   score: 0.0   memory length: 41201   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 224   score: 0.0   memory length: 41325   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 225   score: 1.0   memory length: 41495   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 226   score: 2.0   memory length: 41695   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 227   score: 0.0   memory length: 41818   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 228   score: 1.0   memory length: 41988   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 229   score: 1.0   memory length: 42139   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 230   score: 0.0   memory length: 42263   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 231   score: 2.0   memory length: 42484   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 232   score: 1.0   memory length: 42655   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 233   score: 5.0   memory length: 43002   epsilon: 1.0    steps: 347    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 234   score: 0.0   memory length: 43126   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 235   score: 4.0   memory length: 43403   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 236   score: 1.0   memory length: 43572   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 237   score: 3.0   memory length: 43837   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 238   score: 3.0   memory length: 44085   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 239   score: 0.0   memory length: 44209   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 240   score: 2.0   memory length: 44408   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 241   score: 2.0   memory length: 44606   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 242   score: 0.0   memory length: 44730   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 243   score: 1.0   memory length: 44901   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 244   score: 2.0   memory length: 45099   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 245   score: 3.0   memory length: 45345   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 246   score: 2.0   memory length: 45543   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 247   score: 0.0   memory length: 45667   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 248   score: 0.0   memory length: 45791   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 249   score: 3.0   memory length: 46018   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 250   score: 1.0   memory length: 46188   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 251   score: 2.0   memory length: 46386   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 252   score: 3.0   memory length: 46651   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 253   score: 1.0   memory length: 46821   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 254   score: 3.0   memory length: 47050   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 255   score: 0.0   memory length: 47174   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 256   score: 2.0   memory length: 47392   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 257   score: 0.0   memory length: 47515   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 258   score: 1.0   memory length: 47686   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 259   score: 2.0   memory length: 47885   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 260   score: 2.0   memory length: 48084   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 261   score: 3.0   memory length: 48316   epsilon: 1.0    steps: 232    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 262   score: 1.0   memory length: 48486   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 263   score: 0.0   memory length: 48610   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 264   score: 0.0   memory length: 48734   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 265   score: 2.0   memory length: 48933   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 266   score: 2.0   memory length: 49131   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 267   score: 2.0   memory length: 49330   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 268   score: 1.0   memory length: 49482   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 269   score: 2.0   memory length: 49663   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 270   score: 1.0   memory length: 49832   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 271   score: 2.0   memory length: 50048   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 272   score: 0.0   memory length: 50172   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 273   score: 2.0   memory length: 50352   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 274   score: 0.0   memory length: 50475   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 275   score: 0.0   memory length: 50599   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 276   score: 1.0   memory length: 50750   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 277   score: 3.0   memory length: 51015   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 278   score: 2.0   memory length: 51234   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 279   score: 0.0   memory length: 51358   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 280   score: 0.0   memory length: 51482   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 281   score: 1.0   memory length: 51653   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 282   score: 5.0   memory length: 51958   epsilon: 1.0    steps: 305    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 283   score: 2.0   memory length: 52156   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 284   score: 3.0   memory length: 52404   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 285   score: 2.0   memory length: 52603   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 286   score: 1.0   memory length: 52754   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 287   score: 0.0   memory length: 52878   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 288   score: 0.0   memory length: 53001   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 289   score: 2.0   memory length: 53184   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 290   score: 2.0   memory length: 53369   epsilon: 1.0    steps: 185    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 291   score: 1.0   memory length: 53539   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 292   score: 1.0   memory length: 53708   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 293   score: 3.0   memory length: 53957   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 294   score: 0.0   memory length: 54081   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 295   score: 2.0   memory length: 54280   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 296   score: 4.0   memory length: 54597   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 297   score: 2.0   memory length: 54795   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 298   score: 1.0   memory length: 54946   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 299   score: 2.0   memory length: 55145   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 300   score: 0.0   memory length: 55269   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 301   score: 2.0   memory length: 55487   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 302   score: 3.0   memory length: 55735   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 303   score: 2.0   memory length: 55935   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 304   score: 0.0   memory length: 56058   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 305   score: 0.0   memory length: 56182   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 306   score: 2.0   memory length: 56381   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 307   score: 0.0   memory length: 56504   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 308   score: 1.0   memory length: 56674   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 309   score: 2.0   memory length: 56872   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 310   score: 1.0   memory length: 57023   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 311   score: 2.0   memory length: 57245   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 312   score: 0.0   memory length: 57369   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 313   score: 3.0   memory length: 57599   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 314   score: 2.0   memory length: 57819   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 315   score: 2.0   memory length: 58018   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 316   score: 2.0   memory length: 58237   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 317   score: 1.0   memory length: 58407   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 318   score: 0.0   memory length: 58531   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 319   score: 4.0   memory length: 58826   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 320   score: 1.0   memory length: 58978   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 321   score: 2.0   memory length: 59177   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 322   score: 0.0   memory length: 59301   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 323   score: 1.0   memory length: 59472   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 324   score: 2.0   memory length: 59652   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 325   score: 1.0   memory length: 59824   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 326   score: 1.0   memory length: 59976   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 327   score: 3.0   memory length: 60244   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 328   score: 0.0   memory length: 60367   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 329   score: 2.0   memory length: 60566   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 330   score: 1.0   memory length: 60735   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 331   score: 0.0   memory length: 60859   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 332   score: 1.0   memory length: 61010   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 333   score: 2.0   memory length: 61229   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 334   score: 2.0   memory length: 61448   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 335   score: 3.0   memory length: 61678   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 336   score: 2.0   memory length: 61899   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 337   score: 0.0   memory length: 62022   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 338   score: 1.0   memory length: 62192   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 339   score: 0.0   memory length: 62315   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 340   score: 1.0   memory length: 62486   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 341   score: 0.0   memory length: 62610   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 342   score: 2.0   memory length: 62808   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 343   score: 0.0   memory length: 62932   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 344   score: 1.0   memory length: 63086   epsilon: 1.0    steps: 154    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 345   score: 0.0   memory length: 63210   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 346   score: 2.0   memory length: 63429   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 347   score: 2.0   memory length: 63628   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 348   score: 0.0   memory length: 63752   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 349   score: 0.0   memory length: 63876   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 350   score: 0.0   memory length: 63999   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 351   score: 1.0   memory length: 64171   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 352   score: 0.0   memory length: 64295   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 353   score: 0.0   memory length: 64419   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 354   score: 0.0   memory length: 64543   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 355   score: 3.0   memory length: 64812   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 356   score: 1.0   memory length: 64965   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 357   score: 2.0   memory length: 65183   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 358   score: 1.0   memory length: 65354   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 359   score: 0.0   memory length: 65478   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 360   score: 0.0   memory length: 65601   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 361   score: 1.0   memory length: 65771   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 362   score: 3.0   memory length: 66038   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 363   score: 3.0   memory length: 66265   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 364   score: 1.0   memory length: 66436   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 365   score: 0.0   memory length: 66560   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 366   score: 1.0   memory length: 66711   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 367   score: 0.0   memory length: 66834   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 368   score: 0.0   memory length: 66958   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 369   score: 3.0   memory length: 67188   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 370   score: 3.0   memory length: 67435   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 371   score: 1.0   memory length: 67608   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 372   score: 3.0   memory length: 67855   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 373   score: 0.0   memory length: 67978   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 374   score: 2.0   memory length: 68198   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 375   score: 0.0   memory length: 68321   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 376   score: 0.0   memory length: 68445   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 377   score: 0.0   memory length: 68568   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 378   score: 0.0   memory length: 68691   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 379   score: 3.0   memory length: 68960   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 380   score: 2.0   memory length: 69182   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 381   score: 0.0   memory length: 69306   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 382   score: 0.0   memory length: 69430   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 383   score: 1.0   memory length: 69581   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 384   score: 2.0   memory length: 69780   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 385   score: 2.0   memory length: 69979   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 386   score: 0.0   memory length: 70103   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 387   score: 2.0   memory length: 70286   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 388   score: 0.0   memory length: 70410   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 389   score: 2.0   memory length: 70609   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 390   score: 4.0   memory length: 70905   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 391   score: 2.0   memory length: 71125   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 392   score: 3.0   memory length: 71351   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 393   score: 2.0   memory length: 71550   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 394   score: 0.0   memory length: 71673   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 395   score: 0.0   memory length: 71797   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 396   score: 1.0   memory length: 71970   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 397   score: 4.0   memory length: 72252   epsilon: 1.0    steps: 282    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 398   score: 1.0   memory length: 72421   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 399   score: 7.0   memory length: 72723   epsilon: 1.0    steps: 302    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 400   score: 0.0   memory length: 72847   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 401   score: 1.0   memory length: 73019   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 402   score: 2.0   memory length: 73217   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 403   score: 1.0   memory length: 73387   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 404   score: 0.0   memory length: 73510   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 405   score: 2.0   memory length: 73709   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 406   score: 1.0   memory length: 73879   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 407   score: 2.0   memory length: 74062   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 408   score: 1.0   memory length: 74213   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 409   score: 4.0   memory length: 74489   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 410   score: 0.0   memory length: 74613   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 411   score: 4.0   memory length: 74928   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 412   score: 2.0   memory length: 75126   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 413   score: 2.0   memory length: 75324   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 414   score: 4.0   memory length: 75641   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 415   score: 1.0   memory length: 75812   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 416   score: 1.0   memory length: 75982   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 417   score: 6.0   memory length: 76360   epsilon: 1.0    steps: 378    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 418   score: 1.0   memory length: 76530   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 419   score: 2.0   memory length: 76729   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 420   score: 1.0   memory length: 76880   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 421   score: 2.0   memory length: 77099   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 422   score: 0.0   memory length: 77222   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 423   score: 5.0   memory length: 77526   epsilon: 1.0    steps: 304    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 424   score: 0.0   memory length: 77650   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 425   score: 0.0   memory length: 77773   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 426   score: 1.0   memory length: 77925   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 427   score: 0.0   memory length: 78049   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 428   score: 2.0   memory length: 78248   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 429   score: 0.0   memory length: 78372   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 430   score: 2.0   memory length: 78571   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 431   score: 0.0   memory length: 78695   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 432   score: 0.0   memory length: 78819   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 433   score: 0.0   memory length: 78942   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 434   score: 2.0   memory length: 79161   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 435   score: 0.0   memory length: 79285   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 436   score: 0.0   memory length: 79408   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 437   score: 0.0   memory length: 79532   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 438   score: 1.0   memory length: 79686   epsilon: 1.0    steps: 154    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 439   score: 1.0   memory length: 79856   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 440   score: 1.0   memory length: 80026   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 441   score: 3.0   memory length: 80272   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 442   score: 0.0   memory length: 80395   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 443   score: 1.0   memory length: 80565   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 444   score: 2.0   memory length: 80767   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 445   score: 0.0   memory length: 80890   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 446   score: 4.0   memory length: 81192   epsilon: 1.0    steps: 302    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 447   score: 1.0   memory length: 81344   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 448   score: 0.0   memory length: 81468   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 449   score: 3.0   memory length: 81715   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 450   score: 0.0   memory length: 81838   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 451   score: 1.0   memory length: 82008   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 452   score: 1.0   memory length: 82160   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 453   score: 1.0   memory length: 82329   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 454   score: 1.0   memory length: 82481   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 455   score: 2.0   memory length: 82681   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 456   score: 1.0   memory length: 82833   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 457   score: 3.0   memory length: 83081   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 458   score: 2.0   memory length: 83298   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 459   score: 2.0   memory length: 83517   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 460   score: 2.0   memory length: 83715   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 461   score: 0.0   memory length: 83839   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 462   score: 0.0   memory length: 83963   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 463   score: 1.0   memory length: 84135   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 464   score: 1.0   memory length: 84286   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 465   score: 0.0   memory length: 84410   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 466   score: 1.0   memory length: 84579   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 467   score: 2.0   memory length: 84779   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 468   score: 1.0   memory length: 84951   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 469   score: 2.0   memory length: 85150   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 470   score: 2.0   memory length: 85368   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 471   score: 2.0   memory length: 85567   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 472   score: 2.0   memory length: 85787   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 473   score: 1.0   memory length: 85957   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 474   score: 1.0   memory length: 86109   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 475   score: 0.0   memory length: 86232   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 476   score: 1.0   memory length: 86383   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 477   score: 0.0   memory length: 86507   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 478   score: 0.0   memory length: 86630   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 479   score: 4.0   memory length: 86899   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 480   score: 3.0   memory length: 87147   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 481   score: 1.0   memory length: 87299   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 482   score: 1.0   memory length: 87451   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 483   score: 0.0   memory length: 87574   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 484   score: 2.0   memory length: 87774   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 485   score: 0.0   memory length: 87897   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 486   score: 1.0   memory length: 88066   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 487   score: 2.0   memory length: 88285   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 488   score: 2.0   memory length: 88485   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 489   score: 3.0   memory length: 88712   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 490   score: 3.0   memory length: 88980   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 491   score: 1.0   memory length: 89151   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 492   score: 0.0   memory length: 89275   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 493   score: 3.0   memory length: 89504   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 494   score: 5.0   memory length: 89801   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 495   score: 1.0   memory length: 89974   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 496   score: 2.0   memory length: 90155   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 497   score: 1.0   memory length: 90324   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 498   score: 4.0   memory length: 90620   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 499   score: 2.0   memory length: 90839   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 500   score: 0.0   memory length: 90962   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 501   score: 4.0   memory length: 91261   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 502   score: 4.0   memory length: 91574   epsilon: 1.0    steps: 313    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 503   score: 1.0   memory length: 91746   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 504   score: 0.0   memory length: 91870   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 505   score: 2.0   memory length: 92068   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 506   score: 4.0   memory length: 92338   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 507   score: 1.0   memory length: 92508   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 508   score: 0.0   memory length: 92632   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 509   score: 0.0   memory length: 92755   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 510   score: 0.0   memory length: 92878   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 511   score: 1.0   memory length: 93049   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 512   score: 3.0   memory length: 93316   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 513   score: 0.0   memory length: 93440   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 514   score: 2.0   memory length: 93638   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 515   score: 3.0   memory length: 93902   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 516   score: 0.0   memory length: 94026   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 517   score: 2.0   memory length: 94243   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 518   score: 3.0   memory length: 94489   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 519   score: 3.0   memory length: 94757   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 520   score: 1.0   memory length: 94926   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 521   score: 1.0   memory length: 95098   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 522   score: 2.0   memory length: 95297   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 523   score: 1.0   memory length: 95467   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 524   score: 2.0   memory length: 95668   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 525   score: 2.0   memory length: 95850   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 526   score: 4.0   memory length: 96132   epsilon: 1.0    steps: 282    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 527   score: 2.0   memory length: 96350   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 528   score: 1.0   memory length: 96502   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 529   score: 0.0   memory length: 96626   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 530   score: 1.0   memory length: 96778   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 531   score: 2.0   memory length: 96997   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 532   score: 1.0   memory length: 97167   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 533   score: 2.0   memory length: 97388   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 534   score: 1.0   memory length: 97559   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 535   score: 1.0   memory length: 97730   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 536   score: 3.0   memory length: 97981   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 537   score: 1.0   memory length: 98151   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 538   score: 4.0   memory length: 98469   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 539   score: 4.0   memory length: 98746   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 540   score: 0.0   memory length: 98870   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 541   score: 0.0   memory length: 98994   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 542   score: 2.0   memory length: 99177   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 543   score: 1.0   memory length: 99329   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 544   score: 0.0   memory length: 99453   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 545   score: 2.0   memory length: 99655   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 546   score: 3.0   memory length: 99872   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nbsyxx\\Desktop\\assignment5\\assignment5_materials\\memory.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample = np.array(sample)\n",
      "C:\\Users\\Nbsyxx\\Desktop\\assignment5\\assignment5_materials\\agent.py:71: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  mini_batch = np.array(mini_batch).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 547   score: 2.0   memory length: 100000   epsilon: 0.9998574400000031    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 548   score: 3.0   memory length: 100000   epsilon: 0.9993683800000137    steps: 247    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 549   score: 2.0   memory length: 100000   epsilon: 0.9989268400000233    steps: 223    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 550   score: 3.0   memory length: 100000   epsilon: 0.9984021400000347    steps: 265    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 551   score: 0.0   memory length: 100000   epsilon: 0.99815662000004    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 552   score: 2.0   memory length: 100000   epsilon: 0.9977645800000485    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 553   score: 2.0   memory length: 100000   epsilon: 0.997372540000057    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 554   score: 0.0   memory length: 100000   epsilon: 0.9971290000000623    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 555   score: 0.0   memory length: 100000   epsilon: 0.9968834800000677    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 556   score: 3.0   memory length: 100000   epsilon: 0.9963964000000782    steps: 246    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 557   score: 1.0   memory length: 100000   epsilon: 0.9960617800000855    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 558   score: 0.0   memory length: 100000   epsilon: 0.9958162600000908    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 559   score: 0.0   memory length: 100000   epsilon: 0.9955707400000962    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 560   score: 0.0   memory length: 100000   epsilon: 0.9953272000001014    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 561   score: 2.0   memory length: 100000   epsilon: 0.99493516000011    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 562   score: 2.0   memory length: 100000   epsilon: 0.9945371800001186    steps: 201    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 563   score: 5.0   memory length: 100000   epsilon: 0.9938184400001342    steps: 363    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 564   score: 3.0   memory length: 100000   epsilon: 0.993321460000145    steps: 251    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 565   score: 1.0   memory length: 100000   epsilon: 0.9929809000001524    steps: 172    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 566   score: 0.0   memory length: 100000   epsilon: 0.9927353800001577    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 567   score: 0.0   memory length: 100000   epsilon: 0.992489860000163    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 568   score: 3.0   memory length: 100000   epsilon: 0.9920027800001736    steps: 246    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 569   score: 0.0   memory length: 100000   epsilon: 0.9917572600001789    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 570   score: 2.0   memory length: 100000   epsilon: 0.9913652200001875    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 571   score: 2.0   memory length: 100000   epsilon: 0.9909355600001968    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 572   score: 0.0   memory length: 100000   epsilon: 0.9906900400002021    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 573   score: 0.0   memory length: 100000   epsilon: 0.9904445200002074    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 574   score: 2.0   memory length: 100000   epsilon: 0.990052480000216    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 575   score: 1.0   memory length: 100000   epsilon: 0.9897119200002233    steps: 172    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 576   score: 4.0   memory length: 100000   epsilon: 0.9891535600002355    steps: 282    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 577   score: 1.0   memory length: 100000   epsilon: 0.9888169600002428    steps: 170    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 578   score: 3.0   memory length: 100000   epsilon: 0.9883694800002525    steps: 226    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 579   score: 0.0   memory length: 100000   epsilon: 0.9881239600002578    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 580   score: 2.0   memory length: 100000   epsilon: 0.9877299400002664    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 581   score: 0.0   memory length: 100000   epsilon: 0.9874844200002717    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 582   score: 2.0   memory length: 100000   epsilon: 0.9870904000002803    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 583   score: 0.0   memory length: 100000   epsilon: 0.9868448800002856    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 584   score: 3.0   memory length: 100000   epsilon: 0.9863102800002972    steps: 270    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 585   score: 0.0   memory length: 100000   epsilon: 0.9860647600003025    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 586   score: 2.0   memory length: 100000   epsilon: 0.9856687600003111    steps: 200    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 587   score: 2.0   memory length: 100000   epsilon: 0.9852351400003205    steps: 219    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 588   score: 4.0   memory length: 100000   epsilon: 0.9846847000003325    steps: 278    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 589   score: 2.0   memory length: 100000   epsilon: 0.9842491000003419    steps: 220    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 590   score: 0.0   memory length: 100000   epsilon: 0.9840055600003472    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 591   score: 2.0   memory length: 100000   epsilon: 0.9836115400003558    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 592   score: 1.0   memory length: 100000   epsilon: 0.9833086000003624    steps: 153    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 593   score: 5.0   memory length: 100000   epsilon: 0.9825918400003779    steps: 362    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 594   score: 2.0   memory length: 100000   epsilon: 0.9821582200003873    steps: 219    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 595   score: 0.0   memory length: 100000   epsilon: 0.9819146800003926    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 596   score: 1.0   memory length: 100000   epsilon: 0.9815721400004    steps: 173    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 597   score: 2.0   memory length: 100000   epsilon: 0.9811306000004096    steps: 223    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 598   score: 1.0   memory length: 100000   epsilon: 0.980792020000417    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 599   score: 3.0   memory length: 100000   epsilon: 0.9803069200004275    steps: 245    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 600   score: 2.0   memory length: 100000   epsilon: 0.979914880000436    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 601   score: 2.0   memory length: 100000   epsilon: 0.9795545200004439    steps: 182    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 602   score: 2.0   memory length: 100000   epsilon: 0.9791624800004524    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 603   score: 0.0   memory length: 100000   epsilon: 0.9789169600004577    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 604   score: 0.0   memory length: 100000   epsilon: 0.978671440000463    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 605   score: 5.0   memory length: 100000   epsilon: 0.9779923000004778    steps: 343    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 606   score: 0.0   memory length: 100000   epsilon: 0.9777467800004831    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 607   score: 1.0   memory length: 100000   epsilon: 0.9774101800004904    steps: 170    lr: 0.0001     evaluation reward: 1.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 608   score: 2.0   memory length: 100000   epsilon: 0.977016160000499    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 609   score: 2.0   memory length: 100000   epsilon: 0.9766597600005067    steps: 180    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 610   score: 1.0   memory length: 100000   epsilon: 0.9763588000005132    steps: 152    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 611   score: 3.0   memory length: 100000   epsilon: 0.975907360000523    steps: 228    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 612   score: 0.0   memory length: 100000   epsilon: 0.9756638200005283    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 613   score: 4.0   memory length: 100000   epsilon: 0.9751133800005403    steps: 278    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 614   score: 0.0   memory length: 100000   epsilon: 0.9748678600005456    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 615   score: 1.0   memory length: 100000   epsilon: 0.9745688800005521    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 616   score: 2.0   memory length: 100000   epsilon: 0.9741253600005617    steps: 224    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 617   score: 0.0   memory length: 100000   epsilon: 0.973879840000567    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 618   score: 1.0   memory length: 100000   epsilon: 0.9735808600005735    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 619   score: 2.0   memory length: 100000   epsilon: 0.9731531800005828    steps: 216    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 620   score: 3.0   memory length: 100000   epsilon: 0.9726641200005934    steps: 247    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 621   score: 3.0   memory length: 100000   epsilon: 0.9722483200006025    steps: 210    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 622   score: 0.0   memory length: 100000   epsilon: 0.9720028000006078    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 623   score: 0.0   memory length: 100000   epsilon: 0.9717572800006131    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 624   score: 0.0   memory length: 100000   epsilon: 0.9715137400006184    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 625   score: 0.0   memory length: 100000   epsilon: 0.9712702000006237    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 626   score: 1.0   memory length: 100000   epsilon: 0.970933600000631    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 627   score: 1.0   memory length: 100000   epsilon: 0.9705930400006384    steps: 172    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 628   score: 1.0   memory length: 100000   epsilon: 0.9702564400006457    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 629   score: 1.0   memory length: 100000   epsilon: 0.9699554800006522    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 630   score: 5.0   memory length: 100000   epsilon: 0.969412960000664    steps: 274    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 631   score: 2.0   memory length: 100000   epsilon: 0.9690189400006726    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 632   score: 2.0   memory length: 100000   epsilon: 0.9685912600006819    steps: 216    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 633   score: 3.0   memory length: 100000   epsilon: 0.9681418000006916    steps: 227    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 634   score: 0.0   memory length: 100000   epsilon: 0.9678962800006969    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 635   score: 0.0   memory length: 100000   epsilon: 0.9676527400007022    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 636   score: 1.0   memory length: 100000   epsilon: 0.9673161400007095    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 637   score: 1.0   memory length: 100000   epsilon: 0.966973600000717    steps: 173    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 638   score: 1.0   memory length: 100000   epsilon: 0.9666726400007235    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 639   score: 0.0   memory length: 100000   epsilon: 0.9664271200007288    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 640   score: 2.0   memory length: 100000   epsilon: 0.9659915200007383    steps: 220    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 641   score: 0.0   memory length: 100000   epsilon: 0.9657460000007436    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 642   score: 1.0   memory length: 100000   epsilon: 0.9654094000007509    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 643   score: 2.0   memory length: 100000   epsilon: 0.9650153800007595    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 644   score: 1.0   memory length: 100000   epsilon: 0.9646748200007669    steps: 172    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 645   score: 2.0   memory length: 100000   epsilon: 0.9642352600007764    steps: 222    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 646   score: 0.0   memory length: 100000   epsilon: 0.9639917200007817    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 647   score: 2.0   memory length: 100000   epsilon: 0.9635937400007903    steps: 201    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 648   score: 2.0   memory length: 100000   epsilon: 0.9631601200007998    steps: 219    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 649   score: 0.0   memory length: 100000   epsilon: 0.9629146000008051    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 650   score: 3.0   memory length: 100000   epsilon: 0.9624255400008157    steps: 247    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 651   score: 2.0   memory length: 100000   epsilon: 0.9620315200008243    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 652   score: 3.0   memory length: 100000   epsilon: 0.9616058200008335    steps: 215    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 653   score: 0.0   memory length: 100000   epsilon: 0.9613622800008388    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 654   score: 2.0   memory length: 100000   epsilon: 0.9610039000008466    steps: 181    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 655   score: 2.0   memory length: 100000   epsilon: 0.9606118600008551    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 656   score: 2.0   memory length: 100000   epsilon: 0.9601822000008644    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 657   score: 1.0   memory length: 100000   epsilon: 0.9598812400008709    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 658   score: 2.0   memory length: 100000   epsilon: 0.9594892000008794    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 659   score: 1.0   memory length: 100000   epsilon: 0.9591545800008867    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 660   score: 1.0   memory length: 100000   epsilon: 0.9588516400008933    steps: 153    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 661   score: 4.0   memory length: 100000   epsilon: 0.9582952600009054    steps: 281    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 662   score: 1.0   memory length: 100000   epsilon: 0.9579527200009128    steps: 173    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 663   score: 2.0   memory length: 100000   epsilon: 0.9575210800009222    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 664   score: 1.0   memory length: 100000   epsilon: 0.9572201200009287    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 665   score: 3.0   memory length: 100000   epsilon: 0.9567647200009386    steps: 230    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 666   score: 3.0   memory length: 100000   epsilon: 0.9562736800009493    steps: 248    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 667   score: 2.0   memory length: 100000   epsilon: 0.9558400600009587    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 668   score: 4.0   memory length: 100000   epsilon: 0.9552955600009705    steps: 275    lr: 0.0001     evaluation reward: 1.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 669   score: 0.0   memory length: 100000   epsilon: 0.9550500400009758    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 670   score: 2.0   memory length: 100000   epsilon: 0.9546560200009844    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 671   score: 0.0   memory length: 100000   epsilon: 0.9544124800009897    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 672   score: 3.0   memory length: 100000   epsilon: 0.9539491600009997    steps: 234    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 673   score: 2.0   memory length: 100000   epsilon: 0.953519500001009    steps: 217    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 674   score: 2.0   memory length: 100000   epsilon: 0.9530819200010185    steps: 221    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 675   score: 4.0   memory length: 100000   epsilon: 0.9525671200010297    steps: 260    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 676   score: 1.0   memory length: 100000   epsilon: 0.9522661600010363    steps: 152    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 677   score: 4.0   memory length: 100000   epsilon: 0.951680080001049    steps: 296    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 678   score: 2.0   memory length: 100000   epsilon: 0.9512860600010575    steps: 199    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 679   score: 0.0   memory length: 100000   epsilon: 0.9510405400010629    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 680   score: 5.0   memory length: 100000   epsilon: 0.9503594200010776    steps: 344    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 681   score: 2.0   memory length: 100000   epsilon: 0.9499654000010862    steps: 199    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 682   score: 2.0   memory length: 100000   epsilon: 0.9495713800010948    steps: 199    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 683   score: 1.0   memory length: 100000   epsilon: 0.9492724000011012    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 684   score: 0.0   memory length: 100000   epsilon: 0.9490288600011065    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 685   score: 2.0   memory length: 100000   epsilon: 0.9486308800011152    steps: 201    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 686   score: 0.0   memory length: 100000   epsilon: 0.9483853600011205    steps: 124    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 687   score: 2.0   memory length: 100000   epsilon: 0.9479913400011291    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 688   score: 0.0   memory length: 100000   epsilon: 0.9477458200011344    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 689   score: 1.0   memory length: 100000   epsilon: 0.9474468400011409    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 690   score: 2.0   memory length: 100000   epsilon: 0.9470884600011487    steps: 181    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 691   score: 2.0   memory length: 100000   epsilon: 0.946658800001158    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 692   score: 0.0   memory length: 100000   epsilon: 0.9464132800011633    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 693   score: 2.0   memory length: 100000   epsilon: 0.9460192600011719    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 694   score: 2.0   memory length: 100000   epsilon: 0.9455856400011813    steps: 219    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 695   score: 0.0   memory length: 100000   epsilon: 0.9453401200011866    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 696   score: 2.0   memory length: 100000   epsilon: 0.9449441200011952    steps: 200    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 697   score: 1.0   memory length: 100000   epsilon: 0.9446431600012017    steps: 152    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 698   score: 1.0   memory length: 100000   epsilon: 0.944308540001209    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 699   score: 1.0   memory length: 100000   epsilon: 0.9440095600012155    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 700   score: 1.0   memory length: 100000   epsilon: 0.9436749400012228    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 701   score: 1.0   memory length: 100000   epsilon: 0.9433383400012301    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 702   score: 6.0   memory length: 100000   epsilon: 0.9425542600012471    steps: 396    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 703   score: 2.0   memory length: 100000   epsilon: 0.9421127200012567    steps: 223    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 704   score: 1.0   memory length: 100000   epsilon: 0.9418137400012632    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 705   score: 1.0   memory length: 100000   epsilon: 0.9414771400012705    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 706   score: 0.0   memory length: 100000   epsilon: 0.9412316200012758    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 707   score: 0.0   memory length: 100000   epsilon: 0.9409880800012811    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 708   score: 0.0   memory length: 100000   epsilon: 0.9407425600012864    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 709   score: 0.0   memory length: 100000   epsilon: 0.9404970400012918    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 710   score: 3.0   memory length: 100000   epsilon: 0.9399743200013031    steps: 264    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 711   score: 2.0   memory length: 100000   epsilon: 0.9395407000013125    steps: 219    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 712   score: 1.0   memory length: 100000   epsilon: 0.9392041000013198    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 713   score: 2.0   memory length: 100000   epsilon: 0.9388457200013276    steps: 181    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 714   score: 3.0   memory length: 100000   epsilon: 0.9383507200013383    steps: 250    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 715   score: 2.0   memory length: 100000   epsilon: 0.9379567000013469    steps: 199    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 716   score: 5.0   memory length: 100000   epsilon: 0.9372399400013625    steps: 362    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 717   score: 1.0   memory length: 100000   epsilon: 0.936938980001369    steps: 152    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 718   score: 5.0   memory length: 100000   epsilon: 0.9362578600013838    steps: 344    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 719   score: 1.0   memory length: 100000   epsilon: 0.9359569000013903    steps: 152    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 720   score: 0.0   memory length: 100000   epsilon: 0.9357113800013956    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 721   score: 3.0   memory length: 100000   epsilon: 0.9351807400014072    steps: 268    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 722   score: 1.0   memory length: 100000   epsilon: 0.9348441400014145    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 723   score: 2.0   memory length: 100000   epsilon: 0.934452100001423    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 724   score: 0.0   memory length: 100000   epsilon: 0.9342065800014283    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 725   score: 1.0   memory length: 100000   epsilon: 0.9338699800014356    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 726   score: 2.0   memory length: 100000   epsilon: 0.9334759600014442    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 727   score: 3.0   memory length: 100000   epsilon: 0.9330185800014541    steps: 231    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 728   score: 0.0   memory length: 100000   epsilon: 0.9327730600014594    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 729   score: 1.0   memory length: 100000   epsilon: 0.9324364600014667    steps: 170    lr: 0.0001     evaluation reward: 1.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 730   score: 4.0   memory length: 100000   epsilon: 0.9318028600014805    steps: 320    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 731   score: 1.0   memory length: 100000   epsilon: 0.931503880001487    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 732   score: 0.0   memory length: 100000   epsilon: 0.9312583600014923    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 733   score: 2.0   memory length: 100000   epsilon: 0.9308623600015009    steps: 200    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 734   score: 0.0   memory length: 100000   epsilon: 0.9306168400015062    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 735   score: 0.0   memory length: 100000   epsilon: 0.9303713200015116    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 736   score: 2.0   memory length: 100000   epsilon: 0.9299792800015201    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 737   score: 1.0   memory length: 100000   epsilon: 0.9296426800015274    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 738   score: 2.0   memory length: 100000   epsilon: 0.9292110400015368    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 739   score: 3.0   memory length: 100000   epsilon: 0.9287160400015475    steps: 250    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 740   score: 1.0   memory length: 100000   epsilon: 0.9283794400015548    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 741   score: 2.0   memory length: 100000   epsilon: 0.9279854200015634    steps: 199    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 742   score: 1.0   memory length: 100000   epsilon: 0.9276488200015707    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 743   score: 0.0   memory length: 100000   epsilon: 0.927405280001576    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 744   score: 1.0   memory length: 100000   epsilon: 0.9270686800015833    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 745   score: 4.0   memory length: 100000   epsilon: 0.926484580001596    steps: 295    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 746   score: 3.0   memory length: 100000   epsilon: 0.9259935400016066    steps: 248    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 747   score: 1.0   memory length: 100000   epsilon: 0.925654960001614    steps: 171    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 748   score: 0.0   memory length: 100000   epsilon: 0.9254094400016193    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 749   score: 2.0   memory length: 100000   epsilon: 0.9250471000016272    steps: 183    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 750   score: 3.0   memory length: 100000   epsilon: 0.924591700001637    steps: 230    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 751   score: 2.0   memory length: 100000   epsilon: 0.9241541200016465    steps: 221    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 752   score: 4.0   memory length: 100000   epsilon: 0.9235621000016594    steps: 299    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 753   score: 0.0   memory length: 100000   epsilon: 0.9233185600016647    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 754   score: 0.0   memory length: 100000   epsilon: 0.92307304000167    steps: 124    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 755   score: 2.0   memory length: 100000   epsilon: 0.9226810000016785    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 756   score: 0.0   memory length: 100000   epsilon: 0.9224354800016838    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 757   score: 4.0   memory length: 100000   epsilon: 0.9218494000016966    steps: 296    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 758   score: 4.0   memory length: 100000   epsilon: 0.92122966000171    steps: 313    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 759   score: 3.0   memory length: 100000   epsilon: 0.9207802000017198    steps: 227    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 760   score: 2.0   memory length: 100000   epsilon: 0.9203465800017292    steps: 219    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 761   score: 0.0   memory length: 100000   epsilon: 0.9201010600017345    steps: 124    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 762   score: 0.0   memory length: 100000   epsilon: 0.9198555400017399    steps: 124    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 763   score: 0.0   memory length: 100000   epsilon: 0.9196120000017451    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 764   score: 0.0   memory length: 100000   epsilon: 0.9193664800017505    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 765   score: 2.0   memory length: 100000   epsilon: 0.9189685000017591    steps: 201    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 766   score: 3.0   memory length: 100000   epsilon: 0.9184398400017706    steps: 267    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 767   score: 3.0   memory length: 100000   epsilon: 0.9179903800017803    steps: 227    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 768   score: 4.0   memory length: 100000   epsilon: 0.9173963800017932    steps: 300    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 769   score: 0.0   memory length: 100000   epsilon: 0.9171508600017986    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 770   score: 1.0   memory length: 100000   epsilon: 0.9168499000018051    steps: 152    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 771   score: 1.0   memory length: 100000   epsilon: 0.9165133000018124    steps: 170    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 772   score: 3.0   memory length: 100000   epsilon: 0.9160598800018223    steps: 229    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 773   score: 0.0   memory length: 100000   epsilon: 0.9158163400018275    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 774   score: 3.0   memory length: 100000   epsilon: 0.915289660001839    steps: 266    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 775   score: 3.0   memory length: 100000   epsilon: 0.9147966400018497    steps: 249    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 776   score: 0.0   memory length: 100000   epsilon: 0.914551120001855    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 777   score: 2.0   memory length: 100000   epsilon: 0.9141155200018645    steps: 220    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 778   score: 0.0   memory length: 100000   epsilon: 0.9138719800018698    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 779   score: 0.0   memory length: 100000   epsilon: 0.9136264600018751    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 780   score: 6.0   memory length: 100000   epsilon: 0.9130007800018887    steps: 316    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 781   score: 4.0   memory length: 100000   epsilon: 0.9124107400019015    steps: 298    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 782   score: 4.0   memory length: 100000   epsilon: 0.9118682200019133    steps: 274    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 783   score: 1.0   memory length: 100000   epsilon: 0.9115336000019205    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 784   score: 0.0   memory length: 100000   epsilon: 0.9112880800019258    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 785   score: 0.0   memory length: 100000   epsilon: 0.9110445400019311    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 786   score: 0.0   memory length: 100000   epsilon: 0.9107990200019365    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 787   score: 9.0   memory length: 100000   epsilon: 0.9101238400019511    steps: 341    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 788   score: 1.0   memory length: 100000   epsilon: 0.9097813000019586    steps: 173    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 789   score: 1.0   memory length: 100000   epsilon: 0.909438760001966    steps: 173    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 790   score: 0.0   memory length: 100000   epsilon: 0.9091932400019713    steps: 124    lr: 0.0001     evaluation reward: 1.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 791   score: 4.0   memory length: 100000   epsilon: 0.9086467600019832    steps: 276    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 792   score: 4.0   memory length: 100000   epsilon: 0.9080943400019952    steps: 279    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 793   score: 4.0   memory length: 100000   epsilon: 0.907504300002008    steps: 298    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 794   score: 2.0   memory length: 100000   epsilon: 0.9071439400020158    steps: 182    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 795   score: 1.0   memory length: 100000   epsilon: 0.9068073400020231    steps: 170    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 796   score: 2.0   memory length: 100000   epsilon: 0.9064073800020318    steps: 202    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 797   score: 1.0   memory length: 100000   epsilon: 0.9060668200020392    steps: 172    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 798   score: 3.0   memory length: 100000   epsilon: 0.9055441000020505    steps: 264    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 799   score: 0.0   memory length: 100000   epsilon: 0.9052985800020559    steps: 124    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 800   score: 0.0   memory length: 100000   epsilon: 0.9050550400020612    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 801   score: 0.0   memory length: 100000   epsilon: 0.9048095200020665    steps: 124    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 802   score: 2.0   memory length: 100000   epsilon: 0.9044135200020751    steps: 200    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 803   score: 3.0   memory length: 100000   epsilon: 0.9039640600020848    steps: 227    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 804   score: 0.0   memory length: 100000   epsilon: 0.9037185400020902    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 805   score: 0.0   memory length: 100000   epsilon: 0.9034730200020955    steps: 124    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 806   score: 2.0   memory length: 100000   epsilon: 0.903033460002105    steps: 222    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 807   score: 0.0   memory length: 100000   epsilon: 0.9027879400021104    steps: 124    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 808   score: 1.0   memory length: 100000   epsilon: 0.9024513400021177    steps: 170    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 809   score: 0.0   memory length: 100000   epsilon: 0.902205820002123    steps: 124    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 810   score: 2.0   memory length: 100000   epsilon: 0.9018137800021315    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 811   score: 0.0   memory length: 100000   epsilon: 0.9015682600021369    steps: 124    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 812   score: 2.0   memory length: 100000   epsilon: 0.9011742400021454    steps: 199    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 813   score: 1.0   memory length: 100000   epsilon: 0.9008732800021519    steps: 152    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 814   score: 2.0   memory length: 100000   epsilon: 0.9004792600021605    steps: 199    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 815   score: 2.0   memory length: 100000   epsilon: 0.90004366000217    steps: 220    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 816   score: 3.0   memory length: 100000   epsilon: 0.8995922200021798    steps: 228    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 817   score: 2.0   memory length: 100000   epsilon: 0.8991506800021893    steps: 223    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 818   score: 1.0   memory length: 100000   epsilon: 0.8988140800021966    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 819   score: 0.0   memory length: 100000   epsilon: 0.898568560002202    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 820   score: 2.0   memory length: 100000   epsilon: 0.8982101800022098    steps: 181    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 821   score: 2.0   memory length: 100000   epsilon: 0.8978161600022183    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 822   score: 3.0   memory length: 100000   epsilon: 0.89727760000223    steps: 272    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 823   score: 1.0   memory length: 100000   epsilon: 0.8969766400022365    steps: 152    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 824   score: 2.0   memory length: 100000   epsilon: 0.8965806400022451    steps: 200    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 825   score: 2.0   memory length: 100000   epsilon: 0.8961866200022537    steps: 199    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 826   score: 2.0   memory length: 100000   epsilon: 0.8957549800022631    steps: 218    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 827   score: 2.0   memory length: 100000   epsilon: 0.8953253200022724    steps: 217    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 828   score: 2.0   memory length: 100000   epsilon: 0.8949253600022811    steps: 202    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 829   score: 2.0   memory length: 100000   epsilon: 0.8945313400022896    steps: 199    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 830   score: 2.0   memory length: 100000   epsilon: 0.8940937600022991    steps: 221    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 831   score: 2.0   memory length: 100000   epsilon: 0.8937017200023076    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 832   score: 0.0   memory length: 100000   epsilon: 0.893456200002313    steps: 124    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 833   score: 0.0   memory length: 100000   epsilon: 0.8932126600023182    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 834   score: 1.0   memory length: 100000   epsilon: 0.8928721000023256    steps: 172    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 835   score: 6.0   memory length: 100000   epsilon: 0.8921513800023413    steps: 364    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 836   score: 1.0   memory length: 100000   epsilon: 0.8918147800023486    steps: 170    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 837   score: 2.0   memory length: 100000   epsilon: 0.8914227400023571    steps: 198    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 838   score: 2.0   memory length: 100000   epsilon: 0.8910564400023651    steps: 185    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 839   score: 5.0   memory length: 100000   epsilon: 0.8903713600023799    steps: 346    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 840   score: 1.0   memory length: 100000   epsilon: 0.8900704000023865    steps: 152    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 841   score: 2.0   memory length: 100000   epsilon: 0.889678360002395    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 842   score: 1.0   memory length: 100000   epsilon: 0.8893774000024015    steps: 152    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 843   score: 0.0   memory length: 100000   epsilon: 0.8891338600024068    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 844   score: 1.0   memory length: 100000   epsilon: 0.8887972600024141    steps: 170    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 845   score: 1.0   memory length: 100000   epsilon: 0.8884626400024214    steps: 169    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 846   score: 0.0   memory length: 100000   epsilon: 0.8882171200024267    steps: 124    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 847   score: 0.0   memory length: 100000   epsilon: 0.887971600002432    steps: 124    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 848   score: 2.0   memory length: 100000   epsilon: 0.8875756000024406    steps: 200    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 849   score: 3.0   memory length: 100000   epsilon: 0.8870825800024513    steps: 249    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 850   score: 0.0   memory length: 100000   epsilon: 0.8868370600024567    steps: 124    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 851   score: 1.0   memory length: 100000   epsilon: 0.886500460002464    steps: 170    lr: 0.0001     evaluation reward: 1.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 852   score: 1.0   memory length: 100000   epsilon: 0.8861638600024713    steps: 170    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 853   score: 0.0   memory length: 100000   epsilon: 0.8859183400024766    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 854   score: 0.0   memory length: 100000   epsilon: 0.8856728200024819    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 855   score: 0.0   memory length: 100000   epsilon: 0.8854273000024873    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 856   score: 0.0   memory length: 100000   epsilon: 0.8851837600024925    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 857   score: 0.0   memory length: 100000   epsilon: 0.8849382400024979    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 858   score: 0.0   memory length: 100000   epsilon: 0.8846927200025032    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 859   score: 0.0   memory length: 100000   epsilon: 0.8844472000025085    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 860   score: 0.0   memory length: 100000   epsilon: 0.8842016800025139    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 861   score: 2.0   memory length: 100000   epsilon: 0.8837680600025233    steps: 219    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 862   score: 3.0   memory length: 100000   epsilon: 0.8832829600025338    steps: 245    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 863   score: 2.0   memory length: 100000   epsilon: 0.8829206200025417    steps: 183    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 864   score: 0.0   memory length: 100000   epsilon: 0.882675100002547    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 865   score: 1.0   memory length: 100000   epsilon: 0.8823741400025535    steps: 152    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 866   score: 1.0   memory length: 100000   epsilon: 0.88207516000256    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 867   score: 2.0   memory length: 100000   epsilon: 0.8816831200025685    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 868   score: 1.0   memory length: 100000   epsilon: 0.8813465200025759    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 869   score: 0.0   memory length: 100000   epsilon: 0.8811010000025812    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 870   score: 1.0   memory length: 100000   epsilon: 0.8808000400025877    steps: 152    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 871   score: 0.0   memory length: 100000   epsilon: 0.880556500002593    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 872   score: 2.0   memory length: 100000   epsilon: 0.8801228800026024    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 873   score: 2.0   memory length: 100000   epsilon: 0.879679360002612    steps: 224    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 874   score: 2.0   memory length: 100000   epsilon: 0.8792457400026215    steps: 219    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 875   score: 4.0   memory length: 100000   epsilon: 0.8786161000026351    steps: 318    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 876   score: 0.0   memory length: 100000   epsilon: 0.8783705800026405    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 877   score: 1.0   memory length: 100000   epsilon: 0.878071600002647    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 878   score: 2.0   memory length: 100000   epsilon: 0.8776775800026555    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 879   score: 0.0   memory length: 100000   epsilon: 0.8774320600026608    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 880   score: 3.0   memory length: 100000   epsilon: 0.8769410200026715    steps: 248    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 881   score: 3.0   memory length: 100000   epsilon: 0.8764499800026821    steps: 248    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 882   score: 0.0   memory length: 100000   epsilon: 0.8762064400026874    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 883   score: 1.0   memory length: 100000   epsilon: 0.8758658800026948    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 884   score: 0.0   memory length: 100000   epsilon: 0.8756203600027002    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 885   score: 4.0   memory length: 100000   epsilon: 0.875030320002713    steps: 298    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 886   score: 4.0   memory length: 100000   epsilon: 0.8744363200027259    steps: 300    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 887   score: 1.0   memory length: 100000   epsilon: 0.8741353600027324    steps: 152    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 888   score: 1.0   memory length: 100000   epsilon: 0.8737967800027397    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 889   score: 1.0   memory length: 100000   epsilon: 0.8734958200027463    steps: 152    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 890   score: 1.0   memory length: 100000   epsilon: 0.8731532800027537    steps: 173    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 891   score: 0.0   memory length: 100000   epsilon: 0.872907760002759    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 892   score: 0.0   memory length: 100000   epsilon: 0.8726642200027643    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 893   score: 2.0   memory length: 100000   epsilon: 0.8722682200027729    steps: 200    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 894   score: 0.0   memory length: 100000   epsilon: 0.8720227000027783    steps: 124    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 895   score: 1.0   memory length: 100000   epsilon: 0.8716841200027856    steps: 171    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 896   score: 3.0   memory length: 100000   epsilon: 0.8712287200027955    steps: 230    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 897   score: 2.0   memory length: 100000   epsilon: 0.870791140002805    steps: 221    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 898   score: 2.0   memory length: 100000   epsilon: 0.8703971200028136    steps: 199    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 899   score: 2.0   memory length: 100000   epsilon: 0.8700031000028221    steps: 199    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 900   score: 3.0   memory length: 100000   epsilon: 0.8695397800028322    steps: 234    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 901   score: 2.0   memory length: 100000   epsilon: 0.8691457600028407    steps: 199    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 902   score: 0.0   memory length: 100000   epsilon: 0.868902220002846    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 903   score: 1.0   memory length: 100000   epsilon: 0.8686012600028525    steps: 152    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 904   score: 0.0   memory length: 100000   epsilon: 0.8683577200028578    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 905   score: 0.0   memory length: 100000   epsilon: 0.8681122000028632    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 906   score: 2.0   memory length: 100000   epsilon: 0.8676766000028726    steps: 220    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 907   score: 1.0   memory length: 100000   epsilon: 0.8673400000028799    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 908   score: 1.0   memory length: 100000   epsilon: 0.8670034000028872    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 909   score: 2.0   memory length: 100000   epsilon: 0.8665717600028966    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 910   score: 1.0   memory length: 100000   epsilon: 0.8662351600029039    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 911   score: 0.0   memory length: 100000   epsilon: 0.8659896400029092    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 912   score: 2.0   memory length: 100000   epsilon: 0.8655956200029178    steps: 199    lr: 0.0001     evaluation reward: 1.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 913   score: 0.0   memory length: 100000   epsilon: 0.8653501000029231    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 914   score: 1.0   memory length: 100000   epsilon: 0.8650135000029304    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 915   score: 1.0   memory length: 100000   epsilon: 0.8647145200029369    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 916   score: 4.0   memory length: 100000   epsilon: 0.8641640800029489    steps: 278    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 917   score: 5.0   memory length: 100000   epsilon: 0.8635423600029624    steps: 314    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 918   score: 0.0   memory length: 100000   epsilon: 0.8632968400029677    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 919   score: 1.0   memory length: 100000   epsilon: 0.8629543000029751    steps: 173    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 920   score: 1.0   memory length: 100000   epsilon: 0.8626137400029825    steps: 172    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 921   score: 1.0   memory length: 100000   epsilon: 0.862312780002989    steps: 152    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 922   score: 0.0   memory length: 100000   epsilon: 0.8620672600029944    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 923   score: 7.0   memory length: 100000   epsilon: 0.8615742400030051    steps: 249    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 924   score: 0.0   memory length: 100000   epsilon: 0.8613287200030104    steps: 124    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 925   score: 0.0   memory length: 100000   epsilon: 0.8610832000030157    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 926   score: 2.0   memory length: 100000   epsilon: 0.8606535400030251    steps: 217    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 927   score: 0.0   memory length: 100000   epsilon: 0.8604100000030304    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 928   score: 2.0   memory length: 100000   epsilon: 0.8600179600030389    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 929   score: 2.0   memory length: 100000   epsilon: 0.8595823600030483    steps: 220    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 930   score: 0.0   memory length: 100000   epsilon: 0.8593388200030536    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 931   score: 1.0   memory length: 100000   epsilon: 0.858998260003061    steps: 172    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 932   score: 3.0   memory length: 100000   epsilon: 0.8584696000030725    steps: 267    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 933   score: 3.0   memory length: 100000   epsilon: 0.8579785600030831    steps: 248    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 934   score: 1.0   memory length: 100000   epsilon: 0.8576399800030905    steps: 171    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 935   score: 3.0   memory length: 100000   epsilon: 0.857154880003101    steps: 245    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 936   score: 2.0   memory length: 100000   epsilon: 0.8567628400031095    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 937   score: 0.0   memory length: 100000   epsilon: 0.8565193000031148    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 938   score: 0.0   memory length: 100000   epsilon: 0.8562757600031201    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 939   score: 1.0   memory length: 100000   epsilon: 0.8559748000031266    steps: 152    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 940   score: 2.0   memory length: 100000   epsilon: 0.8555827600031352    steps: 198    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 941   score: 0.0   memory length: 100000   epsilon: 0.8553372400031405    steps: 124    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 942   score: 1.0   memory length: 100000   epsilon: 0.8550006400031478    steps: 170    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 943   score: 0.0   memory length: 100000   epsilon: 0.8547571000031531    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 944   score: 3.0   memory length: 100000   epsilon: 0.854301700003163    steps: 230    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 945   score: 3.0   memory length: 100000   epsilon: 0.8538126400031736    steps: 247    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 946   score: 1.0   memory length: 100000   epsilon: 0.8534740600031809    steps: 171    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 947   score: 0.0   memory length: 100000   epsilon: 0.8532285400031863    steps: 124    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 948   score: 1.0   memory length: 100000   epsilon: 0.8529275800031928    steps: 152    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 949   score: 3.0   memory length: 100000   epsilon: 0.8523850600032046    steps: 274    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 950   score: 2.0   memory length: 100000   epsilon: 0.8519930200032131    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 951   score: 0.0   memory length: 100000   epsilon: 0.8517475000032184    steps: 124    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 952   score: 2.0   memory length: 100000   epsilon: 0.8513099200032279    steps: 221    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 953   score: 2.0   memory length: 100000   epsilon: 0.8509119400032366    steps: 201    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 954   score: 3.0   memory length: 100000   epsilon: 0.8504605000032464    steps: 228    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 955   score: 1.0   memory length: 100000   epsilon: 0.8501595400032529    steps: 152    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 956   score: 1.0   memory length: 100000   epsilon: 0.8498249200032602    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 957   score: 0.0   memory length: 100000   epsilon: 0.8495794000032655    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 958   score: 2.0   memory length: 100000   epsilon: 0.849185380003274    steps: 199    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 959   score: 0.0   memory length: 100000   epsilon: 0.8489398600032794    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 960   score: 1.0   memory length: 100000   epsilon: 0.8486032600032867    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 961   score: 0.0   memory length: 100000   epsilon: 0.848357740003292    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 962   score: 1.0   memory length: 100000   epsilon: 0.8480191600032994    steps: 171    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 963   score: 1.0   memory length: 100000   epsilon: 0.8476786000033067    steps: 172    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 964   score: 0.0   memory length: 100000   epsilon: 0.8474330800033121    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 965   score: 0.0   memory length: 100000   epsilon: 0.8471875600033174    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 966   score: 2.0   memory length: 100000   epsilon: 0.8468291800033252    steps: 181    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 967   score: 4.0   memory length: 100000   epsilon: 0.8462807200033371    steps: 277    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 968   score: 1.0   memory length: 100000   epsilon: 0.8459797600033436    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 969   score: 4.0   memory length: 100000   epsilon: 0.8453956600033563    steps: 295    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 970   score: 4.0   memory length: 100000   epsilon: 0.8447699800033699    steps: 316    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 971   score: 0.0   memory length: 100000   epsilon: 0.8445244600033752    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 972   score: 4.0   memory length: 100000   epsilon: 0.843936400003388    steps: 297    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 973   score: 5.0   memory length: 100000   epsilon: 0.8433028000034017    steps: 320    lr: 0.0001     evaluation reward: 1.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 974   score: 3.0   memory length: 100000   epsilon: 0.8428058200034125    steps: 251    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 975   score: 2.0   memory length: 100000   epsilon: 0.8424118000034211    steps: 199    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 976   score: 1.0   memory length: 100000   epsilon: 0.8420692600034285    steps: 173    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 977   score: 1.0   memory length: 100000   epsilon: 0.8417326600034358    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 978   score: 0.0   memory length: 100000   epsilon: 0.8414891200034411    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 979   score: 2.0   memory length: 100000   epsilon: 0.841126780003449    steps: 183    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 980   score: 1.0   memory length: 100000   epsilon: 0.8407862200034564    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 981   score: 1.0   memory length: 100000   epsilon: 0.8404496200034637    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 982   score: 0.0   memory length: 100000   epsilon: 0.840204100003469    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 983   score: 2.0   memory length: 100000   epsilon: 0.8398100800034776    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 984   score: 1.0   memory length: 100000   epsilon: 0.8395091200034841    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 985   score: 3.0   memory length: 100000   epsilon: 0.8390616400034938    steps: 226    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 986   score: 0.0   memory length: 100000   epsilon: 0.8388161200034991    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 987   score: 1.0   memory length: 100000   epsilon: 0.8384755600035065    steps: 172    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 988   score: 0.0   memory length: 100000   epsilon: 0.8382320200035118    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 989   score: 4.0   memory length: 100000   epsilon: 0.8376855400035237    steps: 276    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 990   score: 0.0   memory length: 100000   epsilon: 0.837440020003529    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 991   score: 6.0   memory length: 100000   epsilon: 0.8367351400035443    steps: 356    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 992   score: 0.0   memory length: 100000   epsilon: 0.8364896200035496    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 993   score: 3.0   memory length: 100000   epsilon: 0.8360025400035602    steps: 246    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 994   score: 0.0   memory length: 100000   epsilon: 0.8357570200035656    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 995   score: 0.0   memory length: 100000   epsilon: 0.8355115000035709    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 996   score: 4.0   memory length: 100000   epsilon: 0.8349947200035821    steps: 261    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 997   score: 4.0   memory length: 100000   epsilon: 0.8344403200035941    steps: 280    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 998   score: 6.0   memory length: 100000   epsilon: 0.8337057400036101    steps: 371    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 999   score: 1.0   memory length: 100000   epsilon: 0.8334067600036166    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1000   score: 3.0   memory length: 100000   epsilon: 0.8329196800036271    steps: 246    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1001   score: 3.0   memory length: 100000   epsilon: 0.8324286400036378    steps: 248    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1002   score: 1.0   memory length: 100000   epsilon: 0.8320880800036452    steps: 172    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1003   score: 2.0   memory length: 100000   epsilon: 0.8316524800036547    steps: 220    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1004   score: 2.0   memory length: 100000   epsilon: 0.8312584600036632    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1005   score: 2.0   memory length: 100000   epsilon: 0.8308644400036718    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1006   score: 1.0   memory length: 100000   epsilon: 0.8305278400036791    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1007   score: 4.0   memory length: 100000   epsilon: 0.8299437400036918    steps: 295    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1008   score: 1.0   memory length: 100000   epsilon: 0.8296071400036991    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1009   score: 2.0   memory length: 100000   epsilon: 0.8291735200037085    steps: 219    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1010   score: 1.0   memory length: 100000   epsilon: 0.8288329600037159    steps: 172    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1011   score: 0.0   memory length: 100000   epsilon: 0.8285874400037212    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1012   score: 3.0   memory length: 100000   epsilon: 0.8280964000037319    steps: 248    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 1013   score: 1.0   memory length: 100000   epsilon: 0.8277598000037392    steps: 170    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1014   score: 3.0   memory length: 100000   epsilon: 0.8272687600037498    steps: 248    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 1015   score: 3.0   memory length: 100000   epsilon: 0.8267797000037604    steps: 247    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1016   score: 0.0   memory length: 100000   epsilon: 0.8265361600037657    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1017   score: 2.0   memory length: 100000   epsilon: 0.8261025400037751    steps: 219    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1018   score: 2.0   memory length: 100000   epsilon: 0.8257441600037829    steps: 181    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 1019   score: 4.0   memory length: 100000   epsilon: 0.8251957000037948    steps: 277    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 1020   score: 2.0   memory length: 100000   epsilon: 0.8247937600038036    steps: 203    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1021   score: 6.0   memory length: 100000   epsilon: 0.824080960003819    steps: 360    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 1022   score: 2.0   memory length: 100000   epsilon: 0.8236414000038286    steps: 222    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 1023   score: 1.0   memory length: 100000   epsilon: 0.8233048000038359    steps: 170    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1024   score: 0.0   memory length: 100000   epsilon: 0.8230592800038412    steps: 124    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1025   score: 3.0   memory length: 100000   epsilon: 0.822609820003851    steps: 227    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 1026   score: 0.0   memory length: 100000   epsilon: 0.8223643000038563    steps: 124    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 1027   score: 2.0   memory length: 100000   epsilon: 0.821962360003865    steps: 203    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 1028   score: 1.0   memory length: 100000   epsilon: 0.8216614000038716    steps: 152    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 1029   score: 3.0   memory length: 100000   epsilon: 0.8212119400038813    steps: 227    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 1030   score: 3.0   memory length: 100000   epsilon: 0.8207228800038919    steps: 247    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 1031   score: 3.0   memory length: 100000   epsilon: 0.8202734200039017    steps: 227    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1032   score: 0.0   memory length: 100000   epsilon: 0.820029880003907    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 1033   score: 0.0   memory length: 100000   epsilon: 0.8197863400039123    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 1034   score: 1.0   memory length: 100000   epsilon: 0.8194873600039188    steps: 151    lr: 0.0001     evaluation reward: 1.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1035   score: 0.0   memory length: 100000   epsilon: 0.8192418400039241    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1036   score: 2.0   memory length: 100000   epsilon: 0.8188478200039326    steps: 199    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1037   score: 0.0   memory length: 100000   epsilon: 0.818602300003938    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1038   score: 0.0   memory length: 100000   epsilon: 0.8183567800039433    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1039   score: 3.0   memory length: 100000   epsilon: 0.8179053400039531    steps: 228    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 1040   score: 1.0   memory length: 100000   epsilon: 0.8176063600039596    steps: 151    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1041   score: 3.0   memory length: 100000   epsilon: 0.8171212600039701    steps: 245    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 1042   score: 4.0   memory length: 100000   epsilon: 0.816572800003982    steps: 277    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 1043   score: 1.0   memory length: 100000   epsilon: 0.8162322400039894    steps: 172    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1044   score: 2.0   memory length: 100000   epsilon: 0.815838220003998    steps: 199    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 1045   score: 5.0   memory length: 100000   epsilon: 0.8152066000040117    steps: 319    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1046   score: 0.0   memory length: 100000   epsilon: 0.814961080004017    steps: 124    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1047   score: 1.0   memory length: 100000   epsilon: 0.8146264600040243    steps: 169    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1048   score: 3.0   memory length: 100000   epsilon: 0.8141710600040342    steps: 230    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1049   score: 2.0   memory length: 100000   epsilon: 0.8137790200040427    steps: 198    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1050   score: 2.0   memory length: 100000   epsilon: 0.8134206400040505    steps: 181    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1051   score: 2.0   memory length: 100000   epsilon: 0.813024640004059    steps: 200    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1052   score: 2.0   memory length: 100000   epsilon: 0.8126286400040676    steps: 200    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1053   score: 1.0   memory length: 100000   epsilon: 0.8123276800040742    steps: 152    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1054   score: 1.0   memory length: 100000   epsilon: 0.8119910800040815    steps: 170    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1055   score: 0.0   memory length: 100000   epsilon: 0.8117455600040868    steps: 124    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1056   score: 1.0   memory length: 100000   epsilon: 0.8114089600040941    steps: 170    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1057   score: 1.0   memory length: 100000   epsilon: 0.8110743400041014    steps: 169    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1058   score: 0.0   memory length: 100000   epsilon: 0.8108308000041067    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 1059   score: 2.0   memory length: 100000   epsilon: 0.8104367800041152    steps: 199    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1060   score: 0.0   memory length: 100000   epsilon: 0.8101912600041206    steps: 124    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1061   score: 1.0   memory length: 100000   epsilon: 0.809850700004128    steps: 172    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1062   score: 3.0   memory length: 100000   epsilon: 0.8094012400041377    steps: 227    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1063   score: 3.0   memory length: 100000   epsilon: 0.8089418800041477    steps: 232    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1064   score: 2.0   memory length: 100000   epsilon: 0.8085082600041571    steps: 219    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1065   score: 1.0   memory length: 100000   epsilon: 0.8082092800041636    steps: 151    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1066   score: 2.0   memory length: 100000   epsilon: 0.8078172400041721    steps: 198    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1067   score: 1.0   memory length: 100000   epsilon: 0.8075162800041786    steps: 152    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1068   score: 0.0   memory length: 100000   epsilon: 0.807270760004184    steps: 124    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1069   score: 1.0   memory length: 100000   epsilon: 0.8069302000041914    steps: 172    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1070   score: 3.0   memory length: 100000   epsilon: 0.8064332200042021    steps: 251    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1071   score: 2.0   memory length: 100000   epsilon: 0.80607286000421    steps: 182    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1072   score: 1.0   memory length: 100000   epsilon: 0.8057719000042165    steps: 152    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 1073   score: 6.0   memory length: 100000   epsilon: 0.8050690000042318    steps: 355    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1074   score: 5.0   memory length: 100000   epsilon: 0.8044314400042456    steps: 322    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1075   score: 4.0   memory length: 100000   epsilon: 0.8038453600042583    steps: 296    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1076   score: 1.0   memory length: 100000   epsilon: 0.8035463800042648    steps: 151    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1077   score: 3.0   memory length: 100000   epsilon: 0.8030553400042755    steps: 248    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1078   score: 1.0   memory length: 100000   epsilon: 0.802756360004282    steps: 151    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1079   score: 1.0   memory length: 100000   epsilon: 0.8024554000042885    steps: 152    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1080   score: 1.0   memory length: 100000   epsilon: 0.802154440004295    steps: 152    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1081   score: 1.0   memory length: 100000   epsilon: 0.8018158600043024    steps: 171    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1082   score: 3.0   memory length: 100000   epsilon: 0.801328780004313    steps: 246    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1083   score: 2.0   memory length: 100000   epsilon: 0.8009347600043215    steps: 199    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1084   score: 2.0   memory length: 100000   epsilon: 0.8005031200043309    steps: 218    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1085   score: 1.0   memory length: 100000   epsilon: 0.8002041400043374    steps: 151    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1086   score: 2.0   memory length: 100000   epsilon: 0.799808140004346    steps: 200    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1087   score: 2.0   memory length: 100000   epsilon: 0.7993745200043554    steps: 219    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1088   score: 5.0   memory length: 100000   epsilon: 0.7987171600043697    steps: 332    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 1089   score: 2.0   memory length: 100000   epsilon: 0.7983231400043782    steps: 199    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1090   score: 2.0   memory length: 100000   epsilon: 0.7979291200043868    steps: 199    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 1091   score: 2.0   memory length: 100000   epsilon: 0.7975351000043953    steps: 199    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1092   score: 2.0   memory length: 100000   epsilon: 0.7971430600044038    steps: 198    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1093   score: 2.0   memory length: 100000   epsilon: 0.7967510200044123    steps: 198    lr: 4e-05     evaluation reward: 1.91\n",
      "episode: 1094   score: 3.0   memory length: 100000   epsilon: 0.7963015600044221    steps: 227    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 1095   score: 2.0   memory length: 100000   epsilon: 0.7958659600044315    steps: 220    lr: 4e-05     evaluation reward: 1.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1096   score: 1.0   memory length: 100000   epsilon: 0.7955254000044389    steps: 172    lr: 4e-05     evaluation reward: 1.93\n",
      "episode: 1097   score: 0.0   memory length: 100000   epsilon: 0.7952818600044442    steps: 123    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1098   score: 4.0   memory length: 100000   epsilon: 0.7947670600044554    steps: 260    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1099   score: 1.0   memory length: 100000   epsilon: 0.7944661000044619    steps: 152    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1100   score: 5.0   memory length: 100000   epsilon: 0.7938424000044755    steps: 315    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1101   score: 2.0   memory length: 100000   epsilon: 0.793448380004484    steps: 199    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1102   score: 0.0   memory length: 100000   epsilon: 0.7932028600044894    steps: 124    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1103   score: 2.0   memory length: 100000   epsilon: 0.7928088400044979    steps: 199    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1104   score: 0.0   memory length: 100000   epsilon: 0.7925633200045032    steps: 124    lr: 4e-05     evaluation reward: 1.85\n",
      "episode: 1105   score: 3.0   memory length: 100000   epsilon: 0.7921099000045131    steps: 229    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1106   score: 1.0   memory length: 100000   epsilon: 0.7918109200045196    steps: 151    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1107   score: 0.0   memory length: 100000   epsilon: 0.7915673800045249    steps: 123    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1108   score: 1.0   memory length: 100000   epsilon: 0.7912664200045314    steps: 152    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1109   score: 3.0   memory length: 100000   epsilon: 0.790777360004542    steps: 247    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1110   score: 0.0   memory length: 100000   epsilon: 0.7905318400045473    steps: 124    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1111   score: 2.0   memory length: 100000   epsilon: 0.7901378200045559    steps: 199    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1112   score: 2.0   memory length: 100000   epsilon: 0.7897457800045644    steps: 198    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1113   score: 1.0   memory length: 100000   epsilon: 0.789444820004571    steps: 152    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1114   score: 2.0   memory length: 100000   epsilon: 0.7890527800045795    steps: 198    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1115   score: 2.0   memory length: 100000   epsilon: 0.7886211400045888    steps: 218    lr: 4e-05     evaluation reward: 1.81\n",
      "episode: 1116   score: 3.0   memory length: 100000   epsilon: 0.7881736600045985    steps: 226    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1117   score: 2.0   memory length: 100000   epsilon: 0.7878133000046064    steps: 182    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1118   score: 0.0   memory length: 100000   epsilon: 0.7875677800046117    steps: 124    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1119   score: 0.0   memory length: 100000   epsilon: 0.787324240004617    steps: 123    lr: 4e-05     evaluation reward: 1.78\n",
      "episode: 1120   score: 2.0   memory length: 100000   epsilon: 0.7869203200046258    steps: 204    lr: 4e-05     evaluation reward: 1.78\n",
      "episode: 1121   score: 2.0   memory length: 100000   epsilon: 0.7865579800046336    steps: 183    lr: 4e-05     evaluation reward: 1.74\n",
      "episode: 1122   score: 2.0   memory length: 100000   epsilon: 0.7861639600046422    steps: 199    lr: 4e-05     evaluation reward: 1.74\n",
      "episode: 1123   score: 5.0   memory length: 100000   epsilon: 0.7855541200046554    steps: 308    lr: 4e-05     evaluation reward: 1.78\n",
      "episode: 1124   score: 1.0   memory length: 100000   epsilon: 0.7852531600046619    steps: 152    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1125   score: 1.0   memory length: 100000   epsilon: 0.7849541800046684    steps: 151    lr: 4e-05     evaluation reward: 1.77\n",
      "episode: 1126   score: 0.0   memory length: 100000   epsilon: 0.7847086600046738    steps: 124    lr: 4e-05     evaluation reward: 1.77\n",
      "episode: 1127   score: 2.0   memory length: 100000   epsilon: 0.7843126600046824    steps: 200    lr: 4e-05     evaluation reward: 1.77\n",
      "episode: 1128   score: 2.0   memory length: 100000   epsilon: 0.7838790400046918    steps: 219    lr: 4e-05     evaluation reward: 1.78\n",
      "episode: 1129   score: 1.0   memory length: 100000   epsilon: 0.7835780800046983    steps: 152    lr: 4e-05     evaluation reward: 1.76\n",
      "episode: 1130   score: 5.0   memory length: 100000   epsilon: 0.7828890400047133    steps: 348    lr: 4e-05     evaluation reward: 1.78\n",
      "episode: 1131   score: 3.0   memory length: 100000   epsilon: 0.7824336400047232    steps: 230    lr: 4e-05     evaluation reward: 1.78\n",
      "episode: 1132   score: 1.0   memory length: 100000   epsilon: 0.7821326800047297    steps: 152    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1133   score: 3.0   memory length: 100000   epsilon: 0.7816693600047397    steps: 234    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1134   score: 1.0   memory length: 100000   epsilon: 0.7813703800047462    steps: 151    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1135   score: 4.0   memory length: 100000   epsilon: 0.7808239000047581    steps: 276    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1136   score: 4.0   memory length: 100000   epsilon: 0.7801962400047717    steps: 317    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1137   score: 2.0   memory length: 100000   epsilon: 0.7798042000047802    steps: 198    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1138   score: 2.0   memory length: 100000   epsilon: 0.7794101800047888    steps: 199    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1139   score: 3.0   memory length: 100000   epsilon: 0.7788755800048004    steps: 270    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1140   score: 1.0   memory length: 100000   epsilon: 0.7785409600048077    steps: 169    lr: 4e-05     evaluation reward: 1.92\n",
      "episode: 1141   score: 1.0   memory length: 100000   epsilon: 0.778204360004815    steps: 170    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1142   score: 2.0   memory length: 100000   epsilon: 0.7778103400048235    steps: 199    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1143   score: 0.0   memory length: 100000   epsilon: 0.7775648200048288    steps: 124    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1144   score: 1.0   memory length: 100000   epsilon: 0.7772222800048363    steps: 173    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1145   score: 2.0   memory length: 100000   epsilon: 0.7767906400048457    steps: 218    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1146   score: 4.0   memory length: 100000   epsilon: 0.7762857400048566    steps: 255    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1147   score: 2.0   memory length: 100000   epsilon: 0.7758917200048652    steps: 199    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1148   score: 3.0   memory length: 100000   epsilon: 0.7754363200048751    steps: 230    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1149   score: 0.0   memory length: 100000   epsilon: 0.7751908000048804    steps: 124    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1150   score: 2.0   memory length: 100000   epsilon: 0.7747611400048897    steps: 217    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1151   score: 2.0   memory length: 100000   epsilon: 0.7743691000048982    steps: 198    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1152   score: 2.0   memory length: 100000   epsilon: 0.7739275600049078    steps: 223    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1153   score: 4.0   memory length: 100000   epsilon: 0.7733771200049198    steps: 278    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1154   score: 2.0   memory length: 100000   epsilon: 0.7729415200049292    steps: 220    lr: 4e-05     evaluation reward: 1.9\n",
      "episode: 1155   score: 4.0   memory length: 100000   epsilon: 0.7723554400049419    steps: 296    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 1156   score: 3.0   memory length: 100000   epsilon: 0.7718644000049526    steps: 248    lr: 4e-05     evaluation reward: 1.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1157   score: 2.0   memory length: 100000   epsilon: 0.7715040400049604    steps: 182    lr: 4e-05     evaluation reward: 1.97\n",
      "episode: 1158   score: 2.0   memory length: 100000   epsilon: 0.771110020004969    steps: 199    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1159   score: 5.0   memory length: 100000   epsilon: 0.7704605800049831    steps: 328    lr: 4e-05     evaluation reward: 2.02\n",
      "episode: 1160   score: 3.0   memory length: 100000   epsilon: 0.7700131000049928    steps: 226    lr: 4e-05     evaluation reward: 2.05\n",
      "episode: 1161   score: 3.0   memory length: 100000   epsilon: 0.7695577000050027    steps: 230    lr: 4e-05     evaluation reward: 2.07\n",
      "episode: 1162   score: 2.0   memory length: 100000   epsilon: 0.7691993200050105    steps: 181    lr: 4e-05     evaluation reward: 2.06\n",
      "episode: 1163   score: 1.0   memory length: 100000   epsilon: 0.768898360005017    steps: 152    lr: 4e-05     evaluation reward: 2.04\n",
      "episode: 1164   score: 3.0   memory length: 100000   epsilon: 0.7684469200050268    steps: 228    lr: 4e-05     evaluation reward: 2.05\n",
      "episode: 1165   score: 4.0   memory length: 100000   epsilon: 0.7678608400050395    steps: 296    lr: 4e-05     evaluation reward: 2.08\n",
      "episode: 1166   score: 1.0   memory length: 100000   epsilon: 0.767561860005046    steps: 151    lr: 4e-05     evaluation reward: 2.07\n",
      "episode: 1167   score: 5.0   memory length: 100000   epsilon: 0.76691836000506    steps: 325    lr: 4e-05     evaluation reward: 2.11\n",
      "episode: 1168   score: 3.0   memory length: 100000   epsilon: 0.7664649400050698    steps: 229    lr: 4e-05     evaluation reward: 2.14\n",
      "episode: 1169   score: 4.0   memory length: 100000   epsilon: 0.7658808400050825    steps: 295    lr: 4e-05     evaluation reward: 2.17\n",
      "episode: 1170   score: 3.0   memory length: 100000   epsilon: 0.7654274200050923    steps: 229    lr: 4e-05     evaluation reward: 2.17\n",
      "episode: 1171   score: 2.0   memory length: 100000   epsilon: 0.7649957800051017    steps: 218    lr: 4e-05     evaluation reward: 2.17\n",
      "episode: 1172   score: 5.0   memory length: 100000   epsilon: 0.7643958400051147    steps: 303    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1173   score: 2.0   memory length: 100000   epsilon: 0.7640335000051226    steps: 183    lr: 4e-05     evaluation reward: 2.17\n",
      "episode: 1174   score: 2.0   memory length: 100000   epsilon: 0.763599880005132    steps: 219    lr: 4e-05     evaluation reward: 2.14\n",
      "episode: 1175   score: 1.0   memory length: 100000   epsilon: 0.7632652600051393    steps: 169    lr: 4e-05     evaluation reward: 2.11\n",
      "episode: 1176   score: 5.0   memory length: 100000   epsilon: 0.7625782000051542    steps: 347    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1177   score: 2.0   memory length: 100000   epsilon: 0.7621841800051627    steps: 199    lr: 4e-05     evaluation reward: 2.14\n",
      "episode: 1178   score: 3.0   memory length: 100000   epsilon: 0.7617347200051725    steps: 227    lr: 4e-05     evaluation reward: 2.16\n",
      "episode: 1179   score: 3.0   memory length: 100000   epsilon: 0.7612852600051823    steps: 227    lr: 4e-05     evaluation reward: 2.18\n",
      "episode: 1180   score: 3.0   memory length: 100000   epsilon: 0.760837780005192    steps: 226    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1181   score: 3.0   memory length: 100000   epsilon: 0.7604180200052011    steps: 212    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1182   score: 8.0   memory length: 100000   epsilon: 0.7598002600052145    steps: 312    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1183   score: 3.0   memory length: 100000   epsilon: 0.7593448600052244    steps: 230    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1184   score: 2.0   memory length: 100000   epsilon: 0.7589845000052322    steps: 182    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1185   score: 0.0   memory length: 100000   epsilon: 0.7587389800052375    steps: 124    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1186   score: 2.0   memory length: 100000   epsilon: 0.7583410000052462    steps: 201    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1187   score: 3.0   memory length: 100000   epsilon: 0.757887580005256    steps: 229    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1188   score: 2.0   memory length: 100000   epsilon: 0.7574539600052654    steps: 219    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1189   score: 4.0   memory length: 100000   epsilon: 0.7569035200052774    steps: 278    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1190   score: 1.0   memory length: 100000   epsilon: 0.756600580005284    steps: 153    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1191   score: 4.0   memory length: 100000   epsilon: 0.756091720005295    steps: 257    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1192   score: 4.0   memory length: 100000   epsilon: 0.7555115800053076    steps: 293    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1193   score: 1.0   memory length: 100000   epsilon: 0.7552106200053141    steps: 152    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1194   score: 3.0   memory length: 100000   epsilon: 0.7547908600053232    steps: 212    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1195   score: 6.0   memory length: 100000   epsilon: 0.7541216200053378    steps: 338    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1196   score: 5.0   memory length: 100000   epsilon: 0.7534741600053518    steps: 327    lr: 4e-05     evaluation reward: 2.37\n",
      "episode: 1197   score: 1.0   memory length: 100000   epsilon: 0.7531395400053591    steps: 169    lr: 4e-05     evaluation reward: 2.38\n",
      "episode: 1198   score: 2.0   memory length: 100000   epsilon: 0.7526980000053687    steps: 223    lr: 4e-05     evaluation reward: 2.36\n",
      "episode: 1199   score: 2.0   memory length: 100000   epsilon: 0.7523059600053772    steps: 198    lr: 4e-05     evaluation reward: 2.37\n",
      "episode: 1200   score: 6.0   memory length: 100000   epsilon: 0.751622860005392    steps: 345    lr: 4e-05     evaluation reward: 2.38\n",
      "episode: 1201   score: 3.0   memory length: 100000   epsilon: 0.7511595400054021    steps: 234    lr: 4e-05     evaluation reward: 2.39\n",
      "episode: 1202   score: 1.0   memory length: 100000   epsilon: 0.7508585800054086    steps: 152    lr: 4e-05     evaluation reward: 2.4\n",
      "episode: 1203   score: 1.0   memory length: 100000   epsilon: 0.7505576200054151    steps: 152    lr: 4e-05     evaluation reward: 2.39\n",
      "episode: 1204   score: 1.0   memory length: 100000   epsilon: 0.7502566600054217    steps: 152    lr: 4e-05     evaluation reward: 2.4\n",
      "episode: 1205   score: 5.0   memory length: 100000   epsilon: 0.7496844400054341    steps: 289    lr: 4e-05     evaluation reward: 2.42\n",
      "episode: 1206   score: 1.0   memory length: 100000   epsilon: 0.7493834800054406    steps: 152    lr: 4e-05     evaluation reward: 2.42\n",
      "episode: 1207   score: 4.0   memory length: 100000   epsilon: 0.7488092800054531    steps: 290    lr: 4e-05     evaluation reward: 2.46\n",
      "episode: 1208   score: 3.0   memory length: 100000   epsilon: 0.7483835800054623    steps: 215    lr: 4e-05     evaluation reward: 2.48\n",
      "episode: 1209   score: 0.0   memory length: 100000   epsilon: 0.7481380600054677    steps: 124    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1210   score: 4.0   memory length: 100000   epsilon: 0.7475480200054805    steps: 298    lr: 4e-05     evaluation reward: 2.49\n",
      "episode: 1211   score: 0.0   memory length: 100000   epsilon: 0.7473025000054858    steps: 124    lr: 4e-05     evaluation reward: 2.47\n",
      "episode: 1212   score: 0.0   memory length: 100000   epsilon: 0.7470589600054911    steps: 123    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1213   score: 1.0   memory length: 100000   epsilon: 0.7467599800054976    steps: 151    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1214   score: 1.0   memory length: 100000   epsilon: 0.7464610000055041    steps: 151    lr: 4e-05     evaluation reward: 2.44\n",
      "episode: 1215   score: 1.0   memory length: 100000   epsilon: 0.7461600400055106    steps: 152    lr: 4e-05     evaluation reward: 2.43\n",
      "episode: 1216   score: 2.0   memory length: 100000   epsilon: 0.7457204800055202    steps: 222    lr: 4e-05     evaluation reward: 2.42\n",
      "episode: 1217   score: 5.0   memory length: 100000   epsilon: 0.7450690600055343    steps: 329    lr: 4e-05     evaluation reward: 2.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1218   score: 2.0   memory length: 100000   epsilon: 0.7447087000055421    steps: 182    lr: 4e-05     evaluation reward: 2.47\n",
      "episode: 1219   score: 2.0   memory length: 100000   epsilon: 0.7443146800055507    steps: 199    lr: 4e-05     evaluation reward: 2.49\n",
      "episode: 1220   score: 2.0   memory length: 100000   epsilon: 0.74388304000556    steps: 218    lr: 4e-05     evaluation reward: 2.49\n",
      "episode: 1221   score: 6.0   memory length: 100000   epsilon: 0.7431346000055763    steps: 378    lr: 4e-05     evaluation reward: 2.53\n",
      "episode: 1222   score: 3.0   memory length: 100000   epsilon: 0.742641580005587    steps: 249    lr: 4e-05     evaluation reward: 2.54\n",
      "episode: 1223   score: 4.0   memory length: 100000   epsilon: 0.742087180005599    steps: 280    lr: 4e-05     evaluation reward: 2.53\n",
      "episode: 1224   score: 3.0   memory length: 100000   epsilon: 0.7416377200056088    steps: 227    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1225   score: 1.0   memory length: 100000   epsilon: 0.7413011200056161    steps: 170    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1226   score: 2.0   memory length: 100000   epsilon: 0.740938780005624    steps: 183    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1227   score: 5.0   memory length: 100000   epsilon: 0.7402517200056389    steps: 347    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1228   score: 5.0   memory length: 100000   epsilon: 0.7396082200056528    steps: 325    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1229   score: 2.0   memory length: 100000   epsilon: 0.7392082600056615    steps: 202    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1230   score: 3.0   memory length: 100000   epsilon: 0.7387152400056722    steps: 249    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1231   score: 3.0   memory length: 100000   epsilon: 0.738265780005682    steps: 227    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1232   score: 3.0   memory length: 100000   epsilon: 0.7377767200056926    steps: 247    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1233   score: 7.0   memory length: 100000   epsilon: 0.7369728400057101    steps: 406    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1234   score: 4.0   memory length: 100000   epsilon: 0.7364521000057214    steps: 263    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1235   score: 4.0   memory length: 100000   epsilon: 0.7359333400057326    steps: 262    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1236   score: 2.0   memory length: 100000   epsilon: 0.735499720005742    steps: 219    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1237   score: 2.0   memory length: 100000   epsilon: 0.7351393600057499    steps: 182    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1238   score: 5.0   memory length: 100000   epsilon: 0.734485960005764    steps: 330    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1239   score: 4.0   memory length: 100000   epsilon: 0.7339533400057756    steps: 269    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1240   score: 2.0   memory length: 100000   epsilon: 0.7335553600057843    steps: 201    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1241   score: 3.0   memory length: 100000   epsilon: 0.7331375800057933    steps: 211    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1242   score: 3.0   memory length: 100000   epsilon: 0.7327178200058024    steps: 212    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1243   score: 1.0   memory length: 100000   epsilon: 0.7323812200058097    steps: 170    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1244   score: 3.0   memory length: 100000   epsilon: 0.7319594800058189    steps: 213    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1245   score: 1.0   memory length: 100000   epsilon: 0.7316209000058262    steps: 171    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1246   score: 2.0   memory length: 100000   epsilon: 0.7311932200058355    steps: 216    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1247   score: 3.0   memory length: 100000   epsilon: 0.7307437600058453    steps: 227    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1248   score: 2.0   memory length: 100000   epsilon: 0.7303398400058541    steps: 204    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1249   score: 1.0   memory length: 100000   epsilon: 0.7299973000058615    steps: 173    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1250   score: 0.0   memory length: 100000   epsilon: 0.7297537600058668    steps: 123    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1251   score: 2.0   memory length: 100000   epsilon: 0.7293241000058761    steps: 217    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1252   score: 1.0   memory length: 100000   epsilon: 0.7290231400058826    steps: 152    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1253   score: 4.0   memory length: 100000   epsilon: 0.7284707200058946    steps: 279    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1254   score: 6.0   memory length: 100000   epsilon: 0.7277995000059092    steps: 339    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1255   score: 2.0   memory length: 100000   epsilon: 0.7273678600059186    steps: 218    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1256   score: 3.0   memory length: 100000   epsilon: 0.7269184000059283    steps: 227    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1257   score: 3.0   memory length: 100000   epsilon: 0.7264313200059389    steps: 246    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1258   score: 6.0   memory length: 100000   epsilon: 0.7256175400059566    steps: 411    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1259   score: 3.0   memory length: 100000   epsilon: 0.725090860005968    steps: 266    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1260   score: 3.0   memory length: 100000   epsilon: 0.7245978400059787    steps: 249    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1261   score: 5.0   memory length: 100000   epsilon: 0.7239523600059927    steps: 326    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1262   score: 6.0   memory length: 100000   epsilon: 0.7232554000060079    steps: 352    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1263   score: 1.0   memory length: 100000   epsilon: 0.7229544400060144    steps: 152    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1264   score: 2.0   memory length: 100000   epsilon: 0.7225921000060223    steps: 183    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1265   score: 2.0   memory length: 100000   epsilon: 0.7221941200060309    steps: 201    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1266   score: 5.0   memory length: 100000   epsilon: 0.7216219000060433    steps: 289    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1267   score: 1.0   memory length: 100000   epsilon: 0.7213209400060498    steps: 152    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1268   score: 3.0   memory length: 100000   epsilon: 0.7208734600060596    steps: 226    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1269   score: 4.0   memory length: 100000   epsilon: 0.7202854000060723    steps: 297    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1270   score: 5.0   memory length: 100000   epsilon: 0.7196656600060858    steps: 313    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1271   score: 3.0   memory length: 100000   epsilon: 0.7192459000060949    steps: 212    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1272   score: 3.0   memory length: 100000   epsilon: 0.7187865400061049    steps: 232    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1273   score: 3.0   memory length: 100000   epsilon: 0.718364800006114    steps: 213    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1274   score: 3.0   memory length: 100000   epsilon: 0.7178757400061246    steps: 247    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1275   score: 4.0   memory length: 100000   epsilon: 0.7172441200061384    steps: 319    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1276   score: 8.0   memory length: 100000   epsilon: 0.7164046000061566    steps: 424    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1277   score: 3.0   memory length: 100000   epsilon: 0.7159492000061665    steps: 230    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1278   score: 1.0   memory length: 100000   epsilon: 0.715648240006173    steps: 152    lr: 4e-05     evaluation reward: 2.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1279   score: 1.0   memory length: 100000   epsilon: 0.7153057000061804    steps: 173    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1280   score: 3.0   memory length: 100000   epsilon: 0.714816640006191    steps: 247    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1281   score: 1.0   memory length: 100000   epsilon: 0.7144760800061984    steps: 172    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1282   score: 2.0   memory length: 100000   epsilon: 0.7141177000062062    steps: 181    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1283   score: 1.0   memory length: 100000   epsilon: 0.7138167400062128    steps: 152    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1284   score: 3.0   memory length: 100000   epsilon: 0.7133276800062234    steps: 247    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1285   score: 0.0   memory length: 100000   epsilon: 0.7130821600062287    steps: 124    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1286   score: 3.0   memory length: 100000   epsilon: 0.7126327000062385    steps: 227    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1287   score: 3.0   memory length: 100000   epsilon: 0.7121852200062482    steps: 226    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1288   score: 2.0   memory length: 100000   epsilon: 0.711822880006256    steps: 183    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1289   score: 0.0   memory length: 100000   epsilon: 0.7115773600062614    steps: 124    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1290   score: 3.0   memory length: 100000   epsilon: 0.7111239400062712    steps: 229    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1291   score: 3.0   memory length: 100000   epsilon: 0.7106685400062811    steps: 230    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1292   score: 2.0   memory length: 100000   epsilon: 0.710306200006289    steps: 183    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1293   score: 3.0   memory length: 100000   epsilon: 0.7098547600062988    steps: 228    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1294   score: 2.0   memory length: 100000   epsilon: 0.7094607400063073    steps: 199    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1295   score: 6.0   memory length: 100000   epsilon: 0.7086786400063243    steps: 395    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1296   score: 3.0   memory length: 100000   epsilon: 0.7081895800063349    steps: 247    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1297   score: 5.0   memory length: 100000   epsilon: 0.7075282600063493    steps: 334    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1298   score: 1.0   memory length: 100000   epsilon: 0.7071916600063566    steps: 170    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1299   score: 4.0   memory length: 100000   epsilon: 0.7066729000063678    steps: 262    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1300   score: 1.0   memory length: 100000   epsilon: 0.7063343200063752    steps: 171    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1301   score: 1.0   memory length: 100000   epsilon: 0.7060313800063818    steps: 153    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1302   score: 5.0   memory length: 100000   epsilon: 0.7053562000063964    steps: 341    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1303   score: 3.0   memory length: 100000   epsilon: 0.7049027800064063    steps: 229    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1304   score: 3.0   memory length: 100000   epsilon: 0.7044790600064155    steps: 214    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1305   score: 7.0   memory length: 100000   epsilon: 0.7039900000064261    steps: 247    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1306   score: 1.0   memory length: 100000   epsilon: 0.7036890400064326    steps: 152    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1307   score: 2.0   memory length: 100000   epsilon: 0.7032950200064412    steps: 199    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1308   score: 1.0   memory length: 100000   epsilon: 0.7029940600064477    steps: 152    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1309   score: 3.0   memory length: 100000   epsilon: 0.7025465800064574    steps: 226    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1310   score: 2.0   memory length: 100000   epsilon: 0.7021842400064653    steps: 183    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1311   score: 1.0   memory length: 100000   epsilon: 0.7018417000064727    steps: 173    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1312   score: 1.0   memory length: 100000   epsilon: 0.7015011400064801    steps: 172    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1313   score: 2.0   memory length: 100000   epsilon: 0.7011031600064888    steps: 201    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1314   score: 0.0   memory length: 100000   epsilon: 0.700859620006494    steps: 123    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1315   score: 5.0   memory length: 100000   epsilon: 0.700170580006509    steps: 348    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1316   score: 1.0   memory length: 100000   epsilon: 0.6998716000065155    steps: 151    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1317   score: 1.0   memory length: 100000   epsilon: 0.699572620006522    steps: 151    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1318   score: 5.0   memory length: 100000   epsilon: 0.6989667400065351    steps: 306    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1319   score: 5.0   memory length: 100000   epsilon: 0.6982876000065499    steps: 343    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1320   score: 4.0   memory length: 100000   epsilon: 0.6976916200065628    steps: 301    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1321   score: 3.0   memory length: 100000   epsilon: 0.6972718600065719    steps: 212    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1322   score: 3.0   memory length: 100000   epsilon: 0.6968164600065818    steps: 230    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1323   score: 5.0   memory length: 100000   epsilon: 0.6961690000065959    steps: 327    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1324   score: 2.0   memory length: 100000   epsilon: 0.6957749800066044    steps: 199    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1325   score: 6.0   memory length: 100000   epsilon: 0.6950305000066206    steps: 376    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1326   score: 7.0   memory length: 100000   epsilon: 0.6942543400066374    steps: 392    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1327   score: 1.0   memory length: 100000   epsilon: 0.6939157600066448    steps: 171    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1328   score: 2.0   memory length: 100000   epsilon: 0.6935554000066526    steps: 182    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1329   score: 3.0   memory length: 100000   epsilon: 0.6930584200066634    steps: 251    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1330   score: 1.0   memory length: 100000   epsilon: 0.6927594400066699    steps: 151    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1331   score: 3.0   memory length: 100000   epsilon: 0.6922703800066805    steps: 247    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1332   score: 3.0   memory length: 100000   epsilon: 0.6917833000066911    steps: 246    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1333   score: 0.0   memory length: 100000   epsilon: 0.6915397600066964    steps: 123    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1334   score: 6.0   memory length: 100000   epsilon: 0.6908428000067115    steps: 352    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1335   score: 2.0   memory length: 100000   epsilon: 0.6904804600067194    steps: 183    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1336   score: 1.0   memory length: 100000   epsilon: 0.6901399000067268    steps: 172    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1337   score: 3.0   memory length: 100000   epsilon: 0.6896825200067367    steps: 231    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1338   score: 4.0   memory length: 100000   epsilon: 0.6890944600067495    steps: 297    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1339   score: 2.0   memory length: 100000   epsilon: 0.6886964800067581    steps: 201    lr: 4e-05     evaluation reward: 2.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1340   score: 3.0   memory length: 100000   epsilon: 0.6882430600067679    steps: 229    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1341   score: 5.0   memory length: 100000   epsilon: 0.6875995600067819    steps: 325    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1342   score: 1.0   memory length: 100000   epsilon: 0.6872590000067893    steps: 172    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1343   score: 3.0   memory length: 100000   epsilon: 0.6868075600067991    steps: 228    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1344   score: 2.0   memory length: 100000   epsilon: 0.6864491800068069    steps: 181    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1345   score: 1.0   memory length: 100000   epsilon: 0.6861086200068143    steps: 172    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1346   score: 0.0   memory length: 100000   epsilon: 0.6858631000068196    steps: 124    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1347   score: 2.0   memory length: 100000   epsilon: 0.685429480006829    steps: 219    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1348   score: 2.0   memory length: 100000   epsilon: 0.6849899200068386    steps: 222    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1349   score: 4.0   memory length: 100000   epsilon: 0.684510760006849    steps: 242    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1350   score: 3.0   memory length: 100000   epsilon: 0.6840197200068596    steps: 248    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1351   score: 1.0   memory length: 100000   epsilon: 0.6837187600068662    steps: 152    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1352   score: 0.0   memory length: 100000   epsilon: 0.6834732400068715    steps: 124    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1353   score: 1.0   memory length: 100000   epsilon: 0.683172280006878    steps: 152    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1354   score: 2.0   memory length: 100000   epsilon: 0.6828139000068858    steps: 181    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1355   score: 3.0   memory length: 100000   epsilon: 0.6823644400068956    steps: 227    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1356   score: 3.0   memory length: 100000   epsilon: 0.6819050800069055    steps: 232    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1357   score: 2.0   memory length: 100000   epsilon: 0.6815110600069141    steps: 199    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1358   score: 6.0   memory length: 100000   epsilon: 0.6807626200069303    steps: 378    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1359   score: 2.0   memory length: 100000   epsilon: 0.680362660006939    steps: 202    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1360   score: 1.0   memory length: 100000   epsilon: 0.6800636800069455    steps: 151    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1361   score: 2.0   memory length: 100000   epsilon: 0.6796696600069541    steps: 199    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1362   score: 2.0   memory length: 100000   epsilon: 0.6793132600069618    steps: 180    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1363   score: 1.0   memory length: 100000   epsilon: 0.6789707200069692    steps: 173    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1364   score: 1.0   memory length: 100000   epsilon: 0.6786717400069757    steps: 151    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1365   score: 2.0   memory length: 100000   epsilon: 0.6782777200069843    steps: 199    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1366   score: 4.0   memory length: 100000   epsilon: 0.6777253000069963    steps: 279    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1367   score: 2.0   memory length: 100000   epsilon: 0.6773332600070048    steps: 198    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1368   score: 2.0   memory length: 100000   epsilon: 0.6769748800070126    steps: 181    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1369   score: 2.0   memory length: 100000   epsilon: 0.6765808600070211    steps: 199    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1370   score: 3.0   memory length: 100000   epsilon: 0.6761333800070308    steps: 226    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1371   score: 3.0   memory length: 100000   epsilon: 0.6757136200070399    steps: 212    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1372   score: 5.0   memory length: 100000   epsilon: 0.6750740800070538    steps: 323    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1373   score: 1.0   memory length: 100000   epsilon: 0.6747374800070611    steps: 170    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1374   score: 1.0   memory length: 100000   epsilon: 0.6743949400070686    steps: 173    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1375   score: 3.0   memory length: 100000   epsilon: 0.6739751800070777    steps: 212    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1376   score: 2.0   memory length: 100000   epsilon: 0.6735811600070862    steps: 199    lr: 4e-05     evaluation reward: 2.49\n",
      "episode: 1377   score: 3.0   memory length: 100000   epsilon: 0.673131700007096    steps: 227    lr: 4e-05     evaluation reward: 2.49\n",
      "episode: 1378   score: 2.0   memory length: 100000   epsilon: 0.6727753000071037    steps: 180    lr: 4e-05     evaluation reward: 2.5\n",
      "episode: 1379   score: 3.0   memory length: 100000   epsilon: 0.6723218800071136    steps: 229    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1380   score: 5.0   memory length: 100000   epsilon: 0.6716467000071282    steps: 341    lr: 4e-05     evaluation reward: 2.54\n",
      "episode: 1381   score: 1.0   memory length: 100000   epsilon: 0.6713081200071356    steps: 171    lr: 4e-05     evaluation reward: 2.54\n",
      "episode: 1382   score: 4.0   memory length: 100000   epsilon: 0.6708329200071459    steps: 240    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1383   score: 2.0   memory length: 100000   epsilon: 0.6704745400071537    steps: 181    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1384   score: 4.0   memory length: 100000   epsilon: 0.6699221200071657    steps: 279    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1385   score: 4.0   memory length: 100000   epsilon: 0.6693578200071779    steps: 285    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1386   score: 3.0   memory length: 100000   epsilon: 0.6689321200071872    steps: 215    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1387   score: 1.0   memory length: 100000   epsilon: 0.6685935400071945    steps: 171    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1388   score: 4.0   memory length: 100000   epsilon: 0.6680470600072064    steps: 276    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1389   score: 1.0   memory length: 100000   epsilon: 0.6677461000072129    steps: 152    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1390   score: 1.0   memory length: 100000   epsilon: 0.6674055400072203    steps: 172    lr: 4e-05     evaluation reward: 2.61\n",
      "episode: 1391   score: 2.0   memory length: 100000   epsilon: 0.6670115200072289    steps: 199    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1392   score: 2.0   memory length: 100000   epsilon: 0.6666175000072374    steps: 199    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1393   score: 1.0   memory length: 100000   epsilon: 0.6663165400072439    steps: 152    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1394   score: 0.0   memory length: 100000   epsilon: 0.6660730000072492    steps: 123    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1395   score: 1.0   memory length: 100000   epsilon: 0.6657720400072558    steps: 152    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1396   score: 4.0   memory length: 100000   epsilon: 0.6652156600072678    steps: 281    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1397   score: 3.0   memory length: 100000   epsilon: 0.6647978800072769    steps: 211    lr: 4e-05     evaluation reward: 2.5\n",
      "episode: 1398   score: 4.0   memory length: 100000   epsilon: 0.6643207000072873    steps: 241    lr: 4e-05     evaluation reward: 2.53\n",
      "episode: 1399   score: 2.0   memory length: 100000   epsilon: 0.663918760007296    steps: 203    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1400   score: 8.0   memory length: 100000   epsilon: 0.6630198400073155    steps: 454    lr: 4e-05     evaluation reward: 2.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1401   score: 1.0   memory length: 100000   epsilon: 0.662718880007322    steps: 152    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1402   score: 1.0   memory length: 100000   epsilon: 0.6624199000073285    steps: 151    lr: 4e-05     evaluation reward: 2.54\n",
      "episode: 1403   score: 7.0   memory length: 100000   epsilon: 0.6616180000073459    steps: 405    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1404   score: 3.0   memory length: 100000   epsilon: 0.661198240007355    steps: 212    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1405   score: 1.0   memory length: 100000   epsilon: 0.6608972800073616    steps: 152    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1406   score: 5.0   memory length: 100000   epsilon: 0.6602933800073747    steps: 305    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1407   score: 3.0   memory length: 100000   epsilon: 0.6598736200073838    steps: 212    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1408   score: 6.0   memory length: 100000   epsilon: 0.6592004200073984    steps: 340    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1409   score: 5.0   memory length: 100000   epsilon: 0.6585886000074117    steps: 309    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1410   score: 3.0   memory length: 100000   epsilon: 0.6580559800074233    steps: 269    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1411   score: 0.0   memory length: 100000   epsilon: 0.6578124400074286    steps: 123    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1412   score: 4.0   memory length: 100000   epsilon: 0.6572323000074412    steps: 293    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1413   score: 3.0   memory length: 100000   epsilon: 0.6567670000074513    steps: 235    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1414   score: 0.0   memory length: 100000   epsilon: 0.6565234600074565    steps: 123    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1415   score: 2.0   memory length: 100000   epsilon: 0.656089840007466    steps: 219    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1416   score: 3.0   memory length: 100000   epsilon: 0.6556344400074758    steps: 230    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1417   score: 2.0   memory length: 100000   epsilon: 0.6552404200074844    steps: 199    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1418   score: 4.0   memory length: 100000   epsilon: 0.6546899800074963    steps: 278    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1419   score: 1.0   memory length: 100000   epsilon: 0.6543910000075028    steps: 151    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1420   score: 0.0   memory length: 100000   epsilon: 0.6541474600075081    steps: 123    lr: 4e-05     evaluation reward: 2.59\n",
      "episode: 1421   score: 3.0   memory length: 100000   epsilon: 0.6536980000075179    steps: 227    lr: 4e-05     evaluation reward: 2.59\n",
      "episode: 1422   score: 1.0   memory length: 100000   epsilon: 0.6533970400075244    steps: 152    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1423   score: 3.0   memory length: 100000   epsilon: 0.6529495600075341    steps: 226    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1424   score: 1.0   memory length: 100000   epsilon: 0.6526486000075407    steps: 152    lr: 4e-05     evaluation reward: 2.54\n",
      "episode: 1425   score: 4.0   memory length: 100000   epsilon: 0.6520664800075533    steps: 294    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1426   score: 3.0   memory length: 100000   epsilon: 0.651617020007563    steps: 227    lr: 4e-05     evaluation reward: 2.48\n",
      "episode: 1427   score: 2.0   memory length: 100000   epsilon: 0.6511834000075725    steps: 219    lr: 4e-05     evaluation reward: 2.49\n",
      "episode: 1428   score: 4.0   memory length: 100000   epsilon: 0.6506903800075832    steps: 249    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1429   score: 1.0   memory length: 100000   epsilon: 0.6503537800075905    steps: 170    lr: 4e-05     evaluation reward: 2.49\n",
      "episode: 1430   score: 3.0   memory length: 100000   epsilon: 0.6499300600075997    steps: 214    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1431   score: 5.0   memory length: 100000   epsilon: 0.6492628000076142    steps: 337    lr: 4e-05     evaluation reward: 2.53\n",
      "episode: 1432   score: 2.0   memory length: 100000   epsilon: 0.648900460007622    steps: 183    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1433   score: 4.0   memory length: 100000   epsilon: 0.6484173400076325    steps: 244    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1434   score: 4.0   memory length: 100000   epsilon: 0.6478669000076445    steps: 278    lr: 4e-05     evaluation reward: 2.54\n",
      "episode: 1435   score: 6.0   memory length: 100000   epsilon: 0.6471640000076597    steps: 355    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1436   score: 3.0   memory length: 100000   epsilon: 0.6466690000076705    steps: 250    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1437   score: 3.0   memory length: 100000   epsilon: 0.6461799400076811    steps: 247    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1438   score: 3.0   memory length: 100000   epsilon: 0.6456908800076917    steps: 247    lr: 4e-05     evaluation reward: 2.59\n",
      "episode: 1439   score: 5.0   memory length: 100000   epsilon: 0.6450077800077065    steps: 345    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1440   score: 5.0   memory length: 100000   epsilon: 0.6443702200077204    steps: 322    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1441   score: 2.0   memory length: 100000   epsilon: 0.6440078800077282    steps: 183    lr: 4e-05     evaluation reward: 2.61\n",
      "episode: 1442   score: 1.0   memory length: 100000   epsilon: 0.6436712800077355    steps: 170    lr: 4e-05     evaluation reward: 2.61\n",
      "episode: 1443   score: 5.0   memory length: 100000   epsilon: 0.6430238200077496    steps: 327    lr: 4e-05     evaluation reward: 2.63\n",
      "episode: 1444   score: 4.0   memory length: 100000   epsilon: 0.6424397200077623    steps: 295    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1445   score: 3.0   memory length: 100000   epsilon: 0.6420160000077715    steps: 214    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1446   score: 1.0   memory length: 100000   epsilon: 0.641715040007778    steps: 152    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1447   score: 3.0   memory length: 100000   epsilon: 0.6412933000077872    steps: 213    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1448   score: 2.0   memory length: 100000   epsilon: 0.6409012600077957    steps: 198    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1449   score: 5.0   memory length: 100000   epsilon: 0.6402577600078097    steps: 325    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1450   score: 2.0   memory length: 100000   epsilon: 0.6398617600078182    steps: 200    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1451   score: 2.0   memory length: 100000   epsilon: 0.639505360007826    steps: 180    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1452   score: 4.0   memory length: 100000   epsilon: 0.6389905600078372    steps: 260    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1453   score: 2.0   memory length: 100000   epsilon: 0.6385925800078458    steps: 201    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1454   score: 2.0   memory length: 100000   epsilon: 0.6381985600078544    steps: 199    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1455   score: 3.0   memory length: 100000   epsilon: 0.637709500007865    steps: 247    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1456   score: 0.0   memory length: 100000   epsilon: 0.6374639800078703    steps: 124    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1457   score: 3.0   memory length: 100000   epsilon: 0.6370085800078802    steps: 230    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1458   score: 1.0   memory length: 100000   epsilon: 0.6367076200078867    steps: 152    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1459   score: 1.0   memory length: 100000   epsilon: 0.636371020007894    steps: 170    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1460   score: 3.0   memory length: 100000   epsilon: 0.635913640007904    steps: 231    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1461   score: 3.0   memory length: 100000   epsilon: 0.6354641800079137    steps: 227    lr: 4e-05     evaluation reward: 2.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1462   score: 6.0   memory length: 100000   epsilon: 0.6346523800079313    steps: 410    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1463   score: 3.0   memory length: 100000   epsilon: 0.6342029200079411    steps: 227    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1464   score: 1.0   memory length: 100000   epsilon: 0.6339019600079476    steps: 152    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1465   score: 3.0   memory length: 100000   epsilon: 0.6334426000079576    steps: 232    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1466   score: 5.0   memory length: 100000   epsilon: 0.6327951400079717    steps: 327    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1467   score: 4.0   memory length: 100000   epsilon: 0.6322090600079844    steps: 296    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1468   score: 11.0   memory length: 100000   epsilon: 0.6314824000080002    steps: 367    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1469   score: 2.0   memory length: 100000   epsilon: 0.631120060008008    steps: 183    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1470   score: 3.0   memory length: 100000   epsilon: 0.6306963400080172    steps: 214    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1471   score: 4.0   memory length: 100000   epsilon: 0.6301459000080292    steps: 278    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1472   score: 4.0   memory length: 100000   epsilon: 0.6295954600080411    steps: 278    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1473   score: 4.0   memory length: 100000   epsilon: 0.629096500008052    steps: 252    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1474   score: 5.0   memory length: 100000   epsilon: 0.6284173600080667    steps: 343    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1475   score: 2.0   memory length: 100000   epsilon: 0.6279778000080762    steps: 222    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1476   score: 4.0   memory length: 100000   epsilon: 0.6274729000080872    steps: 255    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1477   score: 5.0   memory length: 100000   epsilon: 0.6268709800081003    steps: 304    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1478   score: 4.0   memory length: 100000   epsilon: 0.6263245000081121    steps: 276    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1479   score: 3.0   memory length: 100000   epsilon: 0.6258572200081223    steps: 236    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1480   score: 2.0   memory length: 100000   epsilon: 0.6254651800081308    steps: 198    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1481   score: 1.0   memory length: 100000   epsilon: 0.6251266000081381    steps: 171    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1482   score: 1.0   memory length: 100000   epsilon: 0.6248256400081447    steps: 152    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1483   score: 4.0   memory length: 100000   epsilon: 0.624209860008158    steps: 311    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1484   score: 3.0   memory length: 100000   epsilon: 0.6237861400081672    steps: 214    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1485   score: 3.0   memory length: 100000   epsilon: 0.623336680008177    steps: 227    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1486   score: 5.0   memory length: 100000   epsilon: 0.6227090200081906    steps: 317    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1487   score: 6.0   memory length: 100000   epsilon: 0.6219209800082077    steps: 398    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1488   score: 1.0   memory length: 100000   epsilon: 0.6216200200082143    steps: 152    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1489   score: 3.0   memory length: 100000   epsilon: 0.6211626400082242    steps: 231    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1490   score: 4.0   memory length: 100000   epsilon: 0.6206122000082361    steps: 278    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1491   score: 3.0   memory length: 100000   epsilon: 0.6201904600082453    steps: 213    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1492   score: 6.0   memory length: 100000   epsilon: 0.6194083600082623    steps: 395    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1493   score: 4.0   memory length: 100000   epsilon: 0.6188876200082736    steps: 263    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1494   score: 5.0   memory length: 100000   epsilon: 0.6182282800082879    steps: 333    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1495   score: 0.0   memory length: 100000   epsilon: 0.6179827600082932    steps: 124    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1496   score: 3.0   memory length: 100000   epsilon: 0.6175234000083032    steps: 232    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1497   score: 2.0   memory length: 100000   epsilon: 0.6171254200083118    steps: 201    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1498   score: 2.0   memory length: 100000   epsilon: 0.6167630800083197    steps: 183    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1499   score: 4.0   memory length: 100000   epsilon: 0.6162166000083316    steps: 276    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1500   score: 5.0   memory length: 100000   epsilon: 0.6155354800083463    steps: 344    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1501   score: 6.0   memory length: 100000   epsilon: 0.6147454600083635    steps: 399    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1502   score: 3.0   memory length: 100000   epsilon: 0.6142960000083733    steps: 227    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1503   score: 6.0   memory length: 100000   epsilon: 0.6135911200083886    steps: 356    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1504   score: 4.0   memory length: 100000   epsilon: 0.6130763200083997    steps: 260    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1505   score: 3.0   memory length: 100000   epsilon: 0.6125872600084104    steps: 247    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1506   score: 3.0   memory length: 100000   epsilon: 0.6121338400084202    steps: 229    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1507   score: 1.0   memory length: 100000   epsilon: 0.6118328800084267    steps: 152    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1508   score: 3.0   memory length: 100000   epsilon: 0.6113854000084364    steps: 226    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1509   score: 5.0   memory length: 100000   epsilon: 0.6107339800084506    steps: 329    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1510   score: 7.0   memory length: 100000   epsilon: 0.609934060008468    steps: 404    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1511   score: 6.0   memory length: 100000   epsilon: 0.6091895800084841    steps: 376    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1512   score: 2.0   memory length: 100000   epsilon: 0.6087955600084927    steps: 199    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1513   score: 4.0   memory length: 100000   epsilon: 0.6082451200085046    steps: 278    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1514   score: 1.0   memory length: 100000   epsilon: 0.6079441600085111    steps: 152    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1515   score: 5.0   memory length: 100000   epsilon: 0.6072313600085266    steps: 360    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1516   score: 3.0   memory length: 100000   epsilon: 0.6067819000085364    steps: 227    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1517   score: 5.0   memory length: 100000   epsilon: 0.6061384000085503    steps: 325    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1518   score: 1.0   memory length: 100000   epsilon: 0.6058394200085568    steps: 151    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1519   score: 3.0   memory length: 100000   epsilon: 0.6053820400085668    steps: 231    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1520   score: 0.0   memory length: 100000   epsilon: 0.6051365200085721    steps: 124    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1521   score: 2.0   memory length: 100000   epsilon: 0.6047425000085807    steps: 199    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1522   score: 2.0   memory length: 100000   epsilon: 0.6043841200085884    steps: 181    lr: 4e-05     evaluation reward: 3.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1523   score: 2.0   memory length: 100000   epsilon: 0.6039920800085969    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.27\n",
      "episode: 1524   score: 4.0   memory length: 100000   epsilon: 0.603485200008608    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1525   score: 5.0   memory length: 100000   epsilon: 0.6028436800086219    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1526   score: 4.0   memory length: 100000   epsilon: 0.6022932400086338    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1527   score: 3.0   memory length: 100000   epsilon: 0.6018022000086445    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1528   score: 3.0   memory length: 100000   epsilon: 0.6013527400086542    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1529   score: 1.0   memory length: 100000   epsilon: 0.6010517800086608    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1530   score: 4.0   memory length: 100000   epsilon: 0.600536980008672    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1531   score: 5.0   memory length: 100000   epsilon: 0.5998598200086867    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1532   score: 3.0   memory length: 100000   epsilon: 0.5993707600086973    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1533   score: 5.0   memory length: 100000   epsilon: 0.5988104200087094    steps: 283    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1534   score: 3.0   memory length: 100000   epsilon: 0.5983530400087194    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1535   score: 2.0   memory length: 100000   epsilon: 0.5979946600087271    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1536   score: 4.0   memory length: 100000   epsilon: 0.5974679800087386    steps: 266    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1537   score: 4.0   memory length: 100000   epsilon: 0.596986840008749    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1538   score: 10.0   memory length: 100000   epsilon: 0.5961532600087671    steps: 421    lr: 1.6000000000000003e-05     evaluation reward: 3.39\n",
      "episode: 1539   score: 4.0   memory length: 100000   epsilon: 0.595606780008779    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.38\n",
      "episode: 1540   score: 3.0   memory length: 100000   epsilon: 0.5951098000087898    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 3.36\n",
      "episode: 1541   score: 4.0   memory length: 100000   epsilon: 0.5945237200088025    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.38\n",
      "episode: 1542   score: 3.0   memory length: 100000   epsilon: 0.5941079200088115    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 3.4\n",
      "episode: 1543   score: 6.0   memory length: 100000   epsilon: 0.5934188800088265    steps: 348    lr: 1.6000000000000003e-05     evaluation reward: 3.41\n",
      "episode: 1544   score: 4.0   memory length: 100000   epsilon: 0.5929219000088373    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 3.41\n",
      "episode: 1545   score: 2.0   memory length: 100000   epsilon: 0.592565500008845    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.4\n",
      "episode: 1546   score: 3.0   memory length: 100000   epsilon: 0.5920724800088557    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 3.42\n",
      "episode: 1547   score: 4.0   memory length: 100000   epsilon: 0.591551740008867    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 3.43\n",
      "episode: 1548   score: 2.0   memory length: 100000   epsilon: 0.5911201000088764    steps: 218    lr: 1.6000000000000003e-05     evaluation reward: 3.43\n",
      "episode: 1549   score: 4.0   memory length: 100000   epsilon: 0.5905696600088883    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.42\n",
      "episode: 1550   score: 4.0   memory length: 100000   epsilon: 0.589983580008901    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.44\n",
      "episode: 1551   score: 3.0   memory length: 100000   epsilon: 0.5894925400089117    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 3.45\n",
      "episode: 1552   score: 3.0   memory length: 100000   epsilon: 0.5890688200089209    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.44\n",
      "episode: 1553   score: 6.0   memory length: 100000   epsilon: 0.588280780008938    steps: 398    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1554   score: 4.0   memory length: 100000   epsilon: 0.5877224200089501    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1555   score: 4.0   memory length: 100000   epsilon: 0.587173960008962    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1556   score: 3.0   memory length: 100000   epsilon: 0.5866888600089726    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1557   score: 5.0   memory length: 100000   epsilon: 0.5860631800089862    steps: 316    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1558   score: 4.0   memory length: 100000   epsilon: 0.585518680008998    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1559   score: 3.0   memory length: 100000   epsilon: 0.5850692200090077    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1560   score: 3.0   memory length: 100000   epsilon: 0.5845861000090182    steps: 244    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1561   score: 3.0   memory length: 100000   epsilon: 0.5841683200090273    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1562   score: 6.0   memory length: 100000   epsilon: 0.5834971000090419    steps: 339    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1563   score: 3.0   memory length: 100000   epsilon: 0.5830813000090509    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1564   score: 7.0   memory length: 100000   epsilon: 0.5822437600090691    steps: 423    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1565   score: 5.0   memory length: 100000   epsilon: 0.5816675800090816    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1566   score: 3.0   memory length: 100000   epsilon: 0.5812181200090913    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1567   score: 4.0   memory length: 100000   epsilon: 0.5806657000091033    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1568   score: 1.0   memory length: 100000   epsilon: 0.5803647400091099    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1569   score: 2.0   memory length: 100000   epsilon: 0.5799707200091184    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1570   score: 4.0   memory length: 100000   epsilon: 0.5793747400091314    steps: 301    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1571   score: 3.0   memory length: 100000   epsilon: 0.5789510200091406    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1572   score: 2.0   memory length: 100000   epsilon: 0.5785510600091492    steps: 202    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1573   score: 3.0   memory length: 100000   epsilon: 0.5781313000091584    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1574   score: 2.0   memory length: 100000   epsilon: 0.5776996600091677    steps: 218    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1575   score: 1.0   memory length: 100000   epsilon: 0.5773987000091743    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1576   score: 4.0   memory length: 100000   epsilon: 0.5768482600091862    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1577   score: 3.0   memory length: 100000   epsilon: 0.576353260009197    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1578   score: 5.0   memory length: 100000   epsilon: 0.5756998600092111    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1579   score: 3.0   memory length: 100000   epsilon: 0.5752424800092211    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1580   score: 4.0   memory length: 100000   epsilon: 0.5746504600092339    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1581   score: 3.0   memory length: 100000   epsilon: 0.5742010000092437    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1582   score: 3.0   memory length: 100000   epsilon: 0.5737495600092535    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1583   score: 1.0   memory length: 100000   epsilon: 0.57344860000926    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1584   score: 4.0   memory length: 100000   epsilon: 0.5728565800092729    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1585   score: 0.0   memory length: 100000   epsilon: 0.5726110600092782    steps: 124    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1586   score: 3.0   memory length: 100000   epsilon: 0.5721477400092883    steps: 234    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1587   score: 3.0   memory length: 100000   epsilon: 0.5716883800092982    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 3.45\n",
      "episode: 1588   score: 5.0   memory length: 100000   epsilon: 0.5710745800093115    steps: 310    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1589   score: 3.0   memory length: 100000   epsilon: 0.5706251200093213    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1590   score: 5.0   memory length: 100000   epsilon: 0.5699638000093357    steps: 334    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1591   score: 3.0   memory length: 100000   epsilon: 0.5694707800093464    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1592   score: 7.0   memory length: 100000   epsilon: 0.5686649200093639    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1593   score: 5.0   memory length: 100000   epsilon: 0.5679838000093786    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1594   score: 4.0   memory length: 100000   epsilon: 0.5674333600093906    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1595   score: 2.0   memory length: 100000   epsilon: 0.5670393400093992    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1596   score: 2.0   memory length: 100000   epsilon: 0.566677000009407    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1597   score: 4.0   memory length: 100000   epsilon: 0.5661206200094191    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1598   score: 1.0   memory length: 100000   epsilon: 0.5657800600094265    steps: 172    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1599   score: 3.0   memory length: 100000   epsilon: 0.5652890200094371    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1600   score: 4.0   memory length: 100000   epsilon: 0.5647029400094499    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1601   score: 6.0   memory length: 100000   epsilon: 0.5640000400094651    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1602   score: 4.0   memory length: 100000   epsilon: 0.5634773200094765    steps: 264    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1603   score: 5.0   memory length: 100000   epsilon: 0.5628397600094903    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1604   score: 2.0   memory length: 100000   epsilon: 0.5624813800094981    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1605   score: 2.0   memory length: 100000   epsilon: 0.5621230000095059    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1606   score: 6.0   memory length: 100000   epsilon: 0.5614537600095204    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1607   score: 2.0   memory length: 100000   epsilon: 0.5610953800095282    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1608   score: 0.0   memory length: 100000   epsilon: 0.5608518400095335    steps: 123    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1609   score: 6.0   memory length: 100000   epsilon: 0.5600756800095503    steps: 392    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1610   score: 4.0   memory length: 100000   epsilon: 0.5595608800095615    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.47\n",
      "episode: 1611   score: 2.0   memory length: 100000   epsilon: 0.55916686000957    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.43\n",
      "episode: 1612   score: 4.0   memory length: 100000   epsilon: 0.5585768200095829    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 3.45\n",
      "episode: 1613   score: 5.0   memory length: 100000   epsilon: 0.558018460009595    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1614   score: 2.0   memory length: 100000   epsilon: 0.5576224600096036    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 3.47\n",
      "episode: 1615   score: 5.0   memory length: 100000   epsilon: 0.5569948000096172    steps: 317    lr: 1.6000000000000003e-05     evaluation reward: 3.47\n",
      "episode: 1616   score: 2.0   memory length: 100000   epsilon: 0.5565968200096258    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1617   score: 4.0   memory length: 100000   epsilon: 0.5561156800096363    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 3.45\n",
      "episode: 1618   score: 4.0   memory length: 100000   epsilon: 0.555529600009649    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1619   score: 4.0   memory length: 100000   epsilon: 0.5550167800096601    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1620   score: 2.0   memory length: 100000   epsilon: 0.5546227600096687    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1621   score: 5.0   memory length: 100000   epsilon: 0.5539970800096823    steps: 316    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1622   score: 4.0   memory length: 100000   epsilon: 0.5534466400096942    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1623   score: 6.0   memory length: 100000   epsilon: 0.5527378000097096    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1624   score: 1.0   memory length: 100000   epsilon: 0.5524368400097162    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1625   score: 5.0   memory length: 100000   epsilon: 0.5518349200097292    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1626   score: 6.0   memory length: 100000   epsilon: 0.5510944000097453    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1627   score: 3.0   memory length: 100000   epsilon: 0.5506746400097544    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1628   score: 3.0   memory length: 100000   epsilon: 0.5502192400097643    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1629   score: 4.0   memory length: 100000   epsilon: 0.549633160009777    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1630   score: 5.0   memory length: 100000   epsilon: 0.5490589600097895    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 3.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1631   score: 3.0   memory length: 100000   epsilon: 0.5486055400097993    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1632   score: 7.0   memory length: 100000   epsilon: 0.5478551200098156    steps: 379    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1633   score: 5.0   memory length: 100000   epsilon: 0.5472116200098296    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1634   score: 4.0   memory length: 100000   epsilon: 0.5466948400098408    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1635   score: 3.0   memory length: 100000   epsilon: 0.54627112000985    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1636   score: 4.0   memory length: 100000   epsilon: 0.5456850400098627    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1637   score: 2.0   memory length: 100000   epsilon: 0.5453246800098706    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1638   score: 4.0   memory length: 100000   epsilon: 0.5447782000098824    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1639   score: 4.0   memory length: 100000   epsilon: 0.5442356800098942    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1640   score: 4.0   memory length: 100000   epsilon: 0.543691180009906    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1641   score: 5.0   memory length: 100000   epsilon: 0.5430437200099201    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1642   score: 2.0   memory length: 100000   epsilon: 0.5426813800099279    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1643   score: 3.0   memory length: 100000   epsilon: 0.5422299400099377    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1644   score: 4.0   memory length: 100000   epsilon: 0.5416399000099505    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1645   score: 3.0   memory length: 100000   epsilon: 0.5412221200099596    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1646   score: 2.0   memory length: 100000   epsilon: 0.5408281000099682    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1647   score: 3.0   memory length: 100000   epsilon: 0.5404083400099773    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1648   score: 8.0   memory length: 100000   epsilon: 0.5395628800099956    steps: 427    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1649   score: 3.0   memory length: 100000   epsilon: 0.5391134200100054    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1650   score: 4.0   memory length: 100000   epsilon: 0.5385966400100166    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1651   score: 3.0   memory length: 100000   epsilon: 0.5381412400100265    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1652   score: 2.0   memory length: 100000   epsilon: 0.5377393000100352    steps: 203    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1653   score: 2.0   memory length: 100000   epsilon: 0.5373452800100438    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1654   score: 4.0   memory length: 100000   epsilon: 0.5368700800100541    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1655   score: 3.0   memory length: 100000   epsilon: 0.5364523000100632    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1656   score: 2.0   memory length: 100000   epsilon: 0.5360543200100718    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1657   score: 3.0   memory length: 100000   epsilon: 0.5355989200100817    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1658   score: 3.0   memory length: 100000   epsilon: 0.5351098600100923    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1659   score: 3.0   memory length: 100000   epsilon: 0.5345851600101037    steps: 265    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1660   score: 6.0   memory length: 100000   epsilon: 0.5338901800101188    steps: 351    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1661   score: 4.0   memory length: 100000   epsilon: 0.5333041000101315    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1662   score: 5.0   memory length: 100000   epsilon: 0.5326190200101464    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1663   score: 5.0   memory length: 100000   epsilon: 0.5319874000101601    steps: 319    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1664   score: 6.0   memory length: 100000   epsilon: 0.5312429200101763    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1665   score: 7.0   memory length: 100000   epsilon: 0.5304944800101925    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1666   score: 1.0   memory length: 100000   epsilon: 0.5301539200101999    steps: 172    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1667   score: 1.0   memory length: 100000   epsilon: 0.5298193000102072    steps: 169    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1668   score: 9.0   memory length: 100000   epsilon: 0.5289164200102268    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1669   score: 3.0   memory length: 100000   epsilon: 0.5284570600102367    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1670   score: 3.0   memory length: 100000   epsilon: 0.5280016600102466    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1671   score: 2.0   memory length: 100000   epsilon: 0.5276076400102552    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1672   score: 3.0   memory length: 100000   epsilon: 0.527154220010265    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1673   score: 2.0   memory length: 100000   epsilon: 0.5267602000102736    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1674   score: 3.0   memory length: 100000   epsilon: 0.5263424200102826    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1675   score: 6.0   memory length: 100000   epsilon: 0.5256019000102987    steps: 374    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1676   score: 3.0   memory length: 100000   epsilon: 0.5251465000103086    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1677   score: 6.0   memory length: 100000   epsilon: 0.5244574600103236    steps: 348    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1678   score: 3.0   memory length: 100000   epsilon: 0.5240080000103333    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1679   score: 4.0   memory length: 100000   epsilon: 0.5234912200103445    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1680   score: 4.0   memory length: 100000   epsilon: 0.5229784000103557    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1681   score: 3.0   memory length: 100000   epsilon: 0.5224913200103662    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1682   score: 4.0   memory length: 100000   epsilon: 0.5220101800103767    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1683   score: 7.0   memory length: 100000   epsilon: 0.5212360000103935    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1684   score: 3.0   memory length: 100000   epsilon: 0.5207786200104034    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1685   score: 3.0   memory length: 100000   epsilon: 0.5203588600104125    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.75\n",
      "episode: 1686   score: 6.0   memory length: 100000   epsilon: 0.5196579400104278    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 3.78\n",
      "episode: 1687   score: 6.0   memory length: 100000   epsilon: 0.519000580010442    steps: 332    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1688   score: 1.0   memory length: 100000   epsilon: 0.5186639800104493    steps: 170    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n",
      "episode: 1689   score: 2.0   memory length: 100000   epsilon: 0.5182323400104587    steps: 218    lr: 1.6000000000000003e-05     evaluation reward: 3.76\n",
      "episode: 1690   score: 2.0   memory length: 100000   epsilon: 0.5178739600104665    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1691   score: 1.0   memory length: 100000   epsilon: 0.517573000010473    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1692   score: 3.0   memory length: 100000   epsilon: 0.5170839400104836    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1693   score: 3.0   memory length: 100000   epsilon: 0.5166641800104927    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1694   score: 2.0   memory length: 100000   epsilon: 0.5163058000105005    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.63\n",
      "episode: 1695   score: 4.0   memory length: 100000   epsilon: 0.5157157600105133    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1696   score: 6.0   memory length: 100000   epsilon: 0.5150049400105288    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1697   score: 3.0   memory length: 100000   epsilon: 0.514581220010538    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.68\n",
      "episode: 1698   score: 3.0   memory length: 100000   epsilon: 0.5140961200105485    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 3.7\n",
      "episode: 1699   score: 4.0   memory length: 100000   epsilon: 0.5135456800105604    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1700   score: 2.0   memory length: 100000   epsilon: 0.5131477000105691    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1701   score: 4.0   memory length: 100000   epsilon: 0.5126388400105801    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1702   score: 7.0   memory length: 100000   epsilon: 0.511910200010596    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 3.7\n",
      "episode: 1703   score: 0.0   memory length: 100000   epsilon: 0.5116646800106013    steps: 124    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1704   score: 4.0   memory length: 100000   epsilon: 0.5111459200106125    steps: 262    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1705   score: 4.0   memory length: 100000   epsilon: 0.5105954800106245    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1706   score: 2.0   memory length: 100000   epsilon: 0.510157900010634    steps: 221    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1707   score: 4.0   memory length: 100000   epsilon: 0.5096114200106459    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1708   score: 4.0   memory length: 100000   epsilon: 0.5090550400106579    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1709   score: 7.0   memory length: 100000   epsilon: 0.5082749200106749    steps: 394    lr: 1.6000000000000003e-05     evaluation reward: 3.72\n",
      "episode: 1710   score: 4.0   memory length: 100000   epsilon: 0.5076908200106875    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 3.72\n",
      "episode: 1711   score: 4.0   memory length: 100000   epsilon: 0.5071344400106996    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1712   score: 3.0   memory length: 100000   epsilon: 0.5066493400107102    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1713   score: 2.0   memory length: 100000   epsilon: 0.5062474000107189    steps: 203    lr: 1.6000000000000003e-05     evaluation reward: 3.7\n",
      "episode: 1714   score: 6.0   memory length: 100000   epsilon: 0.5055365800107343    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1715   score: 4.0   memory length: 100000   epsilon: 0.5050237600107454    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1716   score: 6.0   memory length: 100000   epsilon: 0.5044178800107586    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n",
      "episode: 1717   score: 0.0   memory length: 100000   epsilon: 0.5041723600107639    steps: 124    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1718   score: 5.0   memory length: 100000   epsilon: 0.5035645000107771    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1719   score: 3.0   memory length: 100000   epsilon: 0.5030754400107877    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1720   score: 3.0   memory length: 100000   epsilon: 0.5026220200107976    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1721   score: 6.0   memory length: 100000   epsilon: 0.5019527800108121    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 3.75\n",
      "episode: 1722   score: 2.0   memory length: 100000   epsilon: 0.5015944000108199    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1723   score: 4.0   memory length: 100000   epsilon: 0.5010796000108311    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1724   score: 4.0   memory length: 100000   epsilon: 0.5005271800108431    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1725   score: 3.0   memory length: 100000   epsilon: 0.5000341600108538    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 3.72\n",
      "episode: 1726   score: 6.0   memory length: 100000   epsilon: 0.49928968001085    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 3.72\n",
      "episode: 1727   score: 5.0   memory length: 100000   epsilon: 0.49873132001084647    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1728   score: 3.0   memory length: 100000   epsilon: 0.4982759200108436    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1729   score: 4.0   memory length: 100000   epsilon: 0.49768192001083983    steps: 300    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1730   score: 3.0   memory length: 100000   epsilon: 0.49722850001083696    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.72\n",
      "episode: 1731   score: 8.0   memory length: 100000   epsilon: 0.49632760001083126    steps: 455    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n",
      "episode: 1732   score: 5.0   memory length: 100000   epsilon: 0.4957217200108274    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 3.75\n",
      "episode: 1733   score: 3.0   memory length: 100000   epsilon: 0.49518514001082403    steps: 271    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1734   score: 5.0   memory length: 100000   epsilon: 0.4945792600108202    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1735   score: 5.0   memory length: 100000   epsilon: 0.4939337800108161    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 3.76\n",
      "episode: 1736   score: 2.0   memory length: 100000   epsilon: 0.4935397600108136    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1737   score: 5.0   memory length: 100000   epsilon: 0.4929358600108098    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n",
      "episode: 1738   score: 1.0   memory length: 100000   epsilon: 0.4926349000108079    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1739   score: 4.0   memory length: 100000   epsilon: 0.49208842001080444    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1740   score: 4.0   memory length: 100000   epsilon: 0.49153600001080094    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1741   score: 0.0   memory length: 100000   epsilon: 0.4912904800107994    steps: 124    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1742   score: 7.0   memory length: 100000   epsilon: 0.49045888001079413    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1743   score: 4.0   memory length: 100000   epsilon: 0.4898708200107904    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 3.75\n",
      "episode: 1744   score: 7.0   memory length: 100000   epsilon: 0.4892035600107862    steps: 337    lr: 1.6000000000000003e-05     evaluation reward: 3.78\n",
      "episode: 1745   score: 5.0   memory length: 100000   epsilon: 0.48855214001078207    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1746   score: 4.0   memory length: 100000   epsilon: 0.4880393200107788    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1747   score: 8.0   memory length: 100000   epsilon: 0.48717406001077335    steps: 437    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1748   score: 4.0   memory length: 100000   epsilon: 0.48662362001076986    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1749   score: 4.0   memory length: 100000   epsilon: 0.4861108000107666    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1750   score: 4.0   memory length: 100000   epsilon: 0.4855187800107629    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1751   score: 6.0   memory length: 100000   epsilon: 0.4848158800107584    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1752   score: 7.0   memory length: 100000   epsilon: 0.4839724000107531    steps: 426    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1753   score: 6.0   memory length: 100000   epsilon: 0.48322594001074837    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 3.96\n",
      "episode: 1754   score: 3.0   memory length: 100000   epsilon: 0.4827764800107455    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1755   score: 4.0   memory length: 100000   epsilon: 0.48225970001074225    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.96\n",
      "episode: 1756   score: 4.0   memory length: 100000   epsilon: 0.4817112400107388    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1757   score: 4.0   memory length: 100000   epsilon: 0.4811944600107355    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1758   score: 2.0   memory length: 100000   epsilon: 0.48075886001073276    steps: 220    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1759   score: 1.0   memory length: 100000   epsilon: 0.48045790001073085    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.96\n",
      "episode: 1760   score: 6.0   memory length: 100000   epsilon: 0.4797827200107266    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 3.96\n",
      "episode: 1761   score: 5.0   memory length: 100000   epsilon: 0.47909566001072224    steps: 347    lr: 1.6000000000000003e-05     evaluation reward: 3.97\n",
      "episode: 1762   score: 6.0   memory length: 100000   epsilon: 0.4783927600107178    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1763   score: 3.0   memory length: 100000   epsilon: 0.4779076600107147    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 3.96\n",
      "episode: 1764   score: 3.0   memory length: 100000   epsilon: 0.47745622001071186    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.93\n",
      "episode: 1765   score: 3.0   memory length: 100000   epsilon: 0.47696320001070874    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1766   score: 3.0   memory length: 100000   epsilon: 0.4765097800107059    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1767   score: 5.0   memory length: 100000   epsilon: 0.47587024001070183    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1768   score: 5.0   memory length: 100000   epsilon: 0.4752940600106982    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1769   score: 3.0   memory length: 100000   epsilon: 0.47479906001069505    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1770   score: 6.0   memory length: 100000   epsilon: 0.47411596001069073    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 3.94\n",
      "episode: 1771   score: 5.0   memory length: 100000   epsilon: 0.47356948001068727    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.97\n",
      "episode: 1772   score: 1.0   memory length: 100000   epsilon: 0.47326852001068537    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1773   score: 2.0   memory length: 100000   epsilon: 0.4728764800106829    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1774   score: 6.0   memory length: 100000   epsilon: 0.4721696200106784    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1775   score: 5.0   memory length: 100000   epsilon: 0.4715974000106748    steps: 289    lr: 1.6000000000000003e-05     evaluation reward: 3.97\n",
      "episode: 1776   score: 4.0   memory length: 100000   epsilon: 0.47108656001067156    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1777   score: 3.0   memory length: 100000   epsilon: 0.4706628400106689    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1778   score: 3.0   memory length: 100000   epsilon: 0.4701737800106658    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1779   score: 3.0   memory length: 100000   epsilon: 0.46972234001066293    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.94\n",
      "episode: 1780   score: 2.0   memory length: 100000   epsilon: 0.4692887200106602    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1781   score: 5.0   memory length: 100000   epsilon: 0.46863730001065607    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 3.94\n",
      "episode: 1782   score: 3.0   memory length: 100000   epsilon: 0.4682155600106534    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.93\n",
      "episode: 1783   score: 5.0   memory length: 100000   epsilon: 0.4675562200106492    steps: 333    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1784   score: 4.0   memory length: 100000   epsilon: 0.4670018200106457    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1785   score: 1.0   memory length: 100000   epsilon: 0.4667008600106438    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.9\n",
      "episode: 1786   score: 7.0   memory length: 100000   epsilon: 0.4659068800106388    steps: 401    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1787   score: 6.0   memory length: 100000   epsilon: 0.46526536001063473    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1788   score: 2.0   memory length: 100000   epsilon: 0.464835700010632    steps: 217    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1789   score: 2.0   memory length: 100000   epsilon: 0.46444366001062953    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1790   score: 3.0   memory length: 100000   epsilon: 0.4639130200106262    steps: 268    lr: 1.6000000000000003e-05     evaluation reward: 3.93\n",
      "episode: 1791   score: 4.0   memory length: 100000   epsilon: 0.4634279200106231    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 3.96\n",
      "episode: 1792   score: 3.0   memory length: 100000   epsilon: 0.46297450001062024    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1793   score: 4.0   memory length: 100000   epsilon: 0.4624933600106172    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 3.97\n",
      "episode: 1794   score: 5.0   memory length: 100000   epsilon: 0.4618795600106133    steps: 310    lr: 1.6000000000000003e-05     evaluation reward: 4.0\n",
      "episode: 1795   score: 2.0   memory length: 100000   epsilon: 0.46144594001061057    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1796   score: 3.0   memory length: 100000   epsilon: 0.46099846001060774    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1797   score: 3.0   memory length: 100000   epsilon: 0.4605490000106049    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1798   score: 6.0   memory length: 100000   epsilon: 0.4598678800106006    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1799   score: 8.0   memory length: 100000   epsilon: 0.4592837800105969    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 4.02\n",
      "episode: 1800   score: 6.0   memory length: 100000   epsilon: 0.45859870001059255    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1801   score: 2.0   memory length: 100000   epsilon: 0.45820666001059007    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1802   score: 4.0   memory length: 100000   epsilon: 0.4576938400105868    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.01\n",
      "episode: 1803   score: 4.0   memory length: 100000   epsilon: 0.45714340001058335    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1804   score: 4.0   memory length: 100000   epsilon: 0.4566286000105801    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1805   score: 6.0   memory length: 100000   epsilon: 0.45590986001057554    steps: 363    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1806   score: 5.0   memory length: 100000   epsilon: 0.4553336800105719    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 4.1\n",
      "episode: 1807   score: 5.0   memory length: 100000   epsilon: 0.45474166001056815    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 4.11\n",
      "episode: 1808   score: 2.0   memory length: 100000   epsilon: 0.45434368001056563    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 4.09\n",
      "episode: 1809   score: 5.0   memory length: 100000   epsilon: 0.4536586000105613    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1810   score: 3.0   memory length: 100000   epsilon: 0.4532032000105584    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1811   score: 4.0   memory length: 100000   epsilon: 0.4526488000105549    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1812   score: 2.0   memory length: 100000   epsilon: 0.4522864600105526    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1813   score: 7.0   memory length: 100000   epsilon: 0.4514449600105473    steps: 425    lr: 1.6000000000000003e-05     evaluation reward: 4.1\n",
      "episode: 1814   score: 3.0   memory length: 100000   epsilon: 0.4509836200105444    steps: 233    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1815   score: 0.0   memory length: 100000   epsilon: 0.4507381000105428    steps: 124    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1816   score: 6.0   memory length: 100000   epsilon: 0.44991838001053763    steps: 414    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1817   score: 5.0   memory length: 100000   epsilon: 0.4492768600105336    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n",
      "episode: 1818   score: 5.0   memory length: 100000   epsilon: 0.4486294000105295    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n",
      "episode: 1819   score: 5.0   memory length: 100000   epsilon: 0.44807302001052596    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 4.1\n",
      "episode: 1820   score: 5.0   memory length: 100000   epsilon: 0.4474612000105221    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.12\n",
      "episode: 1821   score: 9.0   memory length: 100000   epsilon: 0.4465820800105165    steps: 444    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1822   score: 6.0   memory length: 100000   epsilon: 0.4459128400105123    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1823   score: 8.0   memory length: 100000   epsilon: 0.44532478001050857    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1824   score: 4.0   memory length: 100000   epsilon: 0.4447426600105049    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1825   score: 6.0   memory length: 100000   epsilon: 0.4440358000105004    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 4.26\n",
      "episode: 1826   score: 4.0   memory length: 100000   epsilon: 0.44349130001049697    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1827   score: 3.0   memory length: 100000   epsilon: 0.4429606600104936    steps: 268    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1828   score: 5.0   memory length: 100000   epsilon: 0.44238646001049    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1829   score: 4.0   memory length: 100000   epsilon: 0.44178454001048617    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1830   score: 4.0   memory length: 100000   epsilon: 0.4412638000104829    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1831   score: 5.0   memory length: 100000   epsilon: 0.440648020010479    steps: 311    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1832   score: 4.0   memory length: 100000   epsilon: 0.44006194001047527    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1833   score: 6.0   memory length: 100000   epsilon: 0.4393531000104708    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1834   score: 9.0   memory length: 100000   epsilon: 0.438442300010465    steps: 460    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1835   score: 4.0   memory length: 100000   epsilon: 0.437961160010462    steps: 243    lr: 1.6000000000000003e-05     evaluation reward: 4.27\n",
      "episode: 1836   score: 7.0   memory length: 100000   epsilon: 0.4371592600104569    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 4.32\n",
      "episode: 1837   score: 5.0   memory length: 100000   epsilon: 0.43661476001045346    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 4.32\n",
      "episode: 1838   score: 0.0   memory length: 100000   epsilon: 0.4363692400104519    steps: 124    lr: 1.6000000000000003e-05     evaluation reward: 4.31\n",
      "episode: 1839   score: 3.0   memory length: 100000   epsilon: 0.435909880010449    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 4.3\n",
      "episode: 1840   score: 2.0   memory length: 100000   epsilon: 0.43547428001044625    steps: 220    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1841   score: 4.0   memory length: 100000   epsilon: 0.4349951200104432    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 4.32\n",
      "episode: 1842   score: 5.0   memory length: 100000   epsilon: 0.43441498001043954    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 4.3\n",
      "episode: 1843   score: 4.0   memory length: 100000   epsilon: 0.43391008001043635    steps: 255    lr: 1.6000000000000003e-05     evaluation reward: 4.3\n",
      "episode: 1844   score: 0.0   memory length: 100000   epsilon: 0.4336665400104348    steps: 123    lr: 1.6000000000000003e-05     evaluation reward: 4.23\n",
      "episode: 1845   score: 3.0   memory length: 100000   epsilon: 0.4332111400104319    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1846   score: 4.0   memory length: 100000   epsilon: 0.4326349600104283    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1847   score: 3.0   memory length: 100000   epsilon: 0.4321775800104254    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1848   score: 2.0   memory length: 100000   epsilon: 0.43177960001042287    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1849   score: 5.0   memory length: 100000   epsilon: 0.43112620001041874    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1850   score: 3.0   memory length: 100000   epsilon: 0.43067278001041587    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1851   score: 3.0   memory length: 100000   epsilon: 0.43018174001041276    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 4.11\n",
      "episode: 1852   score: 2.0   memory length: 100000   epsilon: 0.42978772001041027    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1853   score: 5.0   memory length: 100000   epsilon: 0.4291759000104064    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1854   score: 2.0   memory length: 100000   epsilon: 0.42881752001040413    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1855   score: 3.0   memory length: 100000   epsilon: 0.4283700400104013    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1856   score: 4.0   memory length: 100000   epsilon: 0.42785722001039805    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1857   score: 3.0   memory length: 100000   epsilon: 0.42737014001039497    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 4.02\n",
      "episode: 1858   score: 1.0   memory length: 100000   epsilon: 0.42706720001039306    steps: 153    lr: 1.6000000000000003e-05     evaluation reward: 4.01\n",
      "episode: 1859   score: 4.0   memory length: 100000   epsilon: 0.42658804001039    steps: 242    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1860   score: 6.0   memory length: 100000   epsilon: 0.42592672001038584    steps: 334    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1861   score: 6.0   memory length: 100000   epsilon: 0.42521788001038135    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1862   score: 5.0   memory length: 100000   epsilon: 0.424528840010377    steps: 348    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1863   score: 7.0   memory length: 100000   epsilon: 0.42374476001037203    steps: 396    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n",
      "episode: 1864   score: 5.0   memory length: 100000   epsilon: 0.42310126001036796    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 4.1\n",
      "episode: 1865   score: 5.0   memory length: 100000   epsilon: 0.42248350001036405    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 4.12\n",
      "episode: 1866   score: 6.0   memory length: 100000   epsilon: 0.4218142600103598    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1867   score: 4.0   memory length: 100000   epsilon: 0.42126382001035634    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1868   score: 6.0   memory length: 100000   epsilon: 0.42055498001035185    steps: 358    lr: 1.6000000000000003e-05     evaluation reward: 4.15\n",
      "episode: 1869   score: 5.0   memory length: 100000   epsilon: 0.419943160010348    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 4.17\n",
      "episode: 1870   score: 5.0   memory length: 100000   epsilon: 0.4193729200103444    steps: 288    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1871   score: 3.0   memory length: 100000   epsilon: 0.4189214800103415    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1872   score: 3.0   memory length: 100000   epsilon: 0.4184304400103384    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1873   score: 5.0   memory length: 100000   epsilon: 0.4178107000103345    steps: 313    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1874   score: 7.0   memory length: 100000   epsilon: 0.41700088001032937    steps: 409    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1875   score: 4.0   memory length: 100000   epsilon: 0.4164979600103262    steps: 254    lr: 1.6000000000000003e-05     evaluation reward: 4.19\n",
      "episode: 1876   score: 5.0   memory length: 100000   epsilon: 0.41592376001032255    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1877   score: 3.0   memory length: 100000   epsilon: 0.4154743000103197    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.2\n",
      "episode: 1878   score: 4.0   memory length: 100000   epsilon: 0.4149971200103167    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 4.21\n",
      "episode: 1879   score: 4.0   memory length: 100000   epsilon: 0.4144803400103134    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 4.22\n",
      "episode: 1880   score: 4.0   memory length: 100000   epsilon: 0.4138962400103097    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 4.24\n",
      "episode: 1881   score: 6.0   memory length: 100000   epsilon: 0.4131933400103053    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1882   score: 3.0   memory length: 100000   epsilon: 0.41266468001030193    steps: 267    lr: 1.6000000000000003e-05     evaluation reward: 4.25\n",
      "episode: 1883   score: 8.0   memory length: 100000   epsilon: 0.4117142800102959    steps: 480    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1884   score: 6.0   memory length: 100000   epsilon: 0.41100940001029146    steps: 356    lr: 1.6000000000000003e-05     evaluation reward: 4.3\n",
      "episode: 1885   score: 5.0   memory length: 100000   epsilon: 0.41040550001028764    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 4.34\n",
      "episode: 1886   score: 4.0   memory length: 100000   epsilon: 0.40988872001028437    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 4.31\n",
      "episode: 1887   score: 4.0   memory length: 100000   epsilon: 0.40933234001028085    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1888   score: 2.0   memory length: 100000   epsilon: 0.40897000001027856    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1889   score: 2.0   memory length: 100000   epsilon: 0.40857598001027606    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 4.29\n",
      "episode: 1890   score: 2.0   memory length: 100000   epsilon: 0.4081423600102733    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 4.28\n",
      "episode: 1891   score: 6.0   memory length: 100000   epsilon: 0.40739194001026857    steps: 379    lr: 1.6000000000000003e-05     evaluation reward: 4.3\n",
      "episode: 1892   score: 6.0   memory length: 100000   epsilon: 0.40675636001026455    steps: 321    lr: 1.6000000000000003e-05     evaluation reward: 4.33\n",
      "episode: 1893   score: 5.0   memory length: 100000   epsilon: 0.4061168200102605    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 4.34\n",
      "episode: 1894   score: 5.0   memory length: 100000   epsilon: 0.40543174001025617    steps: 346    lr: 6.400000000000001e-06     evaluation reward: 4.34\n",
      "episode: 1895   score: 5.0   memory length: 100000   epsilon: 0.4047545800102519    steps: 342    lr: 6.400000000000001e-06     evaluation reward: 4.37\n",
      "episode: 1896   score: 4.0   memory length: 100000   epsilon: 0.4042001800102484    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 4.38\n",
      "episode: 1897   score: 5.0   memory length: 100000   epsilon: 0.40362994001024477    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 4.4\n",
      "episode: 1898   score: 6.0   memory length: 100000   epsilon: 0.4028577400102399    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 4.4\n",
      "episode: 1899   score: 5.0   memory length: 100000   epsilon: 0.4022558200102361    steps: 304    lr: 6.400000000000001e-06     evaluation reward: 4.37\n",
      "episode: 1900   score: 2.0   memory length: 100000   epsilon: 0.40185784001023356    steps: 201    lr: 6.400000000000001e-06     evaluation reward: 4.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1901   score: 3.0   memory length: 100000   epsilon: 0.4013232400102302    steps: 270    lr: 6.400000000000001e-06     evaluation reward: 4.34\n",
      "episode: 1902   score: 5.0   memory length: 100000   epsilon: 0.4007411200102265    steps: 294    lr: 6.400000000000001e-06     evaluation reward: 4.35\n",
      "episode: 1903   score: 4.0   memory length: 100000   epsilon: 0.40023028001022326    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 4.35\n",
      "episode: 1904   score: 5.0   memory length: 100000   epsilon: 0.39965806001021964    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 4.36\n",
      "episode: 1905   score: 5.0   memory length: 100000   epsilon: 0.3990680200102159    steps: 298    lr: 6.400000000000001e-06     evaluation reward: 4.35\n",
      "episode: 1906   score: 6.0   memory length: 100000   epsilon: 0.39836116001021143    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 4.36\n",
      "episode: 1907   score: 3.0   memory length: 100000   epsilon: 0.3978681400102083    steps: 249    lr: 6.400000000000001e-06     evaluation reward: 4.34\n",
      "episode: 1908   score: 3.0   memory length: 100000   epsilon: 0.39744442001020563    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 4.35\n",
      "episode: 1909   score: 4.0   memory length: 100000   epsilon: 0.3969296200102024    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.34\n",
      "episode: 1910   score: 4.0   memory length: 100000   epsilon: 0.3963811600101989    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.35\n",
      "episode: 1911   score: 7.0   memory length: 100000   epsilon: 0.3955416400101936    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 4.38\n",
      "episode: 1912   score: 7.0   memory length: 100000   epsilon: 0.39469816001018826    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 4.43\n",
      "episode: 1913   score: 2.0   memory length: 100000   epsilon: 0.394337800010186    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 4.38\n",
      "episode: 1914   score: 3.0   memory length: 100000   epsilon: 0.3938467600101829    steps: 248    lr: 6.400000000000001e-06     evaluation reward: 4.38\n",
      "episode: 1915   score: 8.0   memory length: 100000   epsilon: 0.3929795200101774    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 1916   score: 8.0   memory length: 100000   epsilon: 0.39224494001017274    steps: 371    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 1917   score: 5.0   memory length: 100000   epsilon: 0.3916370800101689    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 1918   score: 7.0   memory length: 100000   epsilon: 0.39093616001016446    steps: 354    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 1919   score: 5.0   memory length: 100000   epsilon: 0.3903144400101605    steps: 314    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 1920   score: 5.0   memory length: 100000   epsilon: 0.38962540001015616    steps: 348    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 1921   score: 4.0   memory length: 100000   epsilon: 0.3890769400101527    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.45\n",
      "episode: 1922   score: 8.0   memory length: 100000   epsilon: 0.3882889000101477    steps: 398    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 1923   score: 6.0   memory length: 100000   epsilon: 0.38758402001014325    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 4.45\n",
      "episode: 1924   score: 2.0   memory length: 100000   epsilon: 0.38719000001014076    steps: 199    lr: 6.400000000000001e-06     evaluation reward: 4.43\n",
      "episode: 1925   score: 5.0   memory length: 100000   epsilon: 0.3865484800101367    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 4.42\n",
      "episode: 1926   score: 4.0   memory length: 100000   epsilon: 0.38606734001013365    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 4.42\n",
      "episode: 1927   score: 4.0   memory length: 100000   epsilon: 0.38551096001013013    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 4.43\n",
      "episode: 1928   score: 12.0   memory length: 100000   epsilon: 0.3843962200101231    steps: 563    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 1929   score: 5.0   memory length: 100000   epsilon: 0.38379034001011925    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 1930   score: 3.0   memory length: 100000   epsilon: 0.38333296001011635    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 1931   score: 1.0   memory length: 100000   epsilon: 0.38303200001011445    steps: 152    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 1932   score: 6.0   memory length: 100000   epsilon: 0.38233306001011    steps: 353    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 1933   score: 5.0   memory length: 100000   epsilon: 0.381693520010106    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 1934   score: 5.0   memory length: 100000   epsilon: 0.38111734001010233    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 4.43\n",
      "episode: 1935   score: 5.0   memory length: 100000   epsilon: 0.38050552001009846    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 4.44\n",
      "episode: 1936   score: 5.0   memory length: 100000   epsilon: 0.37982638001009417    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 4.42\n",
      "episode: 1937   score: 7.0   memory length: 100000   epsilon: 0.37908190001008946    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 4.44\n",
      "episode: 1938   score: 2.0   memory length: 100000   epsilon: 0.37868590001008695    steps: 200    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 1939   score: 4.0   memory length: 100000   epsilon: 0.3782047600100839    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 1940   score: 4.0   memory length: 100000   epsilon: 0.3776186800100802    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 4.49\n",
      "episode: 1941   score: 5.0   memory length: 100000   epsilon: 0.37701082001007635    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 1942   score: 6.0   memory length: 100000   epsilon: 0.3763039600100719    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 1943   score: 6.0   memory length: 100000   epsilon: 0.3755951200100674    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 4.53\n",
      "episode: 1944   score: 5.0   memory length: 100000   epsilon: 0.3749813200100635    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 4.58\n",
      "episode: 1945   score: 6.0   memory length: 100000   epsilon: 0.37419526001005854    steps: 397    lr: 6.400000000000001e-06     evaluation reward: 4.61\n",
      "episode: 1946   score: 8.0   memory length: 100000   epsilon: 0.3732745600100527    steps: 465    lr: 6.400000000000001e-06     evaluation reward: 4.65\n",
      "episode: 1947   score: 6.0   memory length: 100000   epsilon: 0.37257166001004827    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 4.68\n",
      "episode: 1948   score: 8.0   memory length: 100000   epsilon: 0.37170244001004277    steps: 439    lr: 6.400000000000001e-06     evaluation reward: 4.74\n",
      "episode: 1949   score: 5.0   memory length: 100000   epsilon: 0.3710787400100388    steps: 315    lr: 6.400000000000001e-06     evaluation reward: 4.74\n",
      "episode: 1950   score: 7.0   memory length: 100000   epsilon: 0.3702986200100339    steps: 394    lr: 6.400000000000001e-06     evaluation reward: 4.78\n",
      "episode: 1951   score: 8.0   memory length: 100000   epsilon: 0.36932644001002773    steps: 491    lr: 6.400000000000001e-06     evaluation reward: 4.83\n",
      "episode: 1952   score: 6.0   memory length: 100000   epsilon: 0.3686235400100233    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 4.87\n",
      "episode: 1953   score: 5.0   memory length: 100000   epsilon: 0.36793648001001894    steps: 347    lr: 6.400000000000001e-06     evaluation reward: 4.87\n",
      "episode: 1954   score: 4.0   memory length: 100000   epsilon: 0.3673444600100152    steps: 299    lr: 6.400000000000001e-06     evaluation reward: 4.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1955   score: 3.0   memory length: 100000   epsilon: 0.3669207400100125    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 4.89\n",
      "episode: 1956   score: 7.0   memory length: 100000   epsilon: 0.36615052001000764    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 4.92\n",
      "episode: 1957   score: 1.0   memory length: 100000   epsilon: 0.36584956001000574    steps: 152    lr: 6.400000000000001e-06     evaluation reward: 4.9\n",
      "episode: 1958   score: 6.0   memory length: 100000   epsilon: 0.3652417000100019    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 4.95\n",
      "episode: 1959   score: 4.0   memory length: 100000   epsilon: 0.3646556200099982    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 4.95\n",
      "episode: 1960   score: 8.0   memory length: 100000   epsilon: 0.36406558000999445    steps: 298    lr: 6.400000000000001e-06     evaluation reward: 4.97\n",
      "episode: 1961   score: 5.0   memory length: 100000   epsilon: 0.36341614000999034    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 4.96\n",
      "episode: 1962   score: 7.0   memory length: 100000   epsilon: 0.3626795800099857    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 4.98\n",
      "episode: 1963   score: 4.0   memory length: 100000   epsilon: 0.3621311200099822    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.95\n",
      "episode: 1964   score: 6.0   memory length: 100000   epsilon: 0.3613668400099774    steps: 386    lr: 6.400000000000001e-06     evaluation reward: 4.96\n",
      "episode: 1965   score: 5.0   memory length: 100000   epsilon: 0.3607550200099735    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 4.96\n",
      "episode: 1966   score: 4.0   memory length: 100000   epsilon: 0.36023824000997023    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 4.94\n",
      "episode: 1967   score: 9.0   memory length: 100000   epsilon: 0.3593353600099645    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 4.99\n",
      "episode: 1968   score: 6.0   memory length: 100000   epsilon: 0.3586700800099603    steps: 336    lr: 6.400000000000001e-06     evaluation reward: 4.99\n",
      "episode: 1969   score: 6.0   memory length: 100000   epsilon: 0.3580325200099563    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 5.0\n",
      "episode: 1970   score: 7.0   memory length: 100000   epsilon: 0.3572623000099514    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 5.02\n",
      "episode: 1971   score: 7.0   memory length: 100000   epsilon: 0.3564247600099461    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 5.06\n",
      "episode: 1972   score: 4.0   memory length: 100000   epsilon: 0.3558743200099426    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 5.07\n",
      "episode: 1973   score: 6.0   memory length: 100000   epsilon: 0.3552823000099389    steps: 299    lr: 6.400000000000001e-06     evaluation reward: 5.08\n",
      "episode: 1974   score: 8.0   memory length: 100000   epsilon: 0.354510100009934    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 5.09\n",
      "episode: 1975   score: 4.0   memory length: 100000   epsilon: 0.3539616400099305    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 5.09\n",
      "episode: 1976   score: 2.0   memory length: 100000   epsilon: 0.35356762000992803    steps: 199    lr: 6.400000000000001e-06     evaluation reward: 5.06\n",
      "episode: 1977   score: 8.0   memory length: 100000   epsilon: 0.35270236000992256    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 5.11\n",
      "episode: 1978   score: 10.0   memory length: 100000   epsilon: 0.35195590000991783    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 5.17\n",
      "episode: 1979   score: 8.0   memory length: 100000   epsilon: 0.3513302200099139    steps: 316    lr: 6.400000000000001e-06     evaluation reward: 5.21\n",
      "episode: 1980   score: 4.0   memory length: 100000   epsilon: 0.3508134400099106    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 5.21\n",
      "episode: 1981   score: 10.0   memory length: 100000   epsilon: 0.3500689600099059    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 5.25\n",
      "episode: 1982   score: 4.0   memory length: 100000   epsilon: 0.34958782000990285    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 5.26\n",
      "episode: 1983   score: 6.0   memory length: 100000   epsilon: 0.3488829400098984    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 5.24\n",
      "episode: 1984   score: 8.0   memory length: 100000   epsilon: 0.3480117400098929    steps: 440    lr: 6.400000000000001e-06     evaluation reward: 5.26\n",
      "episode: 1985   score: 6.0   memory length: 100000   epsilon: 0.34726528000988816    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 5.27\n",
      "episode: 1986   score: 5.0   memory length: 100000   epsilon: 0.3466910800098845    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 5.28\n",
      "episode: 1987   score: 4.0   memory length: 100000   epsilon: 0.34620598000988145    steps: 245    lr: 6.400000000000001e-06     evaluation reward: 5.28\n",
      "episode: 1988   score: 6.0   memory length: 100000   epsilon: 0.34552486000987714    steps: 344    lr: 6.400000000000001e-06     evaluation reward: 5.32\n",
      "episode: 1989   score: 3.0   memory length: 100000   epsilon: 0.3451051000098745    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 5.33\n",
      "episode: 1990   score: 5.0   memory length: 100000   epsilon: 0.3444932800098706    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 5.36\n",
      "episode: 1991   score: 3.0   memory length: 100000   epsilon: 0.34403788000986774    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 5.33\n",
      "episode: 1992   score: 6.0   memory length: 100000   epsilon: 0.3433349800098633    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 5.33\n",
      "episode: 1993   score: 5.0   memory length: 100000   epsilon: 0.3426914800098592    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 5.33\n",
      "episode: 1994   score: 2.0   memory length: 100000   epsilon: 0.3423291400098569    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 5.3\n",
      "episode: 1995   score: 3.0   memory length: 100000   epsilon: 0.341867800009854    steps: 233    lr: 6.400000000000001e-06     evaluation reward: 5.28\n",
      "episode: 1996   score: 8.0   memory length: 100000   epsilon: 0.34108174000984903    steps: 397    lr: 6.400000000000001e-06     evaluation reward: 5.32\n",
      "episode: 1997   score: 3.0   memory length: 100000   epsilon: 0.34062436000984614    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 5.3\n",
      "episode: 1998   score: 7.0   memory length: 100000   epsilon: 0.33985414000984127    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 5.31\n",
      "episode: 1999   score: 8.0   memory length: 100000   epsilon: 0.33894928000983554    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 5.34\n",
      "episode: 2000   score: 5.0   memory length: 100000   epsilon: 0.33830380000983146    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 5.37\n",
      "episode: 2001   score: 4.0   memory length: 100000   epsilon: 0.33775138000982796    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 5.38\n",
      "episode: 2002   score: 7.0   memory length: 100000   epsilon: 0.33694354000982285    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 5.4\n",
      "episode: 2003   score: 6.0   memory length: 100000   epsilon: 0.33619906000981814    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 5.42\n",
      "episode: 2004   score: 4.0   memory length: 100000   epsilon: 0.3356070400098144    steps: 299    lr: 6.400000000000001e-06     evaluation reward: 5.41\n",
      "episode: 2005   score: 4.0   memory length: 100000   epsilon: 0.3350863000098111    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 5.4\n",
      "episode: 2006   score: 3.0   memory length: 100000   epsilon: 0.33466654000980844    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 5.37\n",
      "episode: 2007   score: 8.0   memory length: 100000   epsilon: 0.3337616800098027    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 5.42\n",
      "episode: 2008   score: 5.0   memory length: 100000   epsilon: 0.3331142200097986    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 5.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2009   score: 6.0   memory length: 100000   epsilon: 0.33233014000979366    steps: 396    lr: 6.400000000000001e-06     evaluation reward: 5.46\n",
      "episode: 2010   score: 5.0   memory length: 100000   epsilon: 0.3317222800097898    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 5.47\n",
      "episode: 2011   score: 9.0   memory length: 100000   epsilon: 0.3307540600097837    steps: 489    lr: 6.400000000000001e-06     evaluation reward: 5.49\n",
      "episode: 2012   score: 4.0   memory length: 100000   epsilon: 0.3301996600097802    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 5.46\n",
      "episode: 2013   score: 4.0   memory length: 100000   epsilon: 0.32964328000977666    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 5.48\n",
      "episode: 2014   score: 4.0   memory length: 100000   epsilon: 0.3290987800097732    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 5.49\n",
      "episode: 2015   score: 3.0   memory length: 100000   epsilon: 0.3286394200097703    steps: 232    lr: 6.400000000000001e-06     evaluation reward: 5.44\n",
      "episode: 2016   score: 5.0   memory length: 100000   epsilon: 0.3279919600097662    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 5.41\n",
      "episode: 2017   score: 7.0   memory length: 100000   epsilon: 0.32722372000976135    steps: 388    lr: 6.400000000000001e-06     evaluation reward: 5.43\n",
      "episode: 2018   score: 13.0   memory length: 100000   epsilon: 0.3259783000097535    steps: 629    lr: 6.400000000000001e-06     evaluation reward: 5.49\n",
      "episode: 2019   score: 7.0   memory length: 100000   epsilon: 0.3252575800097489    steps: 364    lr: 6.400000000000001e-06     evaluation reward: 5.51\n",
      "episode: 2020   score: 9.0   memory length: 100000   epsilon: 0.3242279800097424    steps: 520    lr: 6.400000000000001e-06     evaluation reward: 5.55\n",
      "episode: 2021   score: 5.0   memory length: 100000   epsilon: 0.3236399200097387    steps: 297    lr: 6.400000000000001e-06     evaluation reward: 5.56\n",
      "episode: 2022   score: 9.0   memory length: 100000   epsilon: 0.32267368000973257    steps: 488    lr: 6.400000000000001e-06     evaluation reward: 5.57\n",
      "episode: 2023   score: 4.0   memory length: 100000   epsilon: 0.3221569000097293    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 5.55\n",
      "episode: 2024   score: 6.0   memory length: 100000   epsilon: 0.32137480000972435    steps: 395    lr: 6.400000000000001e-06     evaluation reward: 5.59\n",
      "episode: 2025   score: 3.0   memory length: 100000   epsilon: 0.3209213800097215    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 5.57\n",
      "episode: 2026   score: 14.0   memory length: 100000   epsilon: 0.31971358000971384    steps: 610    lr: 6.400000000000001e-06     evaluation reward: 5.67\n",
      "episode: 2027   score: 3.0   memory length: 100000   epsilon: 0.31925620000971094    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 5.66\n",
      "episode: 2028   score: 10.0   memory length: 100000   epsilon: 0.31852954000970635    steps: 367    lr: 6.400000000000001e-06     evaluation reward: 5.64\n",
      "episode: 2029   score: 7.0   memory length: 100000   epsilon: 0.3177454600097014    steps: 396    lr: 6.400000000000001e-06     evaluation reward: 5.66\n",
      "episode: 2030   score: 6.0   memory length: 100000   epsilon: 0.3169732600096965    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 5.69\n",
      "episode: 2031   score: 8.0   memory length: 100000   epsilon: 0.3160862200096909    steps: 448    lr: 6.400000000000001e-06     evaluation reward: 5.76\n",
      "episode: 2032   score: 10.0   memory length: 100000   epsilon: 0.3149536600096837    steps: 572    lr: 6.400000000000001e-06     evaluation reward: 5.8\n",
      "episode: 2033   score: 6.0   memory length: 100000   epsilon: 0.3142705600096794    steps: 345    lr: 6.400000000000001e-06     evaluation reward: 5.81\n",
      "episode: 2034   score: 6.0   memory length: 100000   epsilon: 0.3135280600096747    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 5.82\n",
      "episode: 2035   score: 7.0   memory length: 100000   epsilon: 0.31273210000966967    steps: 402    lr: 6.400000000000001e-06     evaluation reward: 5.84\n",
      "episode: 2036   score: 3.0   memory length: 100000   epsilon: 0.3122747200096668    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 5.82\n",
      "episode: 2037   score: 5.0   memory length: 100000   epsilon: 0.31170052000966314    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 5.8\n",
      "episode: 2038   score: 3.0   memory length: 100000   epsilon: 0.31121344000966006    steps: 246    lr: 6.400000000000001e-06     evaluation reward: 5.81\n",
      "episode: 2039   score: 3.0   memory length: 100000   epsilon: 0.3107976400096574    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 5.8\n",
      "episode: 2040   score: 3.0   memory length: 100000   epsilon: 0.3103798600096548    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 5.79\n",
      "episode: 2041   score: 4.0   memory length: 100000   epsilon: 0.30982150000965125    steps: 282    lr: 6.400000000000001e-06     evaluation reward: 5.78\n",
      "episode: 2042   score: 5.0   memory length: 100000   epsilon: 0.30921760000964743    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 5.77\n",
      "episode: 2043   score: 3.0   memory length: 100000   epsilon: 0.30879388000964475    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 5.74\n",
      "episode: 2044   score: 10.0   memory length: 100000   epsilon: 0.30774844000963814    steps: 528    lr: 6.400000000000001e-06     evaluation reward: 5.79\n",
      "episode: 2045   score: 4.0   memory length: 100000   epsilon: 0.30726334000963507    steps: 245    lr: 6.400000000000001e-06     evaluation reward: 5.77\n",
      "episode: 2046   score: 6.0   memory length: 100000   epsilon: 0.3064772800096301    steps: 397    lr: 6.400000000000001e-06     evaluation reward: 5.75\n",
      "episode: 2047   score: 7.0   memory length: 100000   epsilon: 0.30567736000962503    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 5.76\n",
      "episode: 2048   score: 5.0   memory length: 100000   epsilon: 0.30502990000962094    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
      "episode: 2049   score: 5.0   memory length: 100000   epsilon: 0.3044260000096171    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 5.73\n",
      "episode: 2050   score: 5.0   memory length: 100000   epsilon: 0.30381220000961323    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 5.71\n",
      "episode: 2051   score: 8.0   memory length: 100000   epsilon: 0.3029093200096075    steps: 456    lr: 6.400000000000001e-06     evaluation reward: 5.71\n",
      "episode: 2052   score: 3.0   memory length: 100000   epsilon: 0.30248758000960485    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 5.68\n",
      "episode: 2053   score: 9.0   memory length: 100000   epsilon: 0.3018440800096008    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 5.72\n",
      "episode: 2054   score: 6.0   memory length: 100000   epsilon: 0.3011372200095963    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 5.74\n",
      "episode: 2055   score: 6.0   memory length: 100000   epsilon: 0.3004283800095918    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 5.77\n",
      "episode: 2056   score: 6.0   memory length: 100000   epsilon: 0.2997136000095873    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 5.76\n",
      "episode: 2057   score: 4.0   memory length: 100000   epsilon: 0.29919880000958404    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 5.79\n",
      "episode: 2058   score: 4.0   memory length: 100000   epsilon: 0.29864440000958054    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 5.77\n",
      "episode: 2059   score: 9.0   memory length: 100000   epsilon: 0.29768212000957445    steps: 486    lr: 6.400000000000001e-06     evaluation reward: 5.82\n",
      "episode: 2060   score: 10.0   memory length: 100000   epsilon: 0.296824780009569    steps: 433    lr: 6.400000000000001e-06     evaluation reward: 5.84\n",
      "episode: 2061   score: 6.0   memory length: 100000   epsilon: 0.2961080200095645    steps: 362    lr: 6.400000000000001e-06     evaluation reward: 5.85\n",
      "episode: 2062   score: 7.0   memory length: 100000   epsilon: 0.2953338400095596    steps: 391    lr: 6.400000000000001e-06     evaluation reward: 5.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2063   score: 7.0   memory length: 100000   epsilon: 0.29448838000955424    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
      "episode: 2064   score: 7.0   memory length: 100000   epsilon: 0.2937102400095493    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
      "episode: 2065   score: 8.0   memory length: 100000   epsilon: 0.2928212200095437    steps: 449    lr: 6.400000000000001e-06     evaluation reward: 5.92\n",
      "episode: 2066   score: 5.0   memory length: 100000   epsilon: 0.2921737600095396    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
      "episode: 2067   score: 4.0   memory length: 100000   epsilon: 0.2915857000095359    steps: 297    lr: 6.400000000000001e-06     evaluation reward: 5.88\n",
      "episode: 2068   score: 5.0   memory length: 100000   epsilon: 0.290973880009532    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 5.87\n",
      "episode: 2069   score: 6.0   memory length: 100000   epsilon: 0.2902630600095275    steps: 359    lr: 6.400000000000001e-06     evaluation reward: 5.87\n",
      "episode: 2070   score: 9.0   memory length: 100000   epsilon: 0.289231480009521    steps: 521    lr: 6.400000000000001e-06     evaluation reward: 5.89\n",
      "episode: 2071   score: 16.0   memory length: 100000   epsilon: 0.2881108000095139    steps: 566    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
      "episode: 2072   score: 5.0   memory length: 100000   epsilon: 0.28753462000951024    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
      "episode: 2073   score: 4.0   memory length: 100000   epsilon: 0.287021800009507    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
      "episode: 2074   score: 8.0   memory length: 100000   epsilon: 0.2861525800095015    steps: 439    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
      "episode: 2075   score: 3.0   memory length: 100000   epsilon: 0.2857288600094988    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 5.96\n",
      "episode: 2076   score: 2.0   memory length: 100000   epsilon: 0.2853665200094965    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 5.96\n",
      "episode: 2077   score: 9.0   memory length: 100000   epsilon: 0.2844577000094908    steps: 459    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
      "episode: 2078   score: 6.0   memory length: 100000   epsilon: 0.2837508400094863    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 5.93\n",
      "episode: 2079   score: 12.0   memory length: 100000   epsilon: 0.2829608200094813    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 5.97\n",
      "episode: 2080   score: 9.0   memory length: 100000   epsilon: 0.28213120000947606    steps: 419    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
      "episode: 2081   score: 7.0   memory length: 100000   epsilon: 0.2812857400094707    steps: 427    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
      "episode: 2082   score: 3.0   memory length: 100000   epsilon: 0.28086598000946805    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
      "episode: 2083   score: 6.0   memory length: 100000   epsilon: 0.2802403000094641    steps: 316    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
      "episode: 2084   score: 5.0   memory length: 100000   epsilon: 0.27955324000945975    steps: 347    lr: 6.400000000000001e-06     evaluation reward: 5.95\n",
      "episode: 2085   score: 6.0   memory length: 100000   epsilon: 0.2789137000094557    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 5.95\n",
      "episode: 2086   score: 8.0   memory length: 100000   epsilon: 0.27800686000944996    steps: 458    lr: 6.400000000000001e-06     evaluation reward: 5.98\n",
      "episode: 2087   score: 5.0   memory length: 100000   epsilon: 0.27740296000944614    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 5.99\n",
      "episode: 2088   score: 7.0   memory length: 100000   epsilon: 0.27662680000944123    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 6.0\n",
      "episode: 2089   score: 4.0   memory length: 100000   epsilon: 0.27607636000943775    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 6.01\n",
      "episode: 2090   score: 11.0   memory length: 100000   epsilon: 0.27501904000943106    steps: 534    lr: 6.400000000000001e-06     evaluation reward: 6.07\n",
      "episode: 2091   score: 7.0   memory length: 100000   epsilon: 0.2741676400094257    steps: 430    lr: 6.400000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2092   score: 8.0   memory length: 100000   epsilon: 0.27332416000942034    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2093   score: 3.0   memory length: 100000   epsilon: 0.2728727200094175    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2094   score: 6.0   memory length: 100000   epsilon: 0.27221734000941333    steps: 331    lr: 6.400000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2095   score: 3.0   memory length: 100000   epsilon: 0.27176194000941045    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2096   score: 6.0   memory length: 100000   epsilon: 0.27101746000940574    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2097   score: 3.0   memory length: 100000   epsilon: 0.2705561200094028    steps: 233    lr: 6.400000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2098   score: 5.0   memory length: 100000   epsilon: 0.2699086600093987    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2099   score: 8.0   memory length: 100000   epsilon: 0.2691166600093937    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2100   score: 6.0   memory length: 100000   epsilon: 0.26841178000938926    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 6.12\n",
      "episode: 2101   score: 7.0   memory length: 100000   epsilon: 0.26761978000938424    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2102   score: 5.0   memory length: 100000   epsilon: 0.26697430000938016    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2103   score: 8.0   memory length: 100000   epsilon: 0.26620012000937526    steps: 391    lr: 6.400000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2104   score: 5.0   memory length: 100000   epsilon: 0.26558236000937135    steps: 312    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2105   score: 8.0   memory length: 100000   epsilon: 0.26463196000936534    steps: 480    lr: 6.400000000000001e-06     evaluation reward: 6.2\n",
      "episode: 2106   score: 7.0   memory length: 100000   epsilon: 0.26389342000936067    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 6.24\n",
      "episode: 2107   score: 3.0   memory length: 100000   epsilon: 0.2634360400093578    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
      "episode: 2108   score: 6.0   memory length: 100000   epsilon: 0.26272126000935325    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 6.2\n",
      "episode: 2109   score: 11.0   memory length: 100000   epsilon: 0.2616877000093467    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 6.25\n",
      "episode: 2110   score: 5.0   memory length: 100000   epsilon: 0.2610699400093428    steps: 312    lr: 6.400000000000001e-06     evaluation reward: 6.25\n",
      "episode: 2111   score: 5.0   memory length: 100000   epsilon: 0.26048980000933913    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 6.21\n",
      "episode: 2112   score: 8.0   memory length: 100000   epsilon: 0.259676020009334    steps: 411    lr: 6.400000000000001e-06     evaluation reward: 6.25\n",
      "episode: 2113   score: 4.0   memory length: 100000   epsilon: 0.25911766000933045    steps: 282    lr: 6.400000000000001e-06     evaluation reward: 6.25\n",
      "episode: 2114   score: 4.0   memory length: 100000   epsilon: 0.25856524000932696    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 6.25\n",
      "episode: 2115   score: 6.0   memory length: 100000   epsilon: 0.2578267000093223    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 6.28\n",
      "episode: 2116   score: 6.0   memory length: 100000   epsilon: 0.257151520009318    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 6.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2117   score: 3.0   memory length: 100000   epsilon: 0.25673176000931536    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 6.25\n",
      "episode: 2118   score: 7.0   memory length: 100000   epsilon: 0.25598926000931066    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
      "episode: 2119   score: 4.0   memory length: 100000   epsilon: 0.25543288000930714    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2120   score: 4.0   memory length: 100000   epsilon: 0.2548448200093034    steps: 297    lr: 6.400000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2121   score: 6.0   memory length: 100000   epsilon: 0.2541003400092987    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 6.12\n",
      "episode: 2122   score: 5.0   memory length: 100000   epsilon: 0.2534964400092949    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 6.08\n",
      "episode: 2123   score: 5.0   memory length: 100000   epsilon: 0.252884620009291    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 2124   score: 6.0   memory length: 100000   epsilon: 0.25213024000928624    steps: 381    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 2125   score: 8.0   memory length: 100000   epsilon: 0.25120162000928037    steps: 469    lr: 6.400000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2126   score: 8.0   memory length: 100000   epsilon: 0.25037398000927513    steps: 418    lr: 6.400000000000001e-06     evaluation reward: 6.08\n",
      "episode: 2127   score: 7.0   memory length: 100000   epsilon: 0.2495463400092699    steps: 418    lr: 6.400000000000001e-06     evaluation reward: 6.12\n",
      "episode: 2128   score: 6.0   memory length: 100000   epsilon: 0.24876820000926497    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 6.08\n",
      "episode: 2129   score: 5.0   memory length: 100000   epsilon: 0.24819598000926135    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 6.06\n",
      "episode: 2130   score: 4.0   memory length: 100000   epsilon: 0.24761584000925768    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
      "episode: 2131   score: 8.0   memory length: 100000   epsilon: 0.24674266000925216    steps: 441    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
      "episode: 2132   score: 8.0   memory length: 100000   epsilon: 0.24581008000924626    steps: 471    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
      "episode: 2133   score: 8.0   memory length: 100000   epsilon: 0.2449487800092408    steps: 435    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
      "episode: 2134   score: 6.0   memory length: 100000   epsilon: 0.24424390000923635    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 6.04\n",
      "episode: 2135   score: 5.0   memory length: 100000   epsilon: 0.24360040000923228    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 6.02\n",
      "episode: 2136   score: 7.0   memory length: 100000   epsilon: 0.24283612000922744    steps: 386    lr: 6.400000000000001e-06     evaluation reward: 6.06\n",
      "episode: 2137   score: 12.0   memory length: 100000   epsilon: 0.2417689000092207    steps: 539    lr: 6.400000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2138   score: 7.0   memory length: 100000   epsilon: 0.24088978000921513    steps: 444    lr: 6.400000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2139   score: 5.0   memory length: 100000   epsilon: 0.2402858800092113    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
      "episode: 2140   score: 9.0   memory length: 100000   epsilon: 0.23948398000920623    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 6.25\n",
      "episode: 2141   score: 7.0   memory length: 100000   epsilon: 0.23867614000920112    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 6.28\n",
      "episode: 2142   score: 4.0   memory length: 100000   epsilon: 0.23816134000919786    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 6.27\n",
      "episode: 2143   score: 8.0   memory length: 100000   epsilon: 0.23731984000919254    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 6.32\n",
      "episode: 2144   score: 9.0   memory length: 100000   epsilon: 0.23640706000918676    steps: 461    lr: 6.400000000000001e-06     evaluation reward: 6.31\n",
      "episode: 2145   score: 4.0   memory length: 100000   epsilon: 0.23588434000918346    steps: 264    lr: 6.400000000000001e-06     evaluation reward: 6.31\n",
      "episode: 2146   score: 9.0   memory length: 100000   epsilon: 0.2349438400091775    steps: 475    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 2147   score: 9.0   memory length: 100000   epsilon: 0.23395384000917124    steps: 500    lr: 6.400000000000001e-06     evaluation reward: 6.36\n",
      "episode: 2148   score: 5.0   memory length: 100000   epsilon: 0.23334994000916742    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 6.36\n",
      "episode: 2149   score: 9.0   memory length: 100000   epsilon: 0.23245102000916174    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 6.4\n",
      "episode: 2150   score: 3.0   memory length: 100000   epsilon: 0.23202928000915907    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 2151   score: 7.0   memory length: 100000   epsilon: 0.2312907400091544    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 2152   score: 5.0   memory length: 100000   epsilon: 0.2306749600091505    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 6.39\n",
      "episode: 2153   score: 6.0   memory length: 100000   epsilon: 0.22995226000914593    steps: 365    lr: 6.400000000000001e-06     evaluation reward: 6.36\n",
      "episode: 2154   score: 8.0   memory length: 100000   epsilon: 0.22901572000914    steps: 473    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 2155   score: 6.0   memory length: 100000   epsilon: 0.22827916000913534    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 6.38\n",
      "episode: 2156   score: 7.0   memory length: 100000   epsilon: 0.22743964000913003    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 6.39\n",
      "episode: 2157   score: 10.0   memory length: 100000   epsilon: 0.22638628000912336    steps: 532    lr: 6.400000000000001e-06     evaluation reward: 6.45\n",
      "episode: 2158   score: 8.0   memory length: 100000   epsilon: 0.22548736000911768    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 6.49\n",
      "episode: 2159   score: 2.0   memory length: 100000   epsilon: 0.22509334000911518    steps: 199    lr: 6.400000000000001e-06     evaluation reward: 6.42\n",
      "episode: 2160   score: 11.0   memory length: 100000   epsilon: 0.22391524000910773    steps: 595    lr: 6.400000000000001e-06     evaluation reward: 6.43\n",
      "episode: 2161   score: 4.0   memory length: 100000   epsilon: 0.22343410000910469    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2162   score: 7.0   memory length: 100000   epsilon: 0.22266784000909984    steps: 387    lr: 6.400000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2163   score: 3.0   memory length: 100000   epsilon: 0.22224808000909718    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 2164   score: 6.0   memory length: 100000   epsilon: 0.2214937000090924    steps: 381    lr: 6.400000000000001e-06     evaluation reward: 6.36\n",
      "episode: 2165   score: 6.0   memory length: 100000   epsilon: 0.22079080000908796    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 2166   score: 7.0   memory length: 100000   epsilon: 0.22005028000908328    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 6.36\n",
      "episode: 2167   score: 5.0   memory length: 100000   epsilon: 0.21943450000907938    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 6.37\n",
      "episode: 2168   score: 3.0   memory length: 100000   epsilon: 0.2190107800090767    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 6.35\n",
      "episode: 2169   score: 5.0   memory length: 100000   epsilon: 0.21843658000907307    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 6.34\n",
      "episode: 2170   score: 4.0   memory length: 100000   epsilon: 0.21785248000906937    steps: 295    lr: 6.400000000000001e-06     evaluation reward: 6.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2171   score: 8.0   memory length: 100000   epsilon: 0.21716542000906502    steps: 347    lr: 6.400000000000001e-06     evaluation reward: 6.21\n",
      "episode: 2172   score: 5.0   memory length: 100000   epsilon: 0.21652588000906098    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 6.21\n",
      "episode: 2173   score: 5.0   memory length: 100000   epsilon: 0.2159120800090571    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 6.22\n",
      "episode: 2174   score: 5.0   memory length: 100000   epsilon: 0.215266600009053    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 6.19\n",
      "episode: 2175   score: 10.0   memory length: 100000   epsilon: 0.2143795600090474    steps: 448    lr: 6.400000000000001e-06     evaluation reward: 6.26\n",
      "episode: 2176   score: 5.0   memory length: 100000   epsilon: 0.21377170000904355    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 6.29\n",
      "episode: 2177   score: 4.0   memory length: 100000   epsilon: 0.21326086000904032    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 6.24\n",
      "episode: 2178   score: 4.0   memory length: 100000   epsilon: 0.21277972000903728    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 6.22\n",
      "episode: 2179   score: 5.0   memory length: 100000   epsilon: 0.2121342400090332    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2180   score: 5.0   memory length: 100000   epsilon: 0.211472920009029    steps: 334    lr: 6.400000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2181   score: 5.0   memory length: 100000   epsilon: 0.21086506000902516    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 6.09\n",
      "episode: 2182   score: 7.0   memory length: 100000   epsilon: 0.21006118000902008    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2183   score: 9.0   memory length: 100000   epsilon: 0.20898406000901326    steps: 544    lr: 6.400000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2184   score: 4.0   memory length: 100000   epsilon: 0.20845936000900994    steps: 265    lr: 6.400000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2185   score: 4.0   memory length: 100000   epsilon: 0.2079485200090067    steps: 258    lr: 2.560000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2186   score: 6.0   memory length: 100000   epsilon: 0.20724364000900225    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2187   score: 5.0   memory length: 100000   epsilon: 0.20659816000899817    steps: 326    lr: 2.560000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2188   score: 10.0   memory length: 100000   epsilon: 0.2056378600089921    steps: 485    lr: 2.560000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2189   score: 6.0   memory length: 100000   epsilon: 0.20490130000898743    steps: 372    lr: 2.560000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2190   score: 5.0   memory length: 100000   epsilon: 0.20429344000898358    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 6.1\n",
      "episode: 2191   score: 9.0   memory length: 100000   epsilon: 0.2033945200089779    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 6.12\n",
      "episode: 2192   score: 7.0   memory length: 100000   epsilon: 0.2025748000089727    steps: 414    lr: 2.560000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2193   score: 5.0   memory length: 100000   epsilon: 0.20192932000896863    steps: 326    lr: 2.560000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2194   score: 3.0   memory length: 100000   epsilon: 0.20150560000896595    steps: 214    lr: 2.560000000000001e-06     evaluation reward: 6.1\n",
      "episode: 2195   score: 7.0   memory length: 100000   epsilon: 0.20074726000896115    steps: 383    lr: 2.560000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2196   score: 3.0   memory length: 100000   epsilon: 0.20025226000895802    steps: 250    lr: 2.560000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2197   score: 11.0   memory length: 100000   epsilon: 0.1992226600089515    steps: 520    lr: 2.560000000000001e-06     evaluation reward: 6.19\n",
      "episode: 2198   score: 6.0   memory length: 100000   epsilon: 0.19844056000894655    steps: 395    lr: 2.560000000000001e-06     evaluation reward: 6.2\n",
      "episode: 2199   score: 3.0   memory length: 100000   epsilon: 0.1979435800089434    steps: 251    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2200   score: 7.0   memory length: 100000   epsilon: 0.19713376000893829    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2201   score: 3.0   memory length: 100000   epsilon: 0.19668034000893542    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 6.12\n",
      "episode: 2202   score: 6.0   memory length: 100000   epsilon: 0.19592794000893066    steps: 380    lr: 2.560000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2203   score: 9.0   memory length: 100000   epsilon: 0.19500922000892484    steps: 464    lr: 2.560000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2204   score: 8.0   memory length: 100000   epsilon: 0.1941479200089194    steps: 435    lr: 2.560000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2205   score: 4.0   memory length: 100000   epsilon: 0.19366678000891635    steps: 243    lr: 2.560000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2206   score: 8.0   memory length: 100000   epsilon: 0.19274608000891053    steps: 465    lr: 2.560000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2207   score: 4.0   memory length: 100000   epsilon: 0.19215010000890675    steps: 301    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2208   score: 4.0   memory length: 100000   epsilon: 0.19159966000890327    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2209   score: 5.0   memory length: 100000   epsilon: 0.1909541800088992    steps: 326    lr: 2.560000000000001e-06     evaluation reward: 6.07\n",
      "episode: 2210   score: 5.0   memory length: 100000   epsilon: 0.19037800000889554    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 6.07\n",
      "episode: 2211   score: 7.0   memory length: 100000   epsilon: 0.18958402000889052    steps: 401    lr: 2.560000000000001e-06     evaluation reward: 6.09\n",
      "episode: 2212   score: 11.0   memory length: 100000   epsilon: 0.18878806000888548    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 6.12\n",
      "episode: 2213   score: 5.0   memory length: 100000   epsilon: 0.1881722800088816    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2214   score: 7.0   memory length: 100000   epsilon: 0.18744364000887698    steps: 368    lr: 2.560000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2215   score: 8.0   memory length: 100000   epsilon: 0.18669718000887225    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 6.18\n",
      "episode: 2216   score: 4.0   memory length: 100000   epsilon: 0.1861526800088688    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2217   score: 5.0   memory length: 100000   epsilon: 0.1855824400088652    steps: 288    lr: 2.560000000000001e-06     evaluation reward: 6.18\n",
      "episode: 2218   score: 4.0   memory length: 100000   epsilon: 0.18510526000886218    steps: 241    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2219   score: 2.0   memory length: 100000   epsilon: 0.1847429200088599    steps: 183    lr: 2.560000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2220   score: 6.0   memory length: 100000   epsilon: 0.18406180000885558    steps: 344    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2221   score: 7.0   memory length: 100000   epsilon: 0.1832440600088504    steps: 413    lr: 2.560000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2222   score: 3.0   memory length: 100000   epsilon: 0.1827866800088475    steps: 231    lr: 2.560000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2223   score: 6.0   memory length: 100000   epsilon: 0.18203230000884274    steps: 381    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2224   score: 5.0   memory length: 100000   epsilon: 0.1814581000088391    steps: 290    lr: 2.560000000000001e-06     evaluation reward: 6.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2225   score: 4.0   memory length: 100000   epsilon: 0.1809809200088361    steps: 241    lr: 2.560000000000001e-06     evaluation reward: 6.1\n",
      "episode: 2226   score: 5.0   memory length: 100000   epsilon: 0.18040474000883244    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 6.07\n",
      "episode: 2227   score: 6.0   memory length: 100000   epsilon: 0.17970580000882802    steps: 353    lr: 2.560000000000001e-06     evaluation reward: 6.06\n",
      "episode: 2228   score: 5.0   memory length: 100000   epsilon: 0.17901478000882365    steps: 349    lr: 2.560000000000001e-06     evaluation reward: 6.05\n",
      "episode: 2229   score: 13.0   memory length: 100000   epsilon: 0.17804062000881749    steps: 492    lr: 2.560000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2230   score: 5.0   memory length: 100000   epsilon: 0.17747632000881391    steps: 285    lr: 2.560000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2231   score: 11.0   memory length: 100000   epsilon: 0.17635762000880684    steps: 565    lr: 2.560000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2232   score: 9.0   memory length: 100000   epsilon: 0.175432960008801    steps: 467    lr: 2.560000000000001e-06     evaluation reward: 6.18\n",
      "episode: 2233   score: 7.0   memory length: 100000   epsilon: 0.1746112600087958    steps: 415    lr: 2.560000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2234   score: 4.0   memory length: 100000   epsilon: 0.17409844000879254    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2235   score: 7.0   memory length: 100000   epsilon: 0.17329258000878744    steps: 407    lr: 2.560000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2236   score: 7.0   memory length: 100000   epsilon: 0.17250652000878247    steps: 397    lr: 2.560000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2237   score: 11.0   memory length: 100000   epsilon: 0.17141356000877556    steps: 552    lr: 2.560000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2238   score: 5.0   memory length: 100000   epsilon: 0.17079184000877162    steps: 314    lr: 2.560000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2239   score: 8.0   memory length: 100000   epsilon: 0.17000380000876664    steps: 398    lr: 2.560000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2240   score: 11.0   memory length: 100000   epsilon: 0.1688435200087593    steps: 586    lr: 2.560000000000001e-06     evaluation reward: 6.19\n",
      "episode: 2241   score: 5.0   memory length: 100000   epsilon: 0.16826932000875566    steps: 290    lr: 2.560000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2242   score: 5.0   memory length: 100000   epsilon: 0.1675941400087514    steps: 341    lr: 2.560000000000001e-06     evaluation reward: 6.18\n",
      "episode: 2243   score: 10.0   memory length: 100000   epsilon: 0.1665526600087448    steps: 526    lr: 2.560000000000001e-06     evaluation reward: 6.2\n",
      "episode: 2244   score: 6.0   memory length: 100000   epsilon: 0.16580224000874005    steps: 379    lr: 2.560000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2245   score: 2.0   memory length: 100000   epsilon: 0.16543990000873776    steps: 183    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2246   score: 10.0   memory length: 100000   epsilon: 0.16440832000873123    steps: 521    lr: 2.560000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2247   score: 8.0   memory length: 100000   epsilon: 0.16351336000872557    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2248   score: 7.0   memory length: 100000   epsilon: 0.1626817600087203    steps: 420    lr: 2.560000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2249   score: 7.0   memory length: 100000   epsilon: 0.16188382000871526    steps: 403    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2250   score: 2.0   memory length: 100000   epsilon: 0.16148980000871277    steps: 199    lr: 2.560000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2251   score: 7.0   memory length: 100000   epsilon: 0.1607037400087078    steps: 397    lr: 2.560000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2252   score: 6.0   memory length: 100000   epsilon: 0.1599612400087031    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2253   score: 13.0   memory length: 100000   epsilon: 0.15877918000869562    steps: 597    lr: 2.560000000000001e-06     evaluation reward: 6.22\n",
      "episode: 2254   score: 3.0   memory length: 100000   epsilon: 0.15832180000869273    steps: 231    lr: 2.560000000000001e-06     evaluation reward: 6.17\n",
      "episode: 2255   score: 2.0   memory length: 100000   epsilon: 0.15795946000869043    steps: 183    lr: 2.560000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2256   score: 4.0   memory length: 100000   epsilon: 0.1574030800086869    steps: 281    lr: 2.560000000000001e-06     evaluation reward: 6.1\n",
      "episode: 2257   score: 8.0   memory length: 100000   epsilon: 0.1565497000086815    steps: 431    lr: 2.560000000000001e-06     evaluation reward: 6.08\n",
      "episode: 2258   score: 5.0   memory length: 100000   epsilon: 0.15597352000867787    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 6.05\n",
      "episode: 2259   score: 8.0   memory length: 100000   epsilon: 0.15507262000867217    steps: 455    lr: 2.560000000000001e-06     evaluation reward: 6.11\n",
      "episode: 2260   score: 8.0   memory length: 100000   epsilon: 0.15420340000866667    steps: 439    lr: 2.560000000000001e-06     evaluation reward: 6.08\n",
      "episode: 2261   score: 11.0   memory length: 100000   epsilon: 0.1531658800086601    steps: 524    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2262   score: 5.0   memory length: 100000   epsilon: 0.152516440008656    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2263   score: 5.0   memory length: 100000   epsilon: 0.15194026000865235    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2264   score: 7.0   memory length: 100000   epsilon: 0.15116410000864744    steps: 392    lr: 2.560000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2265   score: 5.0   memory length: 100000   epsilon: 0.1505562400086436    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 6.15\n",
      "episode: 2266   score: 8.0   memory length: 100000   epsilon: 0.14972860000863836    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2267   score: 9.0   memory length: 100000   epsilon: 0.14904550000863404    steps: 345    lr: 2.560000000000001e-06     evaluation reward: 6.2\n",
      "episode: 2268   score: 5.0   memory length: 100000   epsilon: 0.14843368000863016    steps: 309    lr: 2.560000000000001e-06     evaluation reward: 6.22\n",
      "episode: 2269   score: 6.0   memory length: 100000   epsilon: 0.1477604800086259    steps: 340    lr: 2.560000000000001e-06     evaluation reward: 6.23\n",
      "episode: 2270   score: 8.0   memory length: 100000   epsilon: 0.1468595800086202    steps: 455    lr: 2.560000000000001e-06     evaluation reward: 6.27\n",
      "episode: 2271   score: 5.0   memory length: 100000   epsilon: 0.14628340000861656    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 6.24\n",
      "episode: 2272   score: 7.0   memory length: 100000   epsilon: 0.1454676400086114    steps: 412    lr: 2.560000000000001e-06     evaluation reward: 6.26\n",
      "episode: 2273   score: 8.0   memory length: 100000   epsilon: 0.14456080000860566    steps: 458    lr: 2.560000000000001e-06     evaluation reward: 6.29\n",
      "episode: 2274   score: 3.0   memory length: 100000   epsilon: 0.144141040008603    steps: 212    lr: 2.560000000000001e-06     evaluation reward: 6.27\n",
      "episode: 2275   score: 6.0   memory length: 100000   epsilon: 0.14338468000859822    steps: 382    lr: 2.560000000000001e-06     evaluation reward: 6.23\n",
      "episode: 2276   score: 7.0   memory length: 100000   epsilon: 0.14261842000859337    steps: 387    lr: 2.560000000000001e-06     evaluation reward: 6.25\n",
      "episode: 2277   score: 8.0   memory length: 100000   epsilon: 0.14179474000858816    steps: 416    lr: 2.560000000000001e-06     evaluation reward: 6.29\n",
      "episode: 2278   score: 5.0   memory length: 100000   epsilon: 0.14114332000858404    steps: 329    lr: 2.560000000000001e-06     evaluation reward: 6.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2279   score: 4.0   memory length: 100000   epsilon: 0.14059486000858057    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 6.29\n",
      "episode: 2280   score: 9.0   memory length: 100000   epsilon: 0.13972366000857506    steps: 440    lr: 2.560000000000001e-06     evaluation reward: 6.33\n",
      "episode: 2281   score: 6.0   memory length: 100000   epsilon: 0.13900888000857053    steps: 361    lr: 2.560000000000001e-06     evaluation reward: 6.34\n",
      "episode: 2282   score: 11.0   memory length: 100000   epsilon: 0.13791592000856362    steps: 552    lr: 2.560000000000001e-06     evaluation reward: 6.38\n",
      "episode: 2283   score: 6.0   memory length: 100000   epsilon: 0.13720906000855915    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 6.35\n",
      "episode: 2284   score: 6.0   memory length: 100000   epsilon: 0.13657744000855515    steps: 319    lr: 2.560000000000001e-06     evaluation reward: 6.37\n",
      "episode: 2285   score: 7.0   memory length: 100000   epsilon: 0.1358111800085503    steps: 387    lr: 2.560000000000001e-06     evaluation reward: 6.4\n",
      "episode: 2286   score: 5.0   memory length: 100000   epsilon: 0.1351954000085464    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 6.39\n",
      "episode: 2287   score: 7.0   memory length: 100000   epsilon: 0.13443310000854158    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2288   score: 6.0   memory length: 100000   epsilon: 0.13376782000853737    steps: 336    lr: 2.560000000000001e-06     evaluation reward: 6.37\n",
      "episode: 2289   score: 5.0   memory length: 100000   epsilon: 0.1331540200085335    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 6.36\n",
      "episode: 2290   score: 7.0   memory length: 100000   epsilon: 0.13242538000852888    steps: 368    lr: 2.560000000000001e-06     evaluation reward: 6.38\n",
      "episode: 2291   score: 4.0   memory length: 100000   epsilon: 0.13179970000852492    steps: 316    lr: 2.560000000000001e-06     evaluation reward: 6.33\n",
      "episode: 2292   score: 7.0   memory length: 100000   epsilon: 0.13100176000851987    steps: 403    lr: 2.560000000000001e-06     evaluation reward: 6.33\n",
      "episode: 2293   score: 7.0   memory length: 100000   epsilon: 0.13016224000851456    steps: 424    lr: 2.560000000000001e-06     evaluation reward: 6.35\n",
      "episode: 2294   score: 8.0   memory length: 100000   epsilon: 0.12928906000850904    steps: 441    lr: 2.560000000000001e-06     evaluation reward: 6.4\n",
      "episode: 2295   score: 9.0   memory length: 100000   epsilon: 0.12829312000850274    steps: 503    lr: 2.560000000000001e-06     evaluation reward: 6.42\n",
      "episode: 2296   score: 3.0   memory length: 100000   epsilon: 0.12787336000850008    steps: 212    lr: 2.560000000000001e-06     evaluation reward: 6.42\n",
      "episode: 2297   score: 7.0   memory length: 100000   epsilon: 0.12703186000849476    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 6.38\n",
      "episode: 2298   score: 8.0   memory length: 100000   epsilon: 0.12622798000848967    steps: 406    lr: 2.560000000000001e-06     evaluation reward: 6.4\n",
      "episode: 2299   score: 3.0   memory length: 100000   epsilon: 0.12577654000848681    steps: 228    lr: 2.560000000000001e-06     evaluation reward: 6.4\n",
      "episode: 2300   score: 6.0   memory length: 100000   epsilon: 0.12502810000848208    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 6.39\n",
      "episode: 2301   score: 5.0   memory length: 100000   epsilon: 0.1244202400084823    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2302   score: 5.0   memory length: 100000   epsilon: 0.12373714000848277    steps: 345    lr: 2.560000000000001e-06     evaluation reward: 6.4\n",
      "episode: 2303   score: 9.0   memory length: 100000   epsilon: 0.12291742000848332    steps: 414    lr: 2.560000000000001e-06     evaluation reward: 6.4\n",
      "episode: 2304   score: 9.0   memory length: 100000   epsilon: 0.1220660200084839    steps: 430    lr: 2.560000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2305   score: 13.0   memory length: 100000   epsilon: 0.12094336000848467    steps: 567    lr: 2.560000000000001e-06     evaluation reward: 6.5\n",
      "episode: 2306   score: 5.0   memory length: 100000   epsilon: 0.12025630000848514    steps: 347    lr: 2.560000000000001e-06     evaluation reward: 6.47\n",
      "episode: 2307   score: 7.0   memory length: 100000   epsilon: 0.11949202000848566    steps: 386    lr: 2.560000000000001e-06     evaluation reward: 6.5\n",
      "episode: 2308   score: 8.0   memory length: 100000   epsilon: 0.11864656000848624    steps: 427    lr: 2.560000000000001e-06     evaluation reward: 6.54\n",
      "episode: 2309   score: 7.0   memory length: 100000   epsilon: 0.11788624000848676    steps: 384    lr: 2.560000000000001e-06     evaluation reward: 6.56\n",
      "episode: 2310   score: 7.0   memory length: 100000   epsilon: 0.11707444000848731    steps: 410    lr: 2.560000000000001e-06     evaluation reward: 6.58\n",
      "episode: 2311   score: 4.0   memory length: 100000   epsilon: 0.11656162000848766    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 6.55\n",
      "episode: 2312   score: 6.0   memory length: 100000   epsilon: 0.11581516000848817    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 6.5\n",
      "episode: 2313   score: 6.0   memory length: 100000   epsilon: 0.11514394000848863    steps: 339    lr: 2.560000000000001e-06     evaluation reward: 6.51\n",
      "episode: 2314   score: 8.0   memory length: 100000   epsilon: 0.11435392000848917    steps: 399    lr: 2.560000000000001e-06     evaluation reward: 6.52\n",
      "episode: 2315   score: 5.0   memory length: 100000   epsilon: 0.11374804000848958    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 6.49\n",
      "episode: 2316   score: 5.0   memory length: 100000   epsilon: 0.11313424000849    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 6.5\n",
      "episode: 2317   score: 6.0   memory length: 100000   epsilon: 0.11242540000849048    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 6.51\n",
      "episode: 2318   score: 11.0   memory length: 100000   epsilon: 0.11139382000849118    steps: 521    lr: 2.560000000000001e-06     evaluation reward: 6.58\n",
      "episode: 2319   score: 6.0   memory length: 100000   epsilon: 0.11061964000849171    steps: 391    lr: 2.560000000000001e-06     evaluation reward: 6.62\n",
      "episode: 2320   score: 7.0   memory length: 100000   epsilon: 0.1097623000084923    steps: 433    lr: 2.560000000000001e-06     evaluation reward: 6.63\n",
      "episode: 2321   score: 6.0   memory length: 100000   epsilon: 0.1090198000084928    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 6.62\n",
      "episode: 2322   score: 8.0   memory length: 100000   epsilon: 0.10804564000849347    steps: 492    lr: 2.560000000000001e-06     evaluation reward: 6.67\n",
      "episode: 2323   score: 4.0   memory length: 100000   epsilon: 0.1075645000084938    steps: 243    lr: 2.560000000000001e-06     evaluation reward: 6.65\n",
      "episode: 2324   score: 3.0   memory length: 100000   epsilon: 0.1071130600084941    steps: 228    lr: 2.560000000000001e-06     evaluation reward: 6.63\n",
      "episode: 2325   score: 4.0   memory length: 100000   epsilon: 0.10663192000849443    steps: 243    lr: 2.560000000000001e-06     evaluation reward: 6.63\n",
      "episode: 2326   score: 7.0   memory length: 100000   epsilon: 0.10590328000849493    steps: 368    lr: 2.560000000000001e-06     evaluation reward: 6.65\n",
      "episode: 2327   score: 9.0   memory length: 100000   epsilon: 0.10503406000849552    steps: 439    lr: 2.560000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2328   score: 15.0   memory length: 100000   epsilon: 0.10382824000849634    steps: 609    lr: 2.560000000000001e-06     evaluation reward: 6.78\n",
      "episode: 2329   score: 8.0   memory length: 100000   epsilon: 0.1030045600084969    steps: 416    lr: 2.560000000000001e-06     evaluation reward: 6.73\n",
      "episode: 2330   score: 6.0   memory length: 100000   epsilon: 0.10226008000849741    steps: 376    lr: 2.560000000000001e-06     evaluation reward: 6.74\n",
      "episode: 2331   score: 2.0   memory length: 100000   epsilon: 0.10186210000849769    steps: 201    lr: 2.560000000000001e-06     evaluation reward: 6.65\n",
      "episode: 2332   score: 3.0   memory length: 100000   epsilon: 0.101406700008498    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 6.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2333   score: 8.0   memory length: 100000   epsilon: 0.10061074000849854    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 6.6\n",
      "episode: 2334   score: 9.0   memory length: 100000   epsilon: 0.09971776000849915    steps: 451    lr: 2.560000000000001e-06     evaluation reward: 6.65\n",
      "episode: 2335   score: 7.0   memory length: 100000   epsilon: 0.09887824000849972    steps: 424    lr: 2.560000000000001e-06     evaluation reward: 6.65\n",
      "episode: 2336   score: 8.0   memory length: 100000   epsilon: 0.09811000000850024    steps: 388    lr: 2.560000000000001e-06     evaluation reward: 6.66\n",
      "episode: 2337   score: 8.0   memory length: 100000   epsilon: 0.09726850000850082    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 6.63\n",
      "episode: 2338   score: 10.0   memory length: 100000   epsilon: 0.0962785000085015    steps: 500    lr: 2.560000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2339   score: 4.0   memory length: 100000   epsilon: 0.09573202000850187    steps: 276    lr: 2.560000000000001e-06     evaluation reward: 6.64\n",
      "episode: 2340   score: 4.0   memory length: 100000   epsilon: 0.09521524000850222    steps: 261    lr: 2.560000000000001e-06     evaluation reward: 6.57\n",
      "episode: 2341   score: 8.0   memory length: 100000   epsilon: 0.09427870000850286    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 6.6\n",
      "episode: 2342   score: 12.0   memory length: 100000   epsilon: 0.09302338000850371    steps: 634    lr: 2.560000000000001e-06     evaluation reward: 6.67\n",
      "episode: 2343   score: 3.0   memory length: 100000   epsilon: 0.09257392000850402    steps: 227    lr: 2.560000000000001e-06     evaluation reward: 6.6\n",
      "episode: 2344   score: 11.0   memory length: 100000   epsilon: 0.09135622000850485    steps: 615    lr: 2.560000000000001e-06     evaluation reward: 6.65\n",
      "episode: 2345   score: 6.0   memory length: 100000   epsilon: 0.09065134000850533    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 6.69\n",
      "episode: 2346   score: 4.0   memory length: 100000   epsilon: 0.09013060000850569    steps: 263    lr: 2.560000000000001e-06     evaluation reward: 6.63\n",
      "episode: 2347   score: 7.0   memory length: 100000   epsilon: 0.08928910000850626    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 6.62\n",
      "episode: 2348   score: 7.0   memory length: 100000   epsilon: 0.08855650000850676    steps: 370    lr: 2.560000000000001e-06     evaluation reward: 6.62\n",
      "episode: 2349   score: 7.0   memory length: 100000   epsilon: 0.0880634800085071    steps: 249    lr: 2.560000000000001e-06     evaluation reward: 6.62\n",
      "episode: 2350   score: 7.0   memory length: 100000   epsilon: 0.08736850000850757    steps: 351    lr: 2.560000000000001e-06     evaluation reward: 6.67\n",
      "episode: 2351   score: 6.0   memory length: 100000   epsilon: 0.08657650000850811    steps: 400    lr: 2.560000000000001e-06     evaluation reward: 6.66\n",
      "episode: 2352   score: 6.0   memory length: 100000   epsilon: 0.08583994000850861    steps: 372    lr: 2.560000000000001e-06     evaluation reward: 6.66\n",
      "episode: 2353   score: 7.0   memory length: 100000   epsilon: 0.08509744000850912    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 6.6\n",
      "episode: 2354   score: 7.0   memory length: 100000   epsilon: 0.08435692000850963    steps: 374    lr: 2.560000000000001e-06     evaluation reward: 6.64\n",
      "episode: 2355   score: 6.0   memory length: 100000   epsilon: 0.08368570000851008    steps: 339    lr: 2.560000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2356   score: 6.0   memory length: 100000   epsilon: 0.08294320000851059    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 6.7\n",
      "episode: 2357   score: 6.0   memory length: 100000   epsilon: 0.08220862000851109    steps: 371    lr: 2.560000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2358   score: 7.0   memory length: 100000   epsilon: 0.08145028000851161    steps: 383    lr: 2.560000000000001e-06     evaluation reward: 6.7\n",
      "episode: 2359   score: 9.0   memory length: 100000   epsilon: 0.0804365200085123    steps: 512    lr: 2.560000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2360   score: 12.0   memory length: 100000   epsilon: 0.07939504000851301    steps: 526    lr: 2.560000000000001e-06     evaluation reward: 6.75\n",
      "episode: 2361   score: 5.0   memory length: 100000   epsilon: 0.0788208400085134    steps: 290    lr: 2.560000000000001e-06     evaluation reward: 6.69\n",
      "episode: 2362   score: 6.0   memory length: 100000   epsilon: 0.07813972000851387    steps: 344    lr: 2.560000000000001e-06     evaluation reward: 6.7\n",
      "episode: 2363   score: 6.0   memory length: 100000   epsilon: 0.07751404000851429    steps: 316    lr: 2.560000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2364   score: 5.0   memory length: 100000   epsilon: 0.07690420000851471    steps: 308    lr: 2.560000000000001e-06     evaluation reward: 6.69\n",
      "episode: 2365   score: 3.0   memory length: 100000   epsilon: 0.076484440008515    steps: 212    lr: 2.560000000000001e-06     evaluation reward: 6.67\n",
      "episode: 2366   score: 11.0   memory length: 100000   epsilon: 0.07543306000851571    steps: 531    lr: 2.560000000000001e-06     evaluation reward: 6.7\n",
      "episode: 2367   score: 5.0   memory length: 100000   epsilon: 0.07479154000851615    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 6.66\n",
      "episode: 2368   score: 7.0   memory length: 100000   epsilon: 0.0739896400085167    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2369   score: 6.0   memory length: 100000   epsilon: 0.07328674000851718    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2370   score: 4.0   memory length: 100000   epsilon: 0.07273432000851755    steps: 279    lr: 2.560000000000001e-06     evaluation reward: 6.64\n",
      "episode: 2371   score: 6.0   memory length: 100000   epsilon: 0.07198786000851806    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 6.65\n",
      "episode: 2372   score: 5.0   memory length: 100000   epsilon: 0.07140574000851846    steps: 294    lr: 2.560000000000001e-06     evaluation reward: 6.63\n",
      "episode: 2373   score: 6.0   memory length: 100000   epsilon: 0.07069492000851894    steps: 359    lr: 2.560000000000001e-06     evaluation reward: 6.61\n",
      "episode: 2374   score: 9.0   memory length: 100000   epsilon: 0.06978016000851957    steps: 462    lr: 2.560000000000001e-06     evaluation reward: 6.67\n",
      "episode: 2375   score: 7.0   memory length: 100000   epsilon: 0.06903766000852007    steps: 375    lr: 2.560000000000001e-06     evaluation reward: 6.68\n",
      "episode: 2376   score: 10.0   memory length: 100000   epsilon: 0.06807142000852073    steps: 488    lr: 2.560000000000001e-06     evaluation reward: 6.71\n",
      "episode: 2377   score: 15.0   memory length: 100000   epsilon: 0.06688936000852154    steps: 597    lr: 2.560000000000001e-06     evaluation reward: 6.78\n",
      "episode: 2378   score: 7.0   memory length: 100000   epsilon: 0.06608746000852209    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 6.8\n",
      "episode: 2379   score: 11.0   memory length: 100000   epsilon: 0.06505984000852279    steps: 519    lr: 2.560000000000001e-06     evaluation reward: 6.87\n",
      "episode: 2380   score: 5.0   memory length: 100000   epsilon: 0.06448366000852318    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 6.83\n",
      "episode: 2381   score: 7.0   memory length: 100000   epsilon: 0.0637134400085237    steps: 389    lr: 2.560000000000001e-06     evaluation reward: 6.84\n",
      "episode: 2382   score: 9.0   memory length: 100000   epsilon: 0.06281056000852432    steps: 456    lr: 2.560000000000001e-06     evaluation reward: 6.82\n",
      "episode: 2383   score: 8.0   memory length: 100000   epsilon: 0.061868080008524964    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 6.84\n",
      "episode: 2384   score: 9.0   memory length: 100000   epsilon: 0.06097708000852557    steps: 450    lr: 2.560000000000001e-06     evaluation reward: 6.87\n",
      "episode: 2385   score: 8.0   memory length: 100000   epsilon: 0.06010192000852617    steps: 442    lr: 2.560000000000001e-06     evaluation reward: 6.88\n",
      "episode: 2386   score: 7.0   memory length: 100000   epsilon: 0.059302000008526715    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 6.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2387   score: 5.0   memory length: 100000   epsilon: 0.05868820000852713    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 6.88\n",
      "episode: 2388   score: 10.0   memory length: 100000   epsilon: 0.05769622000852781    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 6.92\n",
      "episode: 2389   score: 8.0   memory length: 100000   epsilon: 0.05684086000852839    steps: 432    lr: 2.560000000000001e-06     evaluation reward: 6.95\n",
      "episode: 2390   score: 4.0   memory length: 100000   epsilon: 0.05628646000852877    steps: 280    lr: 2.560000000000001e-06     evaluation reward: 6.92\n",
      "episode: 2391   score: 6.0   memory length: 100000   epsilon: 0.05554000000852928    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 6.94\n",
      "episode: 2392   score: 4.0   memory length: 100000   epsilon: 0.05502718000852963    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 6.91\n",
      "episode: 2393   score: 6.0   memory length: 100000   epsilon: 0.05438368000853007    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2394   score: 8.0   memory length: 100000   epsilon: 0.0536095000085306    steps: 391    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2395   score: 9.0   memory length: 100000   epsilon: 0.05270068000853122    steps: 459    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2396   score: 9.0   memory length: 100000   epsilon: 0.05174830000853187    steps: 481    lr: 2.560000000000001e-06     evaluation reward: 6.96\n",
      "episode: 2397   score: 4.0   memory length: 100000   epsilon: 0.05123350000853222    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 6.93\n",
      "episode: 2398   score: 9.0   memory length: 100000   epsilon: 0.05029498000853286    steps: 474    lr: 2.560000000000001e-06     evaluation reward: 6.94\n",
      "episode: 2399   score: 6.0   memory length: 100000   epsilon: 0.04955842000853336    steps: 372    lr: 2.560000000000001e-06     evaluation reward: 6.97\n",
      "episode: 2400   score: 9.0   memory length: 100000   epsilon: 0.04852288000853407    steps: 523    lr: 2.560000000000001e-06     evaluation reward: 7.0\n",
      "episode: 2401   score: 12.0   memory length: 100000   epsilon: 0.0473032000085349    steps: 616    lr: 2.560000000000001e-06     evaluation reward: 7.07\n",
      "episode: 2402   score: 8.0   memory length: 100000   epsilon: 0.04646170000853547    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 7.1\n",
      "episode: 2403   score: 3.0   memory length: 100000   epsilon: 0.04593700000853583    steps: 265    lr: 2.560000000000001e-06     evaluation reward: 7.04\n",
      "episode: 2404   score: 12.0   memory length: 100000   epsilon: 0.044976700008536485    steps: 485    lr: 2.560000000000001e-06     evaluation reward: 7.07\n",
      "episode: 2405   score: 7.0   memory length: 100000   epsilon: 0.044168860008537036    steps: 408    lr: 2.560000000000001e-06     evaluation reward: 7.01\n",
      "episode: 2406   score: 10.0   memory length: 100000   epsilon: 0.043190740008537704    steps: 494    lr: 2.560000000000001e-06     evaluation reward: 7.06\n",
      "episode: 2407   score: 6.0   memory length: 100000   epsilon: 0.04258090000853812    steps: 308    lr: 2.560000000000001e-06     evaluation reward: 7.05\n",
      "episode: 2408   score: 9.0   memory length: 100000   epsilon: 0.04164436000853876    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 7.06\n",
      "episode: 2409   score: 11.0   memory length: 100000   epsilon: 0.04061872000853946    steps: 518    lr: 2.560000000000001e-06     evaluation reward: 7.1\n",
      "episode: 2410   score: 6.0   memory length: 100000   epsilon: 0.03991582000853994    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 7.09\n",
      "episode: 2411   score: 13.0   memory length: 100000   epsilon: 0.03869812000854077    steps: 615    lr: 2.560000000000001e-06     evaluation reward: 7.18\n",
      "episode: 2412   score: 6.0   memory length: 100000   epsilon: 0.038042740008541215    steps: 331    lr: 2.560000000000001e-06     evaluation reward: 7.18\n",
      "episode: 2413   score: 9.0   memory length: 100000   epsilon: 0.037003240008541924    steps: 525    lr: 2.560000000000001e-06     evaluation reward: 7.21\n",
      "episode: 2414   score: 6.0   memory length: 100000   epsilon: 0.03629242000854241    steps: 359    lr: 2.560000000000001e-06     evaluation reward: 7.19\n",
      "episode: 2415   score: 10.0   memory length: 100000   epsilon: 0.03523708000854313    steps: 533    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
      "episode: 2416   score: 6.0   memory length: 100000   epsilon: 0.03457774000854358    steps: 333    lr: 2.560000000000001e-06     evaluation reward: 7.25\n",
      "episode: 2417   score: 5.0   memory length: 100000   epsilon: 0.033963940008544    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
      "episode: 2418   score: 6.0   memory length: 100000   epsilon: 0.0332293600085445    steps: 371    lr: 2.560000000000001e-06     evaluation reward: 7.19\n",
      "episode: 2419   score: 8.0   memory length: 100000   epsilon: 0.03228886000854514    steps: 475    lr: 2.560000000000001e-06     evaluation reward: 7.21\n",
      "episode: 2420   score: 7.0   memory length: 100000   epsilon: 0.03149686000854568    steps: 400    lr: 2.560000000000001e-06     evaluation reward: 7.21\n",
      "episode: 2421   score: 9.0   memory length: 100000   epsilon: 0.03064744000854626    steps: 429    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
      "episode: 2422   score: 8.0   memory length: 100000   epsilon: 0.029788120008546845    steps: 434    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
      "episode: 2423   score: 9.0   memory length: 100000   epsilon: 0.028833760008547496    steps: 482    lr: 2.560000000000001e-06     evaluation reward: 7.29\n",
      "episode: 2424   score: 5.0   memory length: 100000   epsilon: 0.028235800008547904    steps: 302    lr: 2.560000000000001e-06     evaluation reward: 7.31\n",
      "episode: 2425   score: 8.0   memory length: 100000   epsilon: 0.027410140008548467    steps: 417    lr: 2.560000000000001e-06     evaluation reward: 7.35\n",
      "episode: 2426   score: 8.0   memory length: 100000   epsilon: 0.026626060008549002    steps: 396    lr: 2.560000000000001e-06     evaluation reward: 7.36\n",
      "episode: 2427   score: 8.0   memory length: 100000   epsilon: 0.025739020008549607    steps: 448    lr: 2.560000000000001e-06     evaluation reward: 7.35\n",
      "episode: 2428   score: 11.0   memory length: 100000   epsilon: 0.0248698000085502    steps: 439    lr: 2.560000000000001e-06     evaluation reward: 7.31\n",
      "episode: 2429   score: 5.0   memory length: 100000   epsilon: 0.02422432000855064    steps: 326    lr: 2.560000000000001e-06     evaluation reward: 7.28\n",
      "episode: 2430   score: 11.0   memory length: 100000   epsilon: 0.023119480008551394    steps: 558    lr: 2.560000000000001e-06     evaluation reward: 7.33\n",
      "episode: 2431   score: 7.0   memory length: 100000   epsilon: 0.022329460008551932    steps: 399    lr: 2.560000000000001e-06     evaluation reward: 7.38\n",
      "episode: 2432   score: 8.0   memory length: 100000   epsilon: 0.021430540008552545    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
      "episode: 2433   score: 7.0   memory length: 100000   epsilon: 0.020620720008553098    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 7.42\n",
      "episode: 2434   score: 10.0   memory length: 100000   epsilon: 0.019630720008553773    steps: 500    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
      "episode: 2435   score: 9.0   memory length: 100000   epsilon: 0.01875358000855437    steps: 443    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
      "episode: 2436   score: 8.0   memory length: 100000   epsilon: 0.01789228000855496    steps: 435    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
      "episode: 2437   score: 11.0   memory length: 100000   epsilon: 0.01675972000855573    steps: 572    lr: 2.560000000000001e-06     evaluation reward: 7.48\n",
      "episode: 2438   score: 5.0   memory length: 100000   epsilon: 0.016151860008556146    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 7.43\n",
      "episode: 2439   score: 6.0   memory length: 100000   epsilon: 0.01549648000855648    steps: 331    lr: 2.560000000000001e-06     evaluation reward: 7.45\n",
      "episode: 2440   score: 4.0   memory length: 100000   epsilon: 0.015019300008556388    steps: 241    lr: 2.560000000000001e-06     evaluation reward: 7.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2441   score: 9.0   memory length: 100000   epsilon: 0.014047120008556199    steps: 491    lr: 2.560000000000001e-06     evaluation reward: 7.46\n",
      "episode: 2442   score: 5.0   memory length: 100000   epsilon: 0.013504600008556094    steps: 274    lr: 2.560000000000001e-06     evaluation reward: 7.39\n",
      "episode: 2443   score: 10.0   memory length: 100000   epsilon: 0.012463120008555892    steps: 526    lr: 2.560000000000001e-06     evaluation reward: 7.46\n",
      "episode: 2444   score: 4.0   memory length: 100000   epsilon: 0.011950300008555792    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 7.39\n",
      "episode: 2445   score: 7.0   memory length: 100000   epsilon: 0.011225620008555651    steps: 366    lr: 2.560000000000001e-06     evaluation reward: 7.4\n",
      "episode: 2446   score: 4.0   memory length: 100000   epsilon: 0.010744480008555558    steps: 243    lr: 2.560000000000001e-06     evaluation reward: 7.4\n",
      "episode: 2447   score: 6.0   memory length: 100000   epsilon: 0.010112860008555435    steps: 319    lr: 2.560000000000001e-06     evaluation reward: 7.39\n",
      "episode: 2448   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 278    lr: 1.0240000000000005e-06     evaluation reward: 7.36\n",
      "episode: 2449   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 7.37\n",
      "episode: 2450   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 421    lr: 1.0240000000000005e-06     evaluation reward: 7.37\n",
      "episode: 2451   score: 11.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 551    lr: 1.0240000000000005e-06     evaluation reward: 7.42\n",
      "episode: 2452   score: 11.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 565    lr: 1.0240000000000005e-06     evaluation reward: 7.47\n",
      "episode: 2453   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 385    lr: 1.0240000000000005e-06     evaluation reward: 7.47\n",
      "episode: 2454   score: 12.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 560    lr: 1.0240000000000005e-06     evaluation reward: 7.52\n",
      "episode: 2455   score: 11.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 608    lr: 1.0240000000000005e-06     evaluation reward: 7.57\n",
      "episode: 2456   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 417    lr: 1.0240000000000005e-06     evaluation reward: 7.59\n",
      "episode: 2457   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 368    lr: 1.0240000000000005e-06     evaluation reward: 7.6\n",
      "episode: 2458   score: 13.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 588    lr: 1.0240000000000005e-06     evaluation reward: 7.66\n",
      "episode: 2459   score: 14.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 537    lr: 1.0240000000000005e-06     evaluation reward: 7.71\n",
      "episode: 2460   score: 11.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 543    lr: 1.0240000000000005e-06     evaluation reward: 7.7\n",
      "episode: 2461   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 494    lr: 1.0240000000000005e-06     evaluation reward: 7.75\n",
      "episode: 2462   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 432    lr: 1.0240000000000005e-06     evaluation reward: 7.77\n",
      "episode: 2463   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 241    lr: 1.0240000000000005e-06     evaluation reward: 7.75\n",
      "episode: 2464   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 431    lr: 1.0240000000000005e-06     evaluation reward: 7.78\n",
      "episode: 2465   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 241    lr: 1.0240000000000005e-06     evaluation reward: 7.79\n",
      "episode: 2466   score: 13.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 611    lr: 1.0240000000000005e-06     evaluation reward: 7.81\n",
      "episode: 2467   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 243    lr: 1.0240000000000005e-06     evaluation reward: 7.8\n",
      "episode: 2468   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 214    lr: 1.0240000000000005e-06     evaluation reward: 7.76\n",
      "episode: 2469   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 514    lr: 1.0240000000000005e-06     evaluation reward: 7.8\n",
      "episode: 2470   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 417    lr: 1.0240000000000005e-06     evaluation reward: 7.84\n",
      "episode: 2471   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 289    lr: 1.0240000000000005e-06     evaluation reward: 7.83\n",
      "episode: 2472   score: 11.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 513    lr: 1.0240000000000005e-06     evaluation reward: 7.89\n",
      "episode: 2473   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 214    lr: 1.0240000000000005e-06     evaluation reward: 7.86\n",
      "episode: 2474   score: 12.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 610    lr: 1.0240000000000005e-06     evaluation reward: 7.89\n",
      "episode: 2475   score: 12.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 486    lr: 1.0240000000000005e-06     evaluation reward: 7.94\n",
      "episode: 2476   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 455    lr: 1.0240000000000005e-06     evaluation reward: 7.93\n",
      "episode: 2477   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 242    lr: 1.0240000000000005e-06     evaluation reward: 7.82\n",
      "episode: 2478   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 474    lr: 1.0240000000000005e-06     evaluation reward: 7.84\n",
      "episode: 2479   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 362    lr: 1.0240000000000005e-06     evaluation reward: 7.8\n",
      "episode: 2480   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 460    lr: 1.0240000000000005e-06     evaluation reward: 7.83\n",
      "episode: 2481   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 435    lr: 1.0240000000000005e-06     evaluation reward: 7.84\n",
      "episode: 2482   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 481    lr: 1.0240000000000005e-06     evaluation reward: 7.84\n",
      "episode: 2483   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 545    lr: 1.0240000000000005e-06     evaluation reward: 7.86\n",
      "episode: 2484   score: 12.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 538    lr: 1.0240000000000005e-06     evaluation reward: 7.89\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24768/3498000806.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# Start training after random sample generation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[1;31m# Update the target network only for Double DQN only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdouble_dqn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mupdate_target_network_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\assignment5\\assignment5_materials\\agent.py\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Compute Q(s_t, a), the Q-value of the current state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;31m### CODE ####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0mQ_values_all_actions_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[1;31m# gathers all values according to action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mQ_values_current\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ_values_all_actions_current\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\assignment5\\assignment5_materials\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 443\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for e in range(start, EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        \n",
    "        # notes: memory management for preventing overuse of memory\n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "#         agent.memory.half_length()\n",
    "        \n",
    "\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "#                 torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                torch.save(agent.policy_net.state_dict(), f\"./breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(agent.memory)\n",
    "# params = dict()\n",
    "# # params[\"epsilon\"] = agent.epsilon\n",
    "# # params[\"agent_memory\"]= agent.memory\n",
    "# params[\"rewards\"] = rewards \n",
    "# params[\"episodes\"] = episodes\n",
    "# params[\"best_eval_reward\"] = best_eval_reward\n",
    "# joblib.dump(agent,\"prev_agent\")\n",
    "# joblib.dump(params,\"prev_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net.state_dict(), f\"./save_model/breakout_DQN_2000_episodes_huber.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
