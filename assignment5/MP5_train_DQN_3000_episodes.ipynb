{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !apt update && apt install -y libpq-dev libsdl2-dev swig xorg-dev xvfb\n",
    "# %pip install -U tf-agents pyvirtualdisplay\n",
    "# %pip install -U gym[box2d,atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -U gym>=0.21.0\n",
    "# !pip3 install --upgrade setuptools --user\n",
    "# !pip3 install ez_setup \n",
    "# !pip3 install -U gym[box2d,atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_training = False\n",
    "\n",
    "if continue_training == False:\n",
    "    rewards, episodes = [], []\n",
    "    best_eval_reward = 0\n",
    "    start = 0\n",
    "else:\n",
    "    # load training params\n",
    "#     agent = joblib.load(\"prev_agent\")\n",
    "#     params = joblib.load(\"prev_params\")\n",
    "    # load training model\n",
    "    agent.policy_net.load_state_dict(torch.load('./save_model/breakout_DQN_2021_episodes.pth', map_location=torch.device('cpu')))\n",
    "#     agent.memory = params[\"agent_memory\"]\n",
    "#     agent.epsilon = params[\"epsilon\"]\n",
    "#     rewards = params[\"rewards\"]\n",
    "#     episodes = params[\"episodes\"]\n",
    "#     best_eval_reward = params[\"best_eval_reward\"]\n",
    "#     start = params[\"episodes\"][-1]  \n",
    "    start = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 0.0   memory length: 123   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.0\n",
      "episode: 1   score: 3.0   memory length: 353   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 2   score: 2.0   memory length: 571   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 3   score: 0.0   memory length: 695   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 4   score: 1.0   memory length: 847   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 5   score: 1.0   memory length: 1020   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
      "episode: 6   score: 1.0   memory length: 1189   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.1428571428571428\n",
      "episode: 7   score: 3.0   memory length: 1418   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 8   score: 2.0   memory length: 1635   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
      "episode: 9   score: 2.0   memory length: 1834   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 10   score: 3.0   memory length: 2086   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 11   score: 0.0   memory length: 2210   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 12   score: 1.0   memory length: 2362   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.4615384615384615\n",
      "episode: 13   score: 1.0   memory length: 2531   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 14   score: 1.0   memory length: 2682   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 15   score: 0.0   memory length: 2806   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3125\n",
      "episode: 16   score: 2.0   memory length: 3009   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.3529411764705883\n",
      "episode: 17   score: 0.0   memory length: 3133   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2777777777777777\n",
      "episode: 18   score: 2.0   memory length: 3332   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.3157894736842106\n",
      "episode: 19   score: 2.0   memory length: 3552   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 20   score: 0.0   memory length: 3676   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
      "episode: 21   score: 0.0   memory length: 3799   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2272727272727273\n",
      "episode: 22   score: 0.0   memory length: 3922   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.173913043478261\n",
      "episode: 23   score: 2.0   memory length: 4120   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.2083333333333333\n",
      "episode: 24   score: 1.0   memory length: 4290   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 25   score: 2.0   memory length: 4507   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.2307692307692308\n",
      "episode: 26   score: 4.0   memory length: 4777   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 27   score: 3.0   memory length: 5021   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.3928571428571428\n",
      "episode: 28   score: 2.0   memory length: 5221   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.4137931034482758\n",
      "episode: 29   score: 2.0   memory length: 5440   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.4333333333333333\n",
      "episode: 30   score: 1.0   memory length: 5609   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4193548387096775\n",
      "episode: 31   score: 5.0   memory length: 5933   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.53125\n",
      "episode: 32   score: 5.0   memory length: 6272   epsilon: 1.0    steps: 339    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 33   score: 4.0   memory length: 6569   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.7058823529411764\n",
      "episode: 34   score: 0.0   memory length: 6693   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6571428571428573\n",
      "episode: 35   score: 0.0   memory length: 6817   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6111111111111112\n",
      "episode: 36   score: 4.0   memory length: 7114   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.6756756756756757\n",
      "episode: 37   score: 1.0   memory length: 7284   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6578947368421053\n",
      "episode: 38   score: 8.0   memory length: 7614   epsilon: 1.0    steps: 330    lr: 0.0001     evaluation reward: 1.8205128205128205\n",
      "episode: 39   score: 0.0   memory length: 7737   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.775\n",
      "episode: 40   score: 0.0   memory length: 7860   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7317073170731707\n",
      "episode: 41   score: 0.0   memory length: 7984   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6904761904761905\n",
      "episode: 42   score: 1.0   memory length: 8135   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6744186046511629\n",
      "episode: 43   score: 1.0   memory length: 8305   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6590909090909092\n",
      "episode: 44   score: 2.0   memory length: 8490   epsilon: 1.0    steps: 185    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 45   score: 3.0   memory length: 8740   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.6956521739130435\n",
      "episode: 46   score: 2.0   memory length: 8939   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.702127659574468\n",
      "episode: 47   score: 2.0   memory length: 9159   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.7083333333333333\n",
      "episode: 48   score: 1.0   memory length: 9328   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6938775510204083\n",
      "episode: 49   score: 0.0   memory length: 9452   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 50   score: 0.0   memory length: 9576   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6274509803921569\n",
      "episode: 51   score: 2.0   memory length: 9793   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.6346153846153846\n",
      "episode: 52   score: 0.0   memory length: 9917   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6037735849056605\n",
      "episode: 53   score: 4.0   memory length: 10214   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.6481481481481481\n",
      "episode: 54   score: 4.0   memory length: 10528   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.690909090909091\n",
      "episode: 55   score: 4.0   memory length: 10824   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.7321428571428572\n",
      "episode: 56   score: 3.0   memory length: 11070   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.7543859649122806\n",
      "episode: 57   score: 0.0   memory length: 11194   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.7241379310344827\n",
      "episode: 58   score: 1.0   memory length: 11346   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.7118644067796611\n",
      "episode: 59   score: 2.0   memory length: 11545   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.7166666666666666\n",
      "episode: 60   score: 2.0   memory length: 11743   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.721311475409836\n",
      "episode: 61   score: 3.0   memory length: 12013   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.7419354838709677\n",
      "episode: 62   score: 0.0   memory length: 12137   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
      "episode: 63   score: 2.0   memory length: 12357   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.71875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 64   score: 2.0   memory length: 12556   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.7230769230769232\n",
      "episode: 65   score: 2.0   memory length: 12755   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.7272727272727273\n",
      "episode: 66   score: 4.0   memory length: 13033   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.7611940298507462\n",
      "episode: 67   score: 0.0   memory length: 13157   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.7352941176470589\n",
      "episode: 68   score: 1.0   memory length: 13327   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.7246376811594204\n",
      "episode: 69   score: 1.0   memory length: 13497   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
      "episode: 70   score: 1.0   memory length: 13649   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.704225352112676\n",
      "episode: 71   score: 1.0   memory length: 13818   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6944444444444444\n",
      "episode: 72   score: 0.0   memory length: 13942   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6712328767123288\n",
      "episode: 73   score: 0.0   memory length: 14065   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6486486486486487\n",
      "episode: 74   score: 5.0   memory length: 14435   epsilon: 1.0    steps: 370    lr: 0.0001     evaluation reward: 1.6933333333333334\n",
      "episode: 75   score: 0.0   memory length: 14559   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6710526315789473\n",
      "episode: 76   score: 0.0   memory length: 14683   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6493506493506493\n",
      "episode: 77   score: 0.0   memory length: 14807   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6282051282051282\n",
      "episode: 78   score: 3.0   memory length: 15054   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6455696202531647\n",
      "episode: 79   score: 2.0   memory length: 15273   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 80   score: 0.0   memory length: 15397   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6296296296296295\n",
      "episode: 81   score: 0.0   memory length: 15520   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6097560975609757\n",
      "episode: 82   score: 0.0   memory length: 15644   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5903614457831325\n",
      "episode: 83   score: 6.0   memory length: 16013   epsilon: 1.0    steps: 369    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
      "episode: 84   score: 3.0   memory length: 16260   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.6588235294117648\n",
      "episode: 85   score: 2.0   memory length: 16458   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6627906976744187\n",
      "episode: 86   score: 1.0   memory length: 16627   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6551724137931034\n",
      "episode: 87   score: 0.0   memory length: 16750   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6363636363636365\n",
      "episode: 88   score: 1.0   memory length: 16923   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.6292134831460674\n",
      "episode: 89   score: 0.0   memory length: 17047   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6111111111111112\n",
      "episode: 90   score: 2.0   memory length: 17246   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.6153846153846154\n",
      "episode: 91   score: 1.0   memory length: 17415   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.608695652173913\n",
      "episode: 92   score: 0.0   memory length: 17539   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5913978494623655\n",
      "episode: 93   score: 1.0   memory length: 17692   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.5851063829787233\n",
      "episode: 94   score: 0.0   memory length: 17815   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.568421052631579\n",
      "episode: 95   score: 0.0   memory length: 17938   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5520833333333333\n",
      "episode: 96   score: 1.0   memory length: 18107   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5463917525773196\n",
      "episode: 97   score: 2.0   memory length: 18305   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5510204081632653\n",
      "episode: 98   score: 0.0   memory length: 18429   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5353535353535352\n",
      "episode: 99   score: 3.0   memory length: 18656   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 100   score: 3.0   memory length: 18884   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 101   score: 2.0   memory length: 19083   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 102   score: 3.0   memory length: 19312   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 103   score: 0.0   memory length: 19436   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 104   score: 1.0   memory length: 19589   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 105   score: 0.0   memory length: 19712   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 106   score: 5.0   memory length: 20049   epsilon: 1.0    steps: 337    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 107   score: 1.0   memory length: 20221   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 108   score: 1.0   memory length: 20394   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 109   score: 0.0   memory length: 20518   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 110   score: 1.0   memory length: 20670   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 111   score: 0.0   memory length: 20794   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 112   score: 1.0   memory length: 20964   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 113   score: 0.0   memory length: 21087   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 114   score: 0.0   memory length: 21210   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 115   score: 2.0   memory length: 21408   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 116   score: 2.0   memory length: 21631   epsilon: 1.0    steps: 223    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 117   score: 2.0   memory length: 21850   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 118   score: 0.0   memory length: 21974   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 119   score: 0.0   memory length: 22097   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 120   score: 2.0   memory length: 22318   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 121   score: 6.0   memory length: 22680   epsilon: 1.0    steps: 362    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 122   score: 1.0   memory length: 22851   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 123   score: 0.0   memory length: 22975   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 124   score: 4.0   memory length: 23300   epsilon: 1.0    steps: 325    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 125   score: 0.0   memory length: 23423   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 126   score: 0.0   memory length: 23547   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 127   score: 1.0   memory length: 23716   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 128   score: 0.0   memory length: 23840   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 129   score: 3.0   memory length: 24066   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 130   score: 1.0   memory length: 24218   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 131   score: 2.0   memory length: 24417   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 132   score: 2.0   memory length: 24616   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 133   score: 2.0   memory length: 24815   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 134   score: 1.0   memory length: 24985   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 135   score: 1.0   memory length: 25137   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 136   score: 2.0   memory length: 25356   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 137   score: 4.0   memory length: 25651   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 138   score: 0.0   memory length: 25775   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 139   score: 4.0   memory length: 26070   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 140   score: 0.0   memory length: 26194   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 141   score: 3.0   memory length: 26421   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 142   score: 6.0   memory length: 26812   epsilon: 1.0    steps: 391    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 143   score: 3.0   memory length: 27039   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 144   score: 2.0   memory length: 27258   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 145   score: 2.0   memory length: 27477   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 146   score: 2.0   memory length: 27676   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 147   score: 2.0   memory length: 27877   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 148   score: 1.0   memory length: 28047   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 149   score: 0.0   memory length: 28171   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 150   score: 3.0   memory length: 28419   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 151   score: 0.0   memory length: 28542   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 152   score: 1.0   memory length: 28711   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 153   score: 1.0   memory length: 28881   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 154   score: 0.0   memory length: 29005   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 155   score: 3.0   memory length: 29234   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 156   score: 2.0   memory length: 29435   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 157   score: 1.0   memory length: 29605   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 158   score: 1.0   memory length: 29775   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 159   score: 0.0   memory length: 29899   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 160   score: 2.0   memory length: 30101   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 161   score: 0.0   memory length: 30225   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 162   score: 0.0   memory length: 30349   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 163   score: 3.0   memory length: 30595   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 164   score: 0.0   memory length: 30718   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 165   score: 2.0   memory length: 30919   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 166   score: 2.0   memory length: 31118   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 167   score: 1.0   memory length: 31291   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 168   score: 1.0   memory length: 31442   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 169   score: 4.0   memory length: 31716   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 170   score: 2.0   memory length: 31914   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 171   score: 2.0   memory length: 32098   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 172   score: 1.0   memory length: 32269   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 173   score: 0.0   memory length: 32393   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 174   score: 2.0   memory length: 32591   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 175   score: 4.0   memory length: 32850   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 176   score: 2.0   memory length: 33048   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 177   score: 0.0   memory length: 33172   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 178   score: 1.0   memory length: 33343   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 179   score: 2.0   memory length: 33544   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 180   score: 1.0   memory length: 33696   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 181   score: 2.0   memory length: 33915   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 182   score: 1.0   memory length: 34067   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 183   score: 1.0   memory length: 34238   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 184   score: 0.0   memory length: 34362   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 185   score: 2.0   memory length: 34543   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 186   score: 4.0   memory length: 34813   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 187   score: 2.0   memory length: 35012   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 188   score: 1.0   memory length: 35165   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 189   score: 4.0   memory length: 35420   epsilon: 1.0    steps: 255    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 190   score: 6.0   memory length: 35739   epsilon: 1.0    steps: 319    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 191   score: 3.0   memory length: 35984   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 192   score: 3.0   memory length: 36232   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 193   score: 3.0   memory length: 36459   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 194   score: 1.0   memory length: 36628   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 195   score: 0.0   memory length: 36752   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 196   score: 1.0   memory length: 36906   epsilon: 1.0    steps: 154    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 197   score: 0.0   memory length: 37029   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 198   score: 2.0   memory length: 37247   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 199   score: 1.0   memory length: 37420   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 200   score: 4.0   memory length: 37737   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 201   score: 2.0   memory length: 37955   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 202   score: 1.0   memory length: 38126   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 203   score: 0.0   memory length: 38249   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 204   score: 3.0   memory length: 38495   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 205   score: 4.0   memory length: 38809   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 206   score: 0.0   memory length: 38933   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 207   score: 4.0   memory length: 39230   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 208   score: 3.0   memory length: 39477   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 209   score: 2.0   memory length: 39676   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 210   score: 1.0   memory length: 39827   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 211   score: 4.0   memory length: 40142   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 212   score: 0.0   memory length: 40266   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 213   score: 1.0   memory length: 40435   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 214   score: 5.0   memory length: 40731   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 215   score: 0.0   memory length: 40855   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 216   score: 3.0   memory length: 41122   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 217   score: 2.0   memory length: 41321   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 218   score: 1.0   memory length: 41472   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 219   score: 1.0   memory length: 41644   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 220   score: 1.0   memory length: 41814   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 221   score: 0.0   memory length: 41938   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 222   score: 0.0   memory length: 42062   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 223   score: 2.0   memory length: 42261   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 224   score: 3.0   memory length: 42488   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 225   score: 1.0   memory length: 42658   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 226   score: 2.0   memory length: 42856   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 227   score: 2.0   memory length: 43075   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 228   score: 2.0   memory length: 43274   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 229   score: 4.0   memory length: 43532   epsilon: 1.0    steps: 258    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 230   score: 0.0   memory length: 43656   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 231   score: 1.0   memory length: 43829   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 232   score: 0.0   memory length: 43952   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 233   score: 5.0   memory length: 44276   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 234   score: 1.0   memory length: 44446   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 235   score: 0.0   memory length: 44570   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 236   score: 0.0   memory length: 44694   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 237   score: 0.0   memory length: 44817   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 238   score: 1.0   memory length: 44986   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 239   score: 2.0   memory length: 45184   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 240   score: 4.0   memory length: 45482   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 241   score: 6.0   memory length: 45879   epsilon: 1.0    steps: 397    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 242   score: 2.0   memory length: 46097   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 243   score: 1.0   memory length: 46267   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 244   score: 0.0   memory length: 46390   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 245   score: 3.0   memory length: 46619   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 246   score: 1.0   memory length: 46770   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 247   score: 1.0   memory length: 46940   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 248   score: 2.0   memory length: 47139   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 249   score: 0.0   memory length: 47262   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 250   score: 2.0   memory length: 47464   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 251   score: 0.0   memory length: 47588   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 252   score: 3.0   memory length: 47817   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 253   score: 2.0   memory length: 48034   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 254   score: 3.0   memory length: 48284   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 255   score: 1.0   memory length: 48454   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 256   score: 1.0   memory length: 48624   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 257   score: 0.0   memory length: 48748   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 258   score: 2.0   memory length: 48948   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 259   score: 4.0   memory length: 49242   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 260   score: 3.0   memory length: 49452   epsilon: 1.0    steps: 210    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 261   score: 1.0   memory length: 49623   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 262   score: 1.0   memory length: 49775   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 263   score: 3.0   memory length: 50022   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 264   score: 1.0   memory length: 50174   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 265   score: 2.0   memory length: 50396   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 266   score: 0.0   memory length: 50520   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 267   score: 1.0   memory length: 50689   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 268   score: 2.0   memory length: 50891   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 269   score: 1.0   memory length: 51044   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 270   score: 0.0   memory length: 51168   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 271   score: 2.0   memory length: 51387   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 272   score: 0.0   memory length: 51511   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 273   score: 0.0   memory length: 51635   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 274   score: 3.0   memory length: 51864   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 275   score: 2.0   memory length: 52063   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 276   score: 2.0   memory length: 52265   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 277   score: 1.0   memory length: 52438   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 278   score: 0.0   memory length: 52562   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 279   score: 1.0   memory length: 52732   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 280   score: 1.0   memory length: 52902   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 281   score: 0.0   memory length: 53025   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 282   score: 3.0   memory length: 53288   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 283   score: 2.0   memory length: 53506   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 284   score: 2.0   memory length: 53688   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 285   score: 2.0   memory length: 53907   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 286   score: 0.0   memory length: 54031   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 287   score: 0.0   memory length: 54154   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 288   score: 2.0   memory length: 54334   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 289   score: 1.0   memory length: 54503   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 290   score: 4.0   memory length: 54780   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 291   score: 1.0   memory length: 54932   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 292   score: 0.0   memory length: 55055   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 293   score: 1.0   memory length: 55225   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 294   score: 1.0   memory length: 55394   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 295   score: 1.0   memory length: 55564   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 296   score: 1.0   memory length: 55733   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 297   score: 3.0   memory length: 56004   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 298   score: 1.0   memory length: 56175   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 299   score: 0.0   memory length: 56298   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 300   score: 1.0   memory length: 56467   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 301   score: 0.0   memory length: 56591   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 302   score: 1.0   memory length: 56763   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 303   score: 0.0   memory length: 56887   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 304   score: 3.0   memory length: 57132   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 305   score: 0.0   memory length: 57256   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 306   score: 1.0   memory length: 57407   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 307   score: 2.0   memory length: 57608   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 308   score: 0.0   memory length: 57731   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 309   score: 0.0   memory length: 57855   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 310   score: 2.0   memory length: 58053   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 311   score: 1.0   memory length: 58223   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 312   score: 1.0   memory length: 58394   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 313   score: 1.0   memory length: 58563   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 314   score: 2.0   memory length: 58783   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 315   score: 2.0   memory length: 58982   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 316   score: 1.0   memory length: 59152   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 317   score: 3.0   memory length: 59402   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 318   score: 3.0   memory length: 59666   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 319   score: 0.0   memory length: 59790   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 320   score: 0.0   memory length: 59913   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 321   score: 2.0   memory length: 60112   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 322   score: 1.0   memory length: 60263   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 323   score: 1.0   memory length: 60433   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 324   score: 2.0   memory length: 60631   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 325   score: 2.0   memory length: 60830   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 326   score: 1.0   memory length: 60982   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 327   score: 2.0   memory length: 61181   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 328   score: 0.0   memory length: 61305   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 329   score: 0.0   memory length: 61429   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 330   score: 0.0   memory length: 61553   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 331   score: 1.0   memory length: 61704   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 332   score: 2.0   memory length: 61921   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 333   score: 1.0   memory length: 62073   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 334   score: 0.0   memory length: 62197   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 335   score: 1.0   memory length: 62367   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 336   score: 2.0   memory length: 62566   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 337   score: 1.0   memory length: 62718   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 338   score: 2.0   memory length: 62916   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 339   score: 3.0   memory length: 63185   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 340   score: 0.0   memory length: 63308   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 341   score: 1.0   memory length: 63478   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 342   score: 2.0   memory length: 63682   epsilon: 1.0    steps: 204    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 343   score: 2.0   memory length: 63881   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 344   score: 2.0   memory length: 64079   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 345   score: 3.0   memory length: 64350   epsilon: 1.0    steps: 271    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 346   score: 0.0   memory length: 64473   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 347   score: 0.0   memory length: 64597   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 348   score: 5.0   memory length: 64903   epsilon: 1.0    steps: 306    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 349   score: 3.0   memory length: 65150   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 350   score: 2.0   memory length: 65348   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 351   score: 0.0   memory length: 65472   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 352   score: 2.0   memory length: 65689   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 353   score: 2.0   memory length: 65907   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 354   score: 1.0   memory length: 66059   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 355   score: 1.0   memory length: 66229   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 356   score: 1.0   memory length: 66381   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 357   score: 4.0   memory length: 66696   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 358   score: 0.0   memory length: 66820   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 359   score: 1.0   memory length: 66991   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 360   score: 2.0   memory length: 67190   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 361   score: 2.0   memory length: 67409   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 362   score: 3.0   memory length: 67658   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 363   score: 2.0   memory length: 67876   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 364   score: 2.0   memory length: 68075   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 365   score: 0.0   memory length: 68198   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 366   score: 2.0   memory length: 68416   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 367   score: 3.0   memory length: 68663   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 368   score: 1.0   memory length: 68815   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 369   score: 3.0   memory length: 69062   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 370   score: 2.0   memory length: 69260   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 371   score: 1.0   memory length: 69412   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 372   score: 1.0   memory length: 69582   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 373   score: 3.0   memory length: 69809   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 374   score: 1.0   memory length: 69961   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 375   score: 3.0   memory length: 70208   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 376   score: 2.0   memory length: 70427   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 377   score: 1.0   memory length: 70580   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 378   score: 0.0   memory length: 70704   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 379   score: 3.0   memory length: 70974   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 380   score: 2.0   memory length: 71193   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 381   score: 1.0   memory length: 71363   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 382   score: 3.0   memory length: 71632   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 383   score: 2.0   memory length: 71831   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 384   score: 0.0   memory length: 71954   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 385   score: 1.0   memory length: 72123   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 386   score: 1.0   memory length: 72293   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 387   score: 0.0   memory length: 72416   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 388   score: 3.0   memory length: 72660   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 389   score: 1.0   memory length: 72829   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 390   score: 3.0   memory length: 73079   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 391   score: 2.0   memory length: 73296   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 392   score: 0.0   memory length: 73419   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 393   score: 2.0   memory length: 73637   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 394   score: 3.0   memory length: 73905   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 395   score: 1.0   memory length: 74056   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 396   score: 1.0   memory length: 74229   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 397   score: 1.0   memory length: 74381   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 398   score: 0.0   memory length: 74505   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 399   score: 3.0   memory length: 74753   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 400   score: 6.0   memory length: 75138   epsilon: 1.0    steps: 385    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 401   score: 1.0   memory length: 75290   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 402   score: 0.0   memory length: 75413   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 403   score: 2.0   memory length: 75632   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 404   score: 5.0   memory length: 75979   epsilon: 1.0    steps: 347    lr: 0.0001     evaluation reward: 1.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 405   score: 3.0   memory length: 76206   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 406   score: 0.0   memory length: 76330   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 407   score: 1.0   memory length: 76500   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 408   score: 0.0   memory length: 76624   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 409   score: 1.0   memory length: 76775   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 410   score: 1.0   memory length: 76929   epsilon: 1.0    steps: 154    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 411   score: 1.0   memory length: 77100   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 412   score: 0.0   memory length: 77224   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 413   score: 1.0   memory length: 77375   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 414   score: 3.0   memory length: 77625   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 415   score: 0.0   memory length: 77748   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 416   score: 1.0   memory length: 77919   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 417   score: 4.0   memory length: 78216   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 418   score: 0.0   memory length: 78340   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 419   score: 1.0   memory length: 78509   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 420   score: 1.0   memory length: 78679   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 421   score: 1.0   memory length: 78831   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 422   score: 1.0   memory length: 78983   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 423   score: 1.0   memory length: 79152   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 424   score: 3.0   memory length: 79401   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 425   score: 2.0   memory length: 79600   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 426   score: 2.0   memory length: 79821   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 427   score: 2.0   memory length: 80022   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 428   score: 3.0   memory length: 80251   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 429   score: 1.0   memory length: 80421   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 430   score: 2.0   memory length: 80620   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 431   score: 4.0   memory length: 80937   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 432   score: 3.0   memory length: 81185   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 433   score: 2.0   memory length: 81387   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 434   score: 1.0   memory length: 81558   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 435   score: 1.0   memory length: 81710   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 436   score: 1.0   memory length: 81862   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 437   score: 1.0   memory length: 82034   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 438   score: 3.0   memory length: 82265   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 439   score: 1.0   memory length: 82437   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 440   score: 2.0   memory length: 82656   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 441   score: 1.0   memory length: 82808   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 442   score: 1.0   memory length: 82979   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 443   score: 0.0   memory length: 83102   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 444   score: 3.0   memory length: 83372   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 445   score: 1.0   memory length: 83541   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 446   score: 0.0   memory length: 83665   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 447   score: 0.0   memory length: 83789   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 448   score: 2.0   memory length: 84009   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 449   score: 3.0   memory length: 84275   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 450   score: 3.0   memory length: 84542   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 451   score: 1.0   memory length: 84712   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 452   score: 2.0   memory length: 84912   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 453   score: 1.0   memory length: 85081   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 454   score: 4.0   memory length: 85376   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 455   score: 2.0   memory length: 85574   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 456   score: 0.0   memory length: 85698   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 457   score: 0.0   memory length: 85822   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 458   score: 1.0   memory length: 85991   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 459   score: 3.0   memory length: 86236   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 460   score: 0.0   memory length: 86360   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 461   score: 0.0   memory length: 86484   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 462   score: 2.0   memory length: 86702   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 463   score: 5.0   memory length: 87040   epsilon: 1.0    steps: 338    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 464   score: 1.0   memory length: 87192   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 465   score: 0.0   memory length: 87315   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 466   score: 3.0   memory length: 87527   epsilon: 1.0    steps: 212    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 467   score: 0.0   memory length: 87651   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 468   score: 3.0   memory length: 87915   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 469   score: 2.0   memory length: 88113   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 470   score: 0.0   memory length: 88237   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 471   score: 2.0   memory length: 88435   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 472   score: 1.0   memory length: 88605   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 473   score: 2.0   memory length: 88823   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 474   score: 0.0   memory length: 88947   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 475   score: 4.0   memory length: 89226   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 476   score: 0.0   memory length: 89349   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 477   score: 2.0   memory length: 89566   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 478   score: 1.0   memory length: 89717   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 479   score: 3.0   memory length: 89981   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 480   score: 0.0   memory length: 90105   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 481   score: 4.0   memory length: 90403   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 482   score: 2.0   memory length: 90602   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 483   score: 2.0   memory length: 90801   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 484   score: 3.0   memory length: 91055   epsilon: 1.0    steps: 254    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 485   score: 2.0   memory length: 91273   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 486   score: 0.0   memory length: 91397   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 487   score: 1.0   memory length: 91549   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 488   score: 0.0   memory length: 91673   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 489   score: 0.0   memory length: 91797   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 490   score: 1.0   memory length: 91967   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 491   score: 0.0   memory length: 92091   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 492   score: 0.0   memory length: 92215   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 493   score: 3.0   memory length: 92464   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 494   score: 0.0   memory length: 92587   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 495   score: 4.0   memory length: 92905   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 496   score: 2.0   memory length: 93104   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 497   score: 0.0   memory length: 93228   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 498   score: 2.0   memory length: 93448   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 499   score: 1.0   memory length: 93621   epsilon: 1.0    steps: 173    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 500   score: 1.0   memory length: 93773   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 501   score: 1.0   memory length: 93924   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 502   score: 2.0   memory length: 94143   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 503   score: 2.0   memory length: 94362   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 504   score: 1.0   memory length: 94531   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 505   score: 1.0   memory length: 94703   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 506   score: 4.0   memory length: 94958   epsilon: 1.0    steps: 255    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 507   score: 0.0   memory length: 95082   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 508   score: 2.0   memory length: 95302   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 509   score: 0.0   memory length: 95426   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 510   score: 0.0   memory length: 95550   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 511   score: 0.0   memory length: 95673   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 512   score: 1.0   memory length: 95825   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 513   score: 2.0   memory length: 96024   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 514   score: 1.0   memory length: 96194   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 515   score: 2.0   memory length: 96395   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 516   score: 2.0   memory length: 96619   epsilon: 1.0    steps: 224    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 517   score: 1.0   memory length: 96788   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 518   score: 2.0   memory length: 97007   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 519   score: 5.0   memory length: 97331   epsilon: 1.0    steps: 324    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 520   score: 2.0   memory length: 97551   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 521   score: 2.0   memory length: 97752   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 522   score: 1.0   memory length: 97923   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 523   score: 2.0   memory length: 98125   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 524   score: 0.0   memory length: 98249   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 525   score: 1.0   memory length: 98421   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 526   score: 1.0   memory length: 98593   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 527   score: 0.0   memory length: 98717   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 528   score: 1.0   memory length: 98869   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 529   score: 1.0   memory length: 99021   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 530   score: 1.0   memory length: 99172   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 531   score: 0.0   memory length: 99296   epsilon: 1.0    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 532   score: 2.0   memory length: 99477   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 533   score: 2.0   memory length: 99695   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 534   score: 3.0   memory length: 99943   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 535   score: 1.0   memory length: 100000   epsilon: 0.999770320000005    steps: 172    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 536   score: 0.0   memory length: 100000   epsilon: 0.9995248000000103    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 537   score: 1.0   memory length: 100000   epsilon: 0.9991842400000177    steps: 172    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 538   score: 3.0   memory length: 100000   epsilon: 0.9986912200000284    steps: 249    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 539   score: 0.0   memory length: 100000   epsilon: 0.9984457000000337    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 540   score: 4.0   memory length: 100000   epsilon: 0.9979130800000453    steps: 269    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 541   score: 0.0   memory length: 100000   epsilon: 0.9976695400000506    steps: 123    lr: 0.0001     evaluation reward: 1.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 542   score: 2.0   memory length: 100000   epsilon: 0.99723394000006    steps: 220    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 543   score: 0.0   memory length: 100000   epsilon: 0.9969884200000654    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 544   score: 3.0   memory length: 100000   epsilon: 0.9964597600000769    steps: 267    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 545   score: 0.0   memory length: 100000   epsilon: 0.9962142400000822    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 546   score: 0.0   memory length: 100000   epsilon: 0.9959707000000875    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 547   score: 2.0   memory length: 100000   epsilon: 0.995576680000096    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 548   score: 2.0   memory length: 100000   epsilon: 0.9951826600001046    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 549   score: 3.0   memory length: 100000   epsilon: 0.9947351800001143    steps: 226    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 550   score: 1.0   memory length: 100000   epsilon: 0.9944342200001208    steps: 152    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 551   score: 3.0   memory length: 100000   epsilon: 0.9939035800001323    steps: 268    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 552   score: 0.0   memory length: 100000   epsilon: 0.9936580600001377    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 553   score: 2.0   memory length: 100000   epsilon: 0.9932640400001462    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 554   score: 2.0   memory length: 100000   epsilon: 0.9928700200001548    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 555   score: 0.0   memory length: 100000   epsilon: 0.9926264800001601    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 556   score: 1.0   memory length: 100000   epsilon: 0.9923255200001666    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 557   score: 1.0   memory length: 100000   epsilon: 0.9920245600001731    steps: 152    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 558   score: 1.0   memory length: 100000   epsilon: 0.9916879600001804    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 559   score: 0.0   memory length: 100000   epsilon: 0.9914424400001858    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 560   score: 0.0   memory length: 100000   epsilon: 0.9911969200001911    steps: 124    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 561   score: 1.0   memory length: 100000   epsilon: 0.9908543800001985    steps: 173    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 562   score: 3.0   memory length: 100000   epsilon: 0.9904029400002083    steps: 228    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 563   score: 5.0   memory length: 100000   epsilon: 0.9897317200002229    steps: 339    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 564   score: 3.0   memory length: 100000   epsilon: 0.9892406800002336    steps: 248    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 565   score: 3.0   memory length: 100000   epsilon: 0.988712020000245    steps: 267    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 566   score: 1.0   memory length: 100000   epsilon: 0.9883714600002524    steps: 172    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 567   score: 0.0   memory length: 100000   epsilon: 0.9881259400002578    steps: 124    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 568   score: 2.0   memory length: 100000   epsilon: 0.9876982600002671    steps: 216    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 569   score: 0.0   memory length: 100000   epsilon: 0.9874527400002724    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 570   score: 2.0   memory length: 100000   epsilon: 0.9870527800002811    steps: 202    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 571   score: 1.0   memory length: 100000   epsilon: 0.9867181600002883    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 572   score: 3.0   memory length: 100000   epsilon: 0.986229100000299    steps: 247    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 573   score: 4.0   memory length: 100000   epsilon: 0.9855895600003128    steps: 323    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 574   score: 1.0   memory length: 100000   epsilon: 0.9852886000003194    steps: 152    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 575   score: 3.0   memory length: 100000   epsilon: 0.9847936000003301    steps: 250    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 576   score: 2.0   memory length: 100000   epsilon: 0.9843520600003397    steps: 223    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 577   score: 1.0   memory length: 100000   epsilon: 0.9840115000003471    steps: 172    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 578   score: 3.0   memory length: 100000   epsilon: 0.9835204600003578    steps: 248    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 579   score: 1.0   memory length: 100000   epsilon: 0.983185840000365    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 580   score: 1.0   memory length: 100000   epsilon: 0.9828492400003723    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 581   score: 1.0   memory length: 100000   epsilon: 0.9825067000003798    steps: 173    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 582   score: 1.0   memory length: 100000   epsilon: 0.9821661400003872    steps: 172    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 583   score: 2.0   memory length: 100000   epsilon: 0.9817345000003965    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 584   score: 0.0   memory length: 100000   epsilon: 0.9814889800004019    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 585   score: 8.0   memory length: 100000   epsilon: 0.9805207600004229    steps: 489    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 586   score: 2.0   memory length: 100000   epsilon: 0.9800871400004323    steps: 219    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 587   score: 0.0   memory length: 100000   epsilon: 0.9798436000004376    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 588   score: 0.0   memory length: 100000   epsilon: 0.9795980800004429    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 589   score: 2.0   memory length: 100000   epsilon: 0.9791704000004522    steps: 216    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 590   score: 0.0   memory length: 100000   epsilon: 0.9789248800004575    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 591   score: 1.0   memory length: 100000   epsilon: 0.9785902600004648    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 592   score: 0.0   memory length: 100000   epsilon: 0.9783467200004701    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 593   score: 2.0   memory length: 100000   epsilon: 0.9779190400004794    steps: 216    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 594   score: 3.0   memory length: 100000   epsilon: 0.9773884000004909    steps: 268    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 595   score: 1.0   memory length: 100000   epsilon: 0.9770874400004974    steps: 152    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 596   score: 1.0   memory length: 100000   epsilon: 0.9767508400005047    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 597   score: 0.0   memory length: 100000   epsilon: 0.97650532000051    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 598   score: 1.0   memory length: 100000   epsilon: 0.9761647600005174    steps: 172    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 599   score: 2.0   memory length: 100000   epsilon: 0.975770740000526    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 600   score: 3.0   memory length: 100000   epsilon: 0.9753193000005358    steps: 228    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 601   score: 0.0   memory length: 100000   epsilon: 0.9750757600005411    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 602   score: 2.0   memory length: 100000   epsilon: 0.9746797600005497    steps: 200    lr: 0.0001     evaluation reward: 1.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 603   score: 1.0   memory length: 100000   epsilon: 0.9743392000005571    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 604   score: 0.0   memory length: 100000   epsilon: 0.9740936800005624    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 605   score: 4.0   memory length: 100000   epsilon: 0.973511560000575    steps: 294    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 606   score: 1.0   memory length: 100000   epsilon: 0.9731749600005823    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 607   score: 4.0   memory length: 100000   epsilon: 0.9726304600005942    steps: 275    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 608   score: 1.0   memory length: 100000   epsilon: 0.9722918800006015    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 609   score: 0.0   memory length: 100000   epsilon: 0.9720463600006068    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 610   score: 1.0   memory length: 100000   epsilon: 0.9717058000006142    steps: 172    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 611   score: 1.0   memory length: 100000   epsilon: 0.9714048400006208    steps: 152    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 612   score: 0.0   memory length: 100000   epsilon: 0.9711593200006261    steps: 124    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 613   score: 1.0   memory length: 100000   epsilon: 0.9708227200006334    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 614   score: 3.0   memory length: 100000   epsilon: 0.9702920800006449    steps: 268    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 615   score: 2.0   memory length: 100000   epsilon: 0.9698980600006535    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 616   score: 0.0   memory length: 100000   epsilon: 0.9696525400006588    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 617   score: 3.0   memory length: 100000   epsilon: 0.9691654600006694    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 618   score: 4.0   memory length: 100000   epsilon: 0.9686189800006813    steps: 276    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 619   score: 1.0   memory length: 100000   epsilon: 0.9682823800006886    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 620   score: 1.0   memory length: 100000   epsilon: 0.9679457800006959    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 621   score: 4.0   memory length: 100000   epsilon: 0.9673161400007095    steps: 318    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 622   score: 3.0   memory length: 100000   epsilon: 0.9668310400007201    steps: 245    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 623   score: 3.0   memory length: 100000   epsilon: 0.9663380200007308    steps: 249    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 624   score: 0.0   memory length: 100000   epsilon: 0.9660925000007361    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 625   score: 0.0   memory length: 100000   epsilon: 0.9658469800007414    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 626   score: 1.0   memory length: 100000   epsilon: 0.9655103800007487    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 627   score: 1.0   memory length: 100000   epsilon: 0.965173780000756    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 628   score: 1.0   memory length: 100000   epsilon: 0.9648728200007626    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 629   score: 0.0   memory length: 100000   epsilon: 0.9646273000007679    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 630   score: 0.0   memory length: 100000   epsilon: 0.9643817800007732    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 631   score: 1.0   memory length: 100000   epsilon: 0.9640432000007806    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 632   score: 0.0   memory length: 100000   epsilon: 0.9637996600007859    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 633   score: 1.0   memory length: 100000   epsilon: 0.9634987000007924    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 634   score: 0.0   memory length: 100000   epsilon: 0.9632531800007977    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 635   score: 2.0   memory length: 100000   epsilon: 0.962825500000807    steps: 216    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 636   score: 1.0   memory length: 100000   epsilon: 0.9624869200008144    steps: 171    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 637   score: 0.0   memory length: 100000   epsilon: 0.9622433800008197    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 638   score: 1.0   memory length: 100000   epsilon: 0.9619424200008262    steps: 152    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 639   score: 2.0   memory length: 100000   epsilon: 0.9615424600008349    steps: 202    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 640   score: 3.0   memory length: 100000   epsilon: 0.9610514200008455    steps: 248    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 641   score: 0.0   memory length: 100000   epsilon: 0.9608078800008508    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 642   score: 3.0   memory length: 100000   epsilon: 0.9603208000008614    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 643   score: 4.0   memory length: 100000   epsilon: 0.9597723400008733    steps: 277    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 644   score: 1.0   memory length: 100000   epsilon: 0.9594733600008798    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 645   score: 2.0   memory length: 100000   epsilon: 0.9590793400008883    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 646   score: 0.0   memory length: 100000   epsilon: 0.9588338200008937    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 647   score: 0.0   memory length: 100000   epsilon: 0.958590280000899    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 648   score: 2.0   memory length: 100000   epsilon: 0.9581962600009075    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 649   score: 2.0   memory length: 100000   epsilon: 0.957804220000916    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 650   score: 0.0   memory length: 100000   epsilon: 0.9575587000009214    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 651   score: 0.0   memory length: 100000   epsilon: 0.9573131800009267    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 652   score: 2.0   memory length: 100000   epsilon: 0.9569132200009354    steps: 202    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 653   score: 1.0   memory length: 100000   epsilon: 0.9566122600009419    steps: 152    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 654   score: 1.0   memory length: 100000   epsilon: 0.9563132800009484    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 655   score: 1.0   memory length: 100000   epsilon: 0.9560123200009549    steps: 152    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 656   score: 2.0   memory length: 100000   epsilon: 0.9555787000009643    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 657   score: 2.0   memory length: 100000   epsilon: 0.9551846800009729    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 658   score: 1.0   memory length: 100000   epsilon: 0.9548441200009803    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 659   score: 0.0   memory length: 100000   epsilon: 0.9545986000009856    steps: 124    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 660   score: 3.0   memory length: 100000   epsilon: 0.9541471600009954    steps: 228    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 661   score: 2.0   memory length: 100000   epsilon: 0.953753140001004    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 662   score: 0.0   memory length: 100000   epsilon: 0.9535076200010093    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 663   score: 0.0   memory length: 100000   epsilon: 0.9532640800010146    steps: 123    lr: 0.0001     evaluation reward: 1.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 664   score: 1.0   memory length: 100000   epsilon: 0.952923520001022    steps: 172    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 665   score: 2.0   memory length: 100000   epsilon: 0.9525255400010306    steps: 201    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 666   score: 1.0   memory length: 100000   epsilon: 0.9522245800010372    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 667   score: 0.0   memory length: 100000   epsilon: 0.9519790600010425    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 668   score: 3.0   memory length: 100000   epsilon: 0.9514900000010531    steps: 247    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 669   score: 1.0   memory length: 100000   epsilon: 0.9511910200010596    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 670   score: 0.0   memory length: 100000   epsilon: 0.9509455000010649    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 671   score: 1.0   memory length: 100000   epsilon: 0.9506465200010714    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 672   score: 3.0   memory length: 100000   epsilon: 0.9501515200010822    steps: 250    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 673   score: 0.0   memory length: 100000   epsilon: 0.9499079800010874    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 674   score: 0.0   memory length: 100000   epsilon: 0.9496624600010928    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 675   score: 3.0   memory length: 100000   epsilon: 0.9491714200011034    steps: 248    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 676   score: 1.0   memory length: 100000   epsilon: 0.9488348200011107    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 677   score: 2.0   memory length: 100000   epsilon: 0.9484427800011193    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 678   score: 1.0   memory length: 100000   epsilon: 0.9481042000011266    steps: 171    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 679   score: 1.0   memory length: 100000   epsilon: 0.947761660001134    steps: 173    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 680   score: 0.0   memory length: 100000   epsilon: 0.9475161400011394    steps: 124    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 681   score: 1.0   memory length: 100000   epsilon: 0.9472151800011459    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 682   score: 0.0   memory length: 100000   epsilon: 0.9469696600011512    steps: 124    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 683   score: 2.0   memory length: 100000   epsilon: 0.9465756400011598    steps: 199    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 684   score: 0.0   memory length: 100000   epsilon: 0.9463301200011651    steps: 124    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 685   score: 2.0   memory length: 100000   epsilon: 0.9459361000011737    steps: 199    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 686   score: 1.0   memory length: 100000   epsilon: 0.9456351400011802    steps: 152    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 687   score: 0.0   memory length: 100000   epsilon: 0.9453916000011855    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 688   score: 0.0   memory length: 100000   epsilon: 0.9451460800011908    steps: 124    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 689   score: 0.0   memory length: 100000   epsilon: 0.9449005600011962    steps: 124    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 690   score: 0.0   memory length: 100000   epsilon: 0.9446550400012015    steps: 124    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 691   score: 2.0   memory length: 100000   epsilon: 0.94426300000121    steps: 198    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 692   score: 1.0   memory length: 100000   epsilon: 0.9439264000012173    steps: 170    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 693   score: 0.0   memory length: 100000   epsilon: 0.9436828600012226    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 694   score: 5.0   memory length: 100000   epsilon: 0.9430136200012371    steps: 338    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 695   score: 3.0   memory length: 100000   epsilon: 0.9425206000012478    steps: 249    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 696   score: 4.0   memory length: 100000   epsilon: 0.9419345200012605    steps: 296    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 697   score: 3.0   memory length: 100000   epsilon: 0.9414454600012712    steps: 247    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 698   score: 0.0   memory length: 100000   epsilon: 0.9411999400012765    steps: 124    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 699   score: 2.0   memory length: 100000   epsilon: 0.940805920001285    steps: 199    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 700   score: 1.0   memory length: 100000   epsilon: 0.9404693200012924    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 701   score: 4.0   memory length: 100000   epsilon: 0.9399248200013042    steps: 275    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 702   score: 2.0   memory length: 100000   epsilon: 0.9395308000013127    steps: 199    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 703   score: 2.0   memory length: 100000   epsilon: 0.9391367800013213    steps: 199    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 704   score: 1.0   memory length: 100000   epsilon: 0.9388378000013278    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 705   score: 1.0   memory length: 100000   epsilon: 0.9385012000013351    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 706   score: 0.0   memory length: 100000   epsilon: 0.9382576600013404    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 707   score: 2.0   memory length: 100000   epsilon: 0.9378953200013482    steps: 183    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 708   score: 1.0   memory length: 100000   epsilon: 0.9375587200013555    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 709   score: 0.0   memory length: 100000   epsilon: 0.9373132000013609    steps: 124    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 710   score: 4.0   memory length: 100000   epsilon: 0.9367588000013729    steps: 280    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 711   score: 1.0   memory length: 100000   epsilon: 0.9364222000013802    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 712   score: 2.0   memory length: 100000   epsilon: 0.9359885800013896    steps: 219    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 713   score: 1.0   memory length: 100000   epsilon: 0.9356519800013969    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 714   score: 3.0   memory length: 100000   epsilon: 0.9351272800014083    steps: 265    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 715   score: 0.0   memory length: 100000   epsilon: 0.9348837400014136    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 716   score: 3.0   memory length: 100000   epsilon: 0.9344362600014233    steps: 226    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 717   score: 5.0   memory length: 100000   epsilon: 0.933808600001437    steps: 317    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 718   score: 2.0   memory length: 100000   epsilon: 0.9334145800014455    steps: 199    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 719   score: 2.0   memory length: 100000   epsilon: 0.9330205600014541    steps: 199    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 720   score: 2.0   memory length: 100000   epsilon: 0.9325849600014635    steps: 220    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 721   score: 1.0   memory length: 100000   epsilon: 0.93228400000147    steps: 152    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 722   score: 3.0   memory length: 100000   epsilon: 0.9317929600014807    steps: 248    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 723   score: 3.0   memory length: 100000   epsilon: 0.9313039000014913    steps: 247    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 724   score: 2.0   memory length: 100000   epsilon: 0.9309098800014999    steps: 199    lr: 0.0001     evaluation reward: 1.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 725   score: 1.0   memory length: 100000   epsilon: 0.9305752600015071    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 726   score: 2.0   memory length: 100000   epsilon: 0.9301812400015157    steps: 199    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 727   score: 1.0   memory length: 100000   epsilon: 0.929844640001523    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 728   score: 2.0   memory length: 100000   epsilon: 0.9294862600015308    steps: 181    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 729   score: 2.0   memory length: 100000   epsilon: 0.9290863000015395    steps: 202    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 730   score: 4.0   memory length: 100000   epsilon: 0.9285022000015521    steps: 295    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 731   score: 3.0   memory length: 100000   epsilon: 0.9280547200015619    steps: 226    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 732   score: 3.0   memory length: 100000   epsilon: 0.9275617000015726    steps: 249    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 733   score: 0.0   memory length: 100000   epsilon: 0.9273161800015779    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 734   score: 0.0   memory length: 100000   epsilon: 0.9270706600015832    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 735   score: 2.0   memory length: 100000   epsilon: 0.9266370400015926    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 736   score: 0.0   memory length: 100000   epsilon: 0.926391520001598    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 737   score: 0.0   memory length: 100000   epsilon: 0.9261479800016033    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 738   score: 4.0   memory length: 100000   epsilon: 0.9255282400016167    steps: 313    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 739   score: 1.0   memory length: 100000   epsilon: 0.925191640001624    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 740   score: 2.0   memory length: 100000   epsilon: 0.9247580200016334    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 741   score: 1.0   memory length: 100000   epsilon: 0.9244214200016407    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 742   score: 1.0   memory length: 100000   epsilon: 0.924084820001648    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 743   score: 1.0   memory length: 100000   epsilon: 0.9237482200016554    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 744   score: 0.0   memory length: 100000   epsilon: 0.9235027000016607    steps: 124    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 745   score: 3.0   memory length: 100000   epsilon: 0.9230176000016712    steps: 245    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 746   score: 3.0   memory length: 100000   epsilon: 0.9225285400016818    steps: 247    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 747   score: 1.0   memory length: 100000   epsilon: 0.9221939200016891    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 748   score: 4.0   memory length: 100000   epsilon: 0.921601900001702    steps: 299    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 749   score: 0.0   memory length: 100000   epsilon: 0.9213563800017073    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 750   score: 0.0   memory length: 100000   epsilon: 0.9211128400017126    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 751   score: 1.0   memory length: 100000   epsilon: 0.9207742600017199    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 752   score: 2.0   memory length: 100000   epsilon: 0.9203822200017284    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 753   score: 1.0   memory length: 100000   epsilon: 0.9200456200017357    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 754   score: 1.0   memory length: 100000   epsilon: 0.9197030800017432    steps: 173    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 755   score: 1.0   memory length: 100000   epsilon: 0.9194041000017497    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 756   score: 1.0   memory length: 100000   epsilon: 0.919067500001757    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 757   score: 1.0   memory length: 100000   epsilon: 0.9187309000017643    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 758   score: 3.0   memory length: 100000   epsilon: 0.9182755000017742    steps: 230    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 759   score: 2.0   memory length: 100000   epsilon: 0.9178478200017834    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 760   score: 2.0   memory length: 100000   epsilon: 0.917455780001792    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 761   score: 2.0   memory length: 100000   epsilon: 0.9170974000017997    steps: 181    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 762   score: 3.0   memory length: 100000   epsilon: 0.9165727000018111    steps: 265    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 763   score: 1.0   memory length: 100000   epsilon: 0.9162380800018184    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 764   score: 0.0   memory length: 100000   epsilon: 0.9159945400018237    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 765   score: 2.0   memory length: 100000   epsilon: 0.9155945800018324    steps: 202    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 766   score: 0.0   memory length: 100000   epsilon: 0.9153490600018377    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 767   score: 0.0   memory length: 100000   epsilon: 0.915103540001843    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 768   score: 0.0   memory length: 100000   epsilon: 0.9148600000018483    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 769   score: 1.0   memory length: 100000   epsilon: 0.9145610200018548    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 770   score: 1.0   memory length: 100000   epsilon: 0.9142224400018621    steps: 171    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 771   score: 3.0   memory length: 100000   epsilon: 0.9137314000018728    steps: 248    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 772   score: 0.0   memory length: 100000   epsilon: 0.9134878600018781    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 773   score: 2.0   memory length: 100000   epsilon: 0.9130938400018866    steps: 199    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 774   score: 3.0   memory length: 100000   epsilon: 0.9126067600018972    steps: 246    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 775   score: 0.0   memory length: 100000   epsilon: 0.9123632200019025    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 776   score: 2.0   memory length: 100000   epsilon: 0.9119296000019119    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 777   score: 2.0   memory length: 100000   epsilon: 0.9115355800019205    steps: 199    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 778   score: 3.0   memory length: 100000   epsilon: 0.9110445400019311    steps: 248    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 779   score: 3.0   memory length: 100000   epsilon: 0.9105970600019409    steps: 226    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 780   score: 2.0   memory length: 100000   epsilon: 0.9102010600019494    steps: 200    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 781   score: 2.0   memory length: 100000   epsilon: 0.909809020001958    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 782   score: 0.0   memory length: 100000   epsilon: 0.9095635000019633    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 783   score: 0.0   memory length: 100000   epsilon: 0.9093199600019686    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 784   score: 2.0   memory length: 100000   epsilon: 0.9089259400019771    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 785   score: 1.0   memory length: 100000   epsilon: 0.9086249800019837    steps: 152    lr: 0.0001     evaluation reward: 1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 786   score: 1.0   memory length: 100000   epsilon: 0.9082824400019911    steps: 173    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 787   score: 2.0   memory length: 100000   epsilon: 0.9078904000019996    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 788   score: 1.0   memory length: 100000   epsilon: 0.907549840002007    steps: 172    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 789   score: 3.0   memory length: 100000   epsilon: 0.9071003800020168    steps: 227    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 790   score: 0.0   memory length: 100000   epsilon: 0.906856840002022    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 791   score: 1.0   memory length: 100000   epsilon: 0.9065558800020286    steps: 152    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 792   score: 0.0   memory length: 100000   epsilon: 0.9063123400020339    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 793   score: 2.0   memory length: 100000   epsilon: 0.9059183200020424    steps: 199    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 794   score: 1.0   memory length: 100000   epsilon: 0.9055817200020497    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 795   score: 0.0   memory length: 100000   epsilon: 0.905338180002055    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 796   score: 1.0   memory length: 100000   epsilon: 0.9050015800020623    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 797   score: 3.0   memory length: 100000   epsilon: 0.9045164800020729    steps: 245    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 798   score: 0.0   memory length: 100000   epsilon: 0.9042709600020782    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 799   score: 1.0   memory length: 100000   epsilon: 0.9039323800020855    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 800   score: 4.0   memory length: 100000   epsilon: 0.9034235200020966    steps: 257    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 801   score: 8.0   memory length: 100000   epsilon: 0.9027919000021103    steps: 319    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 802   score: 0.0   memory length: 100000   epsilon: 0.9025463800021156    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 803   score: 0.0   memory length: 100000   epsilon: 0.902300860002121    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 804   score: 0.0   memory length: 100000   epsilon: 0.9020553400021263    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 805   score: 0.0   memory length: 100000   epsilon: 0.9018098200021316    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 806   score: 1.0   memory length: 100000   epsilon: 0.9014732200021389    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 807   score: 0.0   memory length: 100000   epsilon: 0.9012277000021442    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 808   score: 2.0   memory length: 100000   epsilon: 0.9007940800021537    steps: 219    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 809   score: 2.0   memory length: 100000   epsilon: 0.900362440002163    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 810   score: 6.0   memory length: 100000   epsilon: 0.8997268600021768    steps: 321    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 811   score: 1.0   memory length: 100000   epsilon: 0.8993902600021841    steps: 170    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 812   score: 0.0   memory length: 100000   epsilon: 0.8991467200021894    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 813   score: 1.0   memory length: 100000   epsilon: 0.8988477400021959    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 814   score: 2.0   memory length: 100000   epsilon: 0.8984557000022044    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 815   score: 2.0   memory length: 100000   epsilon: 0.8980636600022129    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 816   score: 2.0   memory length: 100000   epsilon: 0.8976716200022214    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 817   score: 1.0   memory length: 100000   epsilon: 0.897370660002228    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 818   score: 0.0   memory length: 100000   epsilon: 0.8971251400022333    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 819   score: 0.0   memory length: 100000   epsilon: 0.8968816000022386    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 820   score: 3.0   memory length: 100000   epsilon: 0.8963965000022491    steps: 245    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 821   score: 0.0   memory length: 100000   epsilon: 0.8961509800022545    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 822   score: 2.0   memory length: 100000   epsilon: 0.895758940002263    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 823   score: 2.0   memory length: 100000   epsilon: 0.8953649200022715    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 824   score: 4.0   memory length: 100000   epsilon: 0.8947768600022843    steps: 297    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 825   score: 4.0   memory length: 100000   epsilon: 0.8942264200022962    steps: 278    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 826   score: 3.0   memory length: 100000   epsilon: 0.8937353800023069    steps: 248    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 827   score: 1.0   memory length: 100000   epsilon: 0.8934007600023142    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 828   score: 0.0   memory length: 100000   epsilon: 0.8931572200023195    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 829   score: 5.0   memory length: 100000   epsilon: 0.8924919400023339    steps: 336    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 830   score: 2.0   memory length: 100000   epsilon: 0.8920504000023435    steps: 223    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 831   score: 3.0   memory length: 100000   epsilon: 0.8915237200023549    steps: 266    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 832   score: 0.0   memory length: 100000   epsilon: 0.8912782000023602    steps: 124    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 833   score: 3.0   memory length: 100000   epsilon: 0.8907515200023717    steps: 266    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 834   score: 0.0   memory length: 100000   epsilon: 0.890507980002377    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 835   score: 2.0   memory length: 100000   epsilon: 0.8901159400023855    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 836   score: 0.0   memory length: 100000   epsilon: 0.8898704200023908    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 837   score: 4.0   memory length: 100000   epsilon: 0.8892467200024043    steps: 315    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 838   score: 2.0   memory length: 100000   epsilon: 0.8888883400024121    steps: 181    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 839   score: 0.0   memory length: 100000   epsilon: 0.8886428200024175    steps: 124    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 840   score: 0.0   memory length: 100000   epsilon: 0.8883973000024228    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 841   score: 0.0   memory length: 100000   epsilon: 0.8881537600024281    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 842   score: 1.0   memory length: 100000   epsilon: 0.8878528000024346    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 843   score: 2.0   memory length: 100000   epsilon: 0.8874587800024432    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 844   score: 4.0   memory length: 100000   epsilon: 0.8868727000024559    steps: 296    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 845   score: 1.0   memory length: 100000   epsilon: 0.8865717400024624    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 846   score: 4.0   memory length: 100000   epsilon: 0.8860529800024737    steps: 262    lr: 0.0001     evaluation reward: 1.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 847   score: 3.0   memory length: 100000   epsilon: 0.8855599600024844    steps: 249    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 848   score: 0.0   memory length: 100000   epsilon: 0.8853144400024897    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 849   score: 1.0   memory length: 100000   epsilon: 0.8850134800024962    steps: 152    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 850   score: 2.0   memory length: 100000   epsilon: 0.8846214400025048    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 851   score: 2.0   memory length: 100000   epsilon: 0.8842571200025127    steps: 184    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 852   score: 1.0   memory length: 100000   epsilon: 0.88392052000252    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 853   score: 1.0   memory length: 100000   epsilon: 0.8835819400025273    steps: 171    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 854   score: 3.0   memory length: 100000   epsilon: 0.883090900002538    steps: 248    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 855   score: 2.0   memory length: 100000   epsilon: 0.8826988600025465    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 856   score: 3.0   memory length: 100000   epsilon: 0.8822513800025562    steps: 226    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 857   score: 4.0   memory length: 100000   epsilon: 0.8816969800025682    steps: 280    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 858   score: 2.0   memory length: 100000   epsilon: 0.8813029600025768    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 859   score: 2.0   memory length: 100000   epsilon: 0.8809089400025854    steps: 199    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 860   score: 1.0   memory length: 100000   epsilon: 0.8805723400025927    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 861   score: 0.0   memory length: 100000   epsilon: 0.880326820002598    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 862   score: 0.0   memory length: 100000   epsilon: 0.8800813000026033    steps: 124    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 863   score: 1.0   memory length: 100000   epsilon: 0.8797803400026099    steps: 152    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 864   score: 3.0   memory length: 100000   epsilon: 0.8793308800026196    steps: 227    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 865   score: 1.0   memory length: 100000   epsilon: 0.878992300002627    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 866   score: 1.0   memory length: 100000   epsilon: 0.8786913400026335    steps: 152    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 867   score: 2.0   memory length: 100000   epsilon: 0.8782577200026429    steps: 219    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 868   score: 1.0   memory length: 100000   epsilon: 0.8779211200026502    steps: 170    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 869   score: 1.0   memory length: 100000   epsilon: 0.8775865000026575    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 870   score: 0.0   memory length: 100000   epsilon: 0.8773409800026628    steps: 124    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 871   score: 2.0   memory length: 100000   epsilon: 0.8769746800026708    steps: 185    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 872   score: 1.0   memory length: 100000   epsilon: 0.876640060002678    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 873   score: 2.0   memory length: 100000   epsilon: 0.8762420800026867    steps: 201    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 874   score: 2.0   memory length: 100000   epsilon: 0.875810440002696    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 875   score: 2.0   memory length: 100000   epsilon: 0.8754520600027038    steps: 181    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 876   score: 2.0   memory length: 100000   epsilon: 0.8750540800027125    steps: 201    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 877   score: 1.0   memory length: 100000   epsilon: 0.8747135200027198    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 878   score: 1.0   memory length: 100000   epsilon: 0.8743789000027271    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 879   score: 3.0   memory length: 100000   epsilon: 0.8739314200027368    steps: 226    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 880   score: 1.0   memory length: 100000   epsilon: 0.8735948200027441    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 881   score: 0.0   memory length: 100000   epsilon: 0.8733512800027494    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 882   score: 4.0   memory length: 100000   epsilon: 0.8728305400027607    steps: 263    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 883   score: 1.0   memory length: 100000   epsilon: 0.8725295800027673    steps: 152    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 884   score: 0.0   memory length: 100000   epsilon: 0.8722860400027725    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 885   score: 2.0   memory length: 100000   epsilon: 0.8718880600027812    steps: 201    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 886   score: 2.0   memory length: 100000   epsilon: 0.8714544400027906    steps: 219    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 887   score: 2.0   memory length: 100000   epsilon: 0.8710940800027984    steps: 182    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 888   score: 3.0   memory length: 100000   epsilon: 0.8706406600028083    steps: 229    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 889   score: 0.0   memory length: 100000   epsilon: 0.8703951400028136    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 890   score: 3.0   memory length: 100000   epsilon: 0.8699080600028242    steps: 246    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 891   score: 1.0   memory length: 100000   epsilon: 0.8695694800028315    steps: 171    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 892   score: 0.0   memory length: 100000   epsilon: 0.8693239600028368    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 893   score: 1.0   memory length: 100000   epsilon: 0.8689853800028442    steps: 171    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 894   score: 3.0   memory length: 100000   epsilon: 0.8684943400028549    steps: 248    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 895   score: 4.0   memory length: 100000   epsilon: 0.867935980002867    steps: 282    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 896   score: 3.0   memory length: 100000   epsilon: 0.8675063200028763    steps: 217    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 897   score: 2.0   memory length: 100000   epsilon: 0.8671123000028849    steps: 199    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 898   score: 2.0   memory length: 100000   epsilon: 0.8666786800028943    steps: 219    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 899   score: 2.0   memory length: 100000   epsilon: 0.8662846600029028    steps: 199    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 900   score: 2.0   memory length: 100000   epsilon: 0.8658886600029114    steps: 200    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 901   score: 1.0   memory length: 100000   epsilon: 0.8655500800029188    steps: 171    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 902   score: 0.0   memory length: 100000   epsilon: 0.8653045600029241    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 903   score: 1.0   memory length: 100000   epsilon: 0.8649679600029314    steps: 170    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 904   score: 0.0   memory length: 100000   epsilon: 0.8647224400029367    steps: 124    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 905   score: 3.0   memory length: 100000   epsilon: 0.8642274400029475    steps: 250    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 906   score: 2.0   memory length: 100000   epsilon: 0.863833420002956    steps: 199    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 907   score: 2.0   memory length: 100000   epsilon: 0.8634057400029653    steps: 216    lr: 0.0001     evaluation reward: 1.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 908   score: 0.0   memory length: 100000   epsilon: 0.8631602200029707    steps: 124    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 909   score: 2.0   memory length: 100000   epsilon: 0.8627681800029792    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 910   score: 2.0   memory length: 100000   epsilon: 0.8623325800029886    steps: 220    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 911   score: 3.0   memory length: 100000   epsilon: 0.8618712400029986    steps: 233    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 912   score: 0.0   memory length: 100000   epsilon: 0.861625720003004    steps: 124    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 913   score: 2.0   memory length: 100000   epsilon: 0.8611960600030133    steps: 217    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 914   score: 0.0   memory length: 100000   epsilon: 0.8609525200030186    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 915   score: 3.0   memory length: 100000   epsilon: 0.8604218800030301    steps: 268    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 916   score: 5.0   memory length: 100000   epsilon: 0.8597427400030448    steps: 343    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 917   score: 0.0   memory length: 100000   epsilon: 0.8594972200030502    steps: 124    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 918   score: 0.0   memory length: 100000   epsilon: 0.8592517000030555    steps: 124    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 919   score: 0.0   memory length: 100000   epsilon: 0.8590061800030608    steps: 124    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 920   score: 2.0   memory length: 100000   epsilon: 0.8586121600030694    steps: 199    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 921   score: 1.0   memory length: 100000   epsilon: 0.8583112000030759    steps: 152    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 922   score: 1.0   memory length: 100000   epsilon: 0.8580122200030824    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 923   score: 2.0   memory length: 100000   epsilon: 0.8576201800030909    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 924   score: 0.0   memory length: 100000   epsilon: 0.8573746600030963    steps: 124    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 925   score: 3.0   memory length: 100000   epsilon: 0.856927180003106    steps: 226    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 926   score: 2.0   memory length: 100000   epsilon: 0.8564915800031154    steps: 220    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 927   score: 1.0   memory length: 100000   epsilon: 0.8561510200031228    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 928   score: 3.0   memory length: 100000   epsilon: 0.8557253200031321    steps: 215    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 929   score: 1.0   memory length: 100000   epsilon: 0.8554243600031386    steps: 152    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 930   score: 3.0   memory length: 100000   epsilon: 0.8549333200031493    steps: 248    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 931   score: 0.0   memory length: 100000   epsilon: 0.8546878000031546    steps: 124    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 932   score: 2.0   memory length: 100000   epsilon: 0.8542957600031631    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 933   score: 2.0   memory length: 100000   epsilon: 0.8539017400031716    steps: 199    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 934   score: 2.0   memory length: 100000   epsilon: 0.8535433600031794    steps: 181    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 935   score: 1.0   memory length: 100000   epsilon: 0.8532067600031867    steps: 170    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 936   score: 1.0   memory length: 100000   epsilon: 0.852870160003194    steps: 170    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 937   score: 1.0   memory length: 100000   epsilon: 0.8525692000032006    steps: 152    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 938   score: 1.0   memory length: 100000   epsilon: 0.8522326000032079    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 939   score: 1.0   memory length: 100000   epsilon: 0.8519316400032144    steps: 152    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 940   score: 1.0   memory length: 100000   epsilon: 0.8515910800032218    steps: 172    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 941   score: 0.0   memory length: 100000   epsilon: 0.8513455600032271    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 942   score: 1.0   memory length: 100000   epsilon: 0.8510069800032345    steps: 171    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 943   score: 2.0   memory length: 100000   epsilon: 0.8506486000032423    steps: 181    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 944   score: 0.0   memory length: 100000   epsilon: 0.8504030800032476    steps: 124    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 945   score: 0.0   memory length: 100000   epsilon: 0.8501575600032529    steps: 124    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 946   score: 3.0   memory length: 100000   epsilon: 0.849694240003263    steps: 234    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 947   score: 2.0   memory length: 100000   epsilon: 0.8493002200032715    steps: 199    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 948   score: 0.0   memory length: 100000   epsilon: 0.8490566800032768    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 949   score: 2.0   memory length: 100000   epsilon: 0.8486270200032862    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 950   score: 1.0   memory length: 100000   epsilon: 0.8483280400032926    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 951   score: 1.0   memory length: 100000   epsilon: 0.8480270800032992    steps: 152    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 952   score: 2.0   memory length: 100000   epsilon: 0.8475954400033086    steps: 218    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 953   score: 0.0   memory length: 100000   epsilon: 0.8473499200033139    steps: 124    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 954   score: 1.0   memory length: 100000   epsilon: 0.8470073800033213    steps: 173    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 955   score: 2.0   memory length: 100000   epsilon: 0.8466133600033299    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 956   score: 4.0   memory length: 100000   epsilon: 0.8459956000033433    steps: 312    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 957   score: 0.0   memory length: 100000   epsilon: 0.8457500800033486    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 958   score: 0.0   memory length: 100000   epsilon: 0.8455065400033539    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 959   score: 3.0   memory length: 100000   epsilon: 0.8450590600033636    steps: 226    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 960   score: 3.0   memory length: 100000   epsilon: 0.8446076200033734    steps: 228    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 961   score: 1.0   memory length: 100000   epsilon: 0.84430666000338    steps: 152    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 962   score: 2.0   memory length: 100000   epsilon: 0.8439146200033885    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 963   score: 2.0   memory length: 100000   epsilon: 0.8434790200033979    steps: 220    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 964   score: 1.0   memory length: 100000   epsilon: 0.8431780600034045    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 965   score: 2.0   memory length: 100000   epsilon: 0.842740480003414    steps: 221    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 966   score: 2.0   memory length: 100000   epsilon: 0.8423464600034225    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 967   score: 2.0   memory length: 100000   epsilon: 0.8419861000034303    steps: 182    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 968   score: 4.0   memory length: 100000   epsilon: 0.8413881400034433    steps: 302    lr: 0.0001     evaluation reward: 1.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 969   score: 2.0   memory length: 100000   epsilon: 0.840988180003452    steps: 202    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 970   score: 3.0   memory length: 100000   epsilon: 0.8404615000034634    steps: 266    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 971   score: 2.0   memory length: 100000   epsilon: 0.840067480003472    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 972   score: 2.0   memory length: 100000   epsilon: 0.8396279200034815    steps: 222    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 973   score: 1.0   memory length: 100000   epsilon: 0.8392893400034889    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 974   score: 2.0   memory length: 100000   epsilon: 0.8388953200034974    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 975   score: 3.0   memory length: 100000   epsilon: 0.8384023000035081    steps: 249    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 976   score: 0.0   memory length: 100000   epsilon: 0.8381567800035135    steps: 124    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 977   score: 2.0   memory length: 100000   epsilon: 0.8377271200035228    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 978   score: 5.0   memory length: 100000   epsilon: 0.8371212400035359    steps: 306    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 979   score: 2.0   memory length: 100000   epsilon: 0.8367272200035445    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 980   score: 1.0   memory length: 100000   epsilon: 0.8363886400035518    steps: 171    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 981   score: 2.0   memory length: 100000   epsilon: 0.8359946200035604    steps: 199    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 982   score: 1.0   memory length: 100000   epsilon: 0.8356560400035677    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 983   score: 0.0   memory length: 100000   epsilon: 0.8354105200035731    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 984   score: 0.0   memory length: 100000   epsilon: 0.8351650000035784    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 985   score: 5.0   memory length: 100000   epsilon: 0.8345333800035921    steps: 319    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 986   score: 2.0   memory length: 100000   epsilon: 0.8341413400036006    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 987   score: 3.0   memory length: 100000   epsilon: 0.8336483200036113    steps: 249    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 988   score: 0.0   memory length: 100000   epsilon: 0.8334028000036167    steps: 124    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 989   score: 0.0   memory length: 100000   epsilon: 0.833159260003622    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 990   score: 1.0   memory length: 100000   epsilon: 0.8328226600036293    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 991   score: 1.0   memory length: 100000   epsilon: 0.8324880400036365    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 992   score: 2.0   memory length: 100000   epsilon: 0.8320564000036459    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 993   score: 0.0   memory length: 100000   epsilon: 0.8318108800036512    steps: 124    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 994   score: 2.0   memory length: 100000   epsilon: 0.8314188400036597    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 995   score: 1.0   memory length: 100000   epsilon: 0.8311198600036662    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 996   score: 0.0   memory length: 100000   epsilon: 0.8308743400036716    steps: 124    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 997   score: 1.0   memory length: 100000   epsilon: 0.8305733800036781    steps: 152    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 998   score: 0.0   memory length: 100000   epsilon: 0.8303298400036834    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 999   score: 1.0   memory length: 100000   epsilon: 0.8300308600036899    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1000   score: 0.0   memory length: 100000   epsilon: 0.8297853400036952    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1001   score: 0.0   memory length: 100000   epsilon: 0.8295418000037005    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1002   score: 2.0   memory length: 100000   epsilon: 0.829147780003709    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1003   score: 1.0   memory length: 100000   epsilon: 0.8288092000037164    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1004   score: 2.0   memory length: 100000   epsilon: 0.8283736000037258    steps: 220    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1005   score: 0.0   memory length: 100000   epsilon: 0.8281280800037312    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1006   score: 6.0   memory length: 100000   epsilon: 0.8273895400037472    steps: 373    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1007   score: 1.0   memory length: 100000   epsilon: 0.8270885800037537    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1008   score: 1.0   memory length: 100000   epsilon: 0.8267876200037603    steps: 152    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1009   score: 0.0   memory length: 100000   epsilon: 0.8265421000037656    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1010   score: 0.0   memory length: 100000   epsilon: 0.8262965800037709    steps: 124    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1011   score: 0.0   memory length: 100000   epsilon: 0.8260530400037762    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1012   score: 1.0   memory length: 100000   epsilon: 0.8257124800037836    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1013   score: 0.0   memory length: 100000   epsilon: 0.8254689400037889    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1014   score: 3.0   memory length: 100000   epsilon: 0.8249759200037996    steps: 249    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1015   score: 3.0   memory length: 100000   epsilon: 0.8245225000038094    steps: 229    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1016   score: 1.0   memory length: 100000   epsilon: 0.8241839200038168    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1017   score: 0.0   memory length: 100000   epsilon: 0.8239403800038221    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1018   score: 0.0   memory length: 100000   epsilon: 0.8236948600038274    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1019   score: 0.0   memory length: 100000   epsilon: 0.8234493400038327    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1020   score: 1.0   memory length: 100000   epsilon: 0.8231503600038392    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1021   score: 2.0   memory length: 100000   epsilon: 0.8227226800038485    steps: 216    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1022   score: 2.0   memory length: 100000   epsilon: 0.8222950000038578    steps: 216    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1023   score: 2.0   memory length: 100000   epsilon: 0.8219009800038664    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1024   score: 1.0   memory length: 100000   epsilon: 0.8215643800038737    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1025   score: 0.0   memory length: 100000   epsilon: 0.821320840003879    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1026   score: 3.0   memory length: 100000   epsilon: 0.8209010800038881    steps: 212    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1027   score: 3.0   memory length: 100000   epsilon: 0.8203763800038995    steps: 265    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1028   score: 0.0   memory length: 100000   epsilon: 0.8201328400039047    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1029   score: 1.0   memory length: 100000   epsilon: 0.8197903000039122    steps: 173    lr: 0.0001     evaluation reward: 1.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1030   score: 3.0   memory length: 100000   epsilon: 0.8192992600039228    steps: 248    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1031   score: 1.0   memory length: 100000   epsilon: 0.8189626600039301    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1032   score: 0.0   memory length: 100000   epsilon: 0.8187171400039355    steps: 124    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1033   score: 0.0   memory length: 100000   epsilon: 0.8184716200039408    steps: 124    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 1034   score: 0.0   memory length: 100000   epsilon: 0.8182261000039461    steps: 124    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 1035   score: 0.0   memory length: 100000   epsilon: 0.8179825600039514    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1036   score: 1.0   memory length: 100000   epsilon: 0.8176459600039587    steps: 170    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 1037   score: 3.0   memory length: 100000   epsilon: 0.8171549200039694    steps: 248    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 1038   score: 4.0   memory length: 100000   epsilon: 0.8165351800039828    steps: 313    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 1039   score: 3.0   memory length: 100000   epsilon: 0.8160461200039935    steps: 247    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1040   score: 0.0   memory length: 100000   epsilon: 0.8158006000039988    steps: 124    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 1041   score: 2.0   memory length: 100000   epsilon: 0.8154065800040073    steps: 199    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1042   score: 0.0   memory length: 100000   epsilon: 0.8151610600040127    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1043   score: 3.0   memory length: 100000   epsilon: 0.8146700200040233    steps: 248    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1044   score: 0.0   memory length: 100000   epsilon: 0.8144245000040287    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1045   score: 1.0   memory length: 100000   epsilon: 0.8140898800040359    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1046   score: 3.0   memory length: 100000   epsilon: 0.8135533000040476    steps: 271    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1047   score: 0.0   memory length: 100000   epsilon: 0.8133077800040529    steps: 124    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1048   score: 2.0   memory length: 100000   epsilon: 0.8128741600040623    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1049   score: 0.0   memory length: 100000   epsilon: 0.8126306200040676    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 1050   score: 3.0   memory length: 100000   epsilon: 0.8121415600040782    steps: 247    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 1051   score: 0.0   memory length: 100000   epsilon: 0.8118960400040836    steps: 124    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1052   score: 2.0   memory length: 100000   epsilon: 0.8114663800040929    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 1053   score: 2.0   memory length: 100000   epsilon: 0.8110743400041014    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1054   score: 2.0   memory length: 100000   epsilon: 0.8107159600041092    steps: 181    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1055   score: 2.0   memory length: 100000   epsilon: 0.8102783800041187    steps: 221    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1056   score: 4.0   memory length: 100000   epsilon: 0.8097259600041307    steps: 279    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 1057   score: 2.0   memory length: 100000   epsilon: 0.8093279800041393    steps: 201    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1058   score: 3.0   memory length: 100000   epsilon: 0.8088765400041491    steps: 228    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1059   score: 2.0   memory length: 100000   epsilon: 0.8084825200041577    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1060   score: 2.0   memory length: 100000   epsilon: 0.8080489000041671    steps: 219    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1061   score: 2.0   memory length: 100000   epsilon: 0.8076548800041756    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1062   score: 2.0   memory length: 100000   epsilon: 0.807221260004185    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1063   score: 1.0   memory length: 100000   epsilon: 0.8069203000041916    steps: 152    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1064   score: 3.0   memory length: 100000   epsilon: 0.8064253000042023    steps: 250    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1065   score: 2.0   memory length: 100000   epsilon: 0.806025340004211    steps: 202    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1066   score: 3.0   memory length: 100000   epsilon: 0.8055343000042217    steps: 248    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1067   score: 1.0   memory length: 100000   epsilon: 0.8052313600042282    steps: 153    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1068   score: 2.0   memory length: 100000   epsilon: 0.8048393200042367    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1069   score: 3.0   memory length: 100000   epsilon: 0.8043542200042473    steps: 245    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1070   score: 2.0   memory length: 100000   epsilon: 0.8039602000042558    steps: 199    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1071   score: 4.0   memory length: 100000   epsilon: 0.8033721400042686    steps: 297    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1072   score: 0.0   memory length: 100000   epsilon: 0.8031266200042739    steps: 124    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1073   score: 3.0   memory length: 100000   epsilon: 0.8026355800042846    steps: 248    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1074   score: 2.0   memory length: 100000   epsilon: 0.8022752200042924    steps: 182    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1075   score: 1.0   memory length: 100000   epsilon: 0.8019346600042998    steps: 172    lr: 4e-05     evaluation reward: 1.49\n",
      "episode: 1076   score: 2.0   memory length: 100000   epsilon: 0.8015366800043084    steps: 201    lr: 4e-05     evaluation reward: 1.51\n",
      "episode: 1077   score: 3.0   memory length: 100000   epsilon: 0.8010892000043182    steps: 226    lr: 4e-05     evaluation reward: 1.52\n",
      "episode: 1078   score: 3.0   memory length: 100000   epsilon: 0.8006397400043279    steps: 227    lr: 4e-05     evaluation reward: 1.5\n",
      "episode: 1079   score: 2.0   memory length: 100000   epsilon: 0.8002457200043365    steps: 199    lr: 4e-05     evaluation reward: 1.5\n",
      "episode: 1080   score: 3.0   memory length: 100000   epsilon: 0.7998299200043455    steps: 210    lr: 4e-05     evaluation reward: 1.52\n",
      "episode: 1081   score: 4.0   memory length: 100000   epsilon: 0.7992418600043583    steps: 297    lr: 4e-05     evaluation reward: 1.54\n",
      "episode: 1082   score: 2.0   memory length: 100000   epsilon: 0.7988062600043677    steps: 220    lr: 4e-05     evaluation reward: 1.55\n",
      "episode: 1083   score: 2.0   memory length: 100000   epsilon: 0.7983706600043772    steps: 220    lr: 4e-05     evaluation reward: 1.57\n",
      "episode: 1084   score: 2.0   memory length: 100000   epsilon: 0.7979766400043857    steps: 199    lr: 4e-05     evaluation reward: 1.59\n",
      "episode: 1085   score: 3.0   memory length: 100000   epsilon: 0.7975232200043956    steps: 229    lr: 4e-05     evaluation reward: 1.57\n",
      "episode: 1086   score: 0.0   memory length: 100000   epsilon: 0.7972777000044009    steps: 124    lr: 4e-05     evaluation reward: 1.55\n",
      "episode: 1087   score: 1.0   memory length: 100000   epsilon: 0.7969767400044074    steps: 152    lr: 4e-05     evaluation reward: 1.53\n",
      "episode: 1088   score: 1.0   memory length: 100000   epsilon: 0.7966381600044148    steps: 171    lr: 4e-05     evaluation reward: 1.54\n",
      "episode: 1089   score: 2.0   memory length: 100000   epsilon: 0.7962441400044233    steps: 199    lr: 4e-05     evaluation reward: 1.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1090   score: 2.0   memory length: 100000   epsilon: 0.7958521000044319    steps: 198    lr: 4e-05     evaluation reward: 1.57\n",
      "episode: 1091   score: 4.0   memory length: 100000   epsilon: 0.7952640400044446    steps: 297    lr: 4e-05     evaluation reward: 1.6\n",
      "episode: 1092   score: 3.0   memory length: 100000   epsilon: 0.7947789400044551    steps: 245    lr: 4e-05     evaluation reward: 1.61\n",
      "episode: 1093   score: 3.0   memory length: 100000   epsilon: 0.7942879000044658    steps: 248    lr: 4e-05     evaluation reward: 1.64\n",
      "episode: 1094   score: 0.0   memory length: 100000   epsilon: 0.7940443600044711    steps: 123    lr: 4e-05     evaluation reward: 1.62\n",
      "episode: 1095   score: 2.0   memory length: 100000   epsilon: 0.7936523200044796    steps: 198    lr: 4e-05     evaluation reward: 1.63\n",
      "episode: 1096   score: 1.0   memory length: 100000   epsilon: 0.7933157200044869    steps: 170    lr: 4e-05     evaluation reward: 1.64\n",
      "episode: 1097   score: 0.0   memory length: 100000   epsilon: 0.7930702000044922    steps: 124    lr: 4e-05     evaluation reward: 1.63\n",
      "episode: 1098   score: 1.0   memory length: 100000   epsilon: 0.7927336000044996    steps: 170    lr: 4e-05     evaluation reward: 1.64\n",
      "episode: 1099   score: 0.0   memory length: 100000   epsilon: 0.7924880800045049    steps: 124    lr: 4e-05     evaluation reward: 1.63\n",
      "episode: 1100   score: 1.0   memory length: 100000   epsilon: 0.7921514800045122    steps: 170    lr: 4e-05     evaluation reward: 1.64\n",
      "episode: 1101   score: 2.0   memory length: 100000   epsilon: 0.7917178600045216    steps: 219    lr: 4e-05     evaluation reward: 1.66\n",
      "episode: 1102   score: 2.0   memory length: 100000   epsilon: 0.7913555200045295    steps: 183    lr: 4e-05     evaluation reward: 1.66\n",
      "episode: 1103   score: 3.0   memory length: 100000   epsilon: 0.7908288400045409    steps: 266    lr: 4e-05     evaluation reward: 1.68\n",
      "episode: 1104   score: 1.0   memory length: 100000   epsilon: 0.7904902600045483    steps: 171    lr: 4e-05     evaluation reward: 1.67\n",
      "episode: 1105   score: 2.0   memory length: 100000   epsilon: 0.7900942600045568    steps: 200    lr: 4e-05     evaluation reward: 1.69\n",
      "episode: 1106   score: 2.0   memory length: 100000   epsilon: 0.7897022200045654    steps: 198    lr: 4e-05     evaluation reward: 1.65\n",
      "episode: 1107   score: 2.0   memory length: 100000   epsilon: 0.7893002800045741    steps: 203    lr: 4e-05     evaluation reward: 1.66\n",
      "episode: 1108   score: 5.0   memory length: 100000   epsilon: 0.7886944000045872    steps: 306    lr: 4e-05     evaluation reward: 1.7\n",
      "episode: 1109   score: 3.0   memory length: 100000   epsilon: 0.7882390000045971    steps: 230    lr: 4e-05     evaluation reward: 1.73\n",
      "episode: 1110   score: 0.0   memory length: 100000   epsilon: 0.7879954600046024    steps: 123    lr: 4e-05     evaluation reward: 1.73\n",
      "episode: 1111   score: 3.0   memory length: 100000   epsilon: 0.7875044200046131    steps: 248    lr: 4e-05     evaluation reward: 1.76\n",
      "episode: 1112   score: 5.0   memory length: 100000   epsilon: 0.7868708200046268    steps: 320    lr: 4e-05     evaluation reward: 1.8\n",
      "episode: 1113   score: 3.0   memory length: 100000   epsilon: 0.7863817600046374    steps: 247    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1114   score: 3.0   memory length: 100000   epsilon: 0.7859323000046472    steps: 227    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1115   score: 2.0   memory length: 100000   epsilon: 0.7855382800046558    steps: 199    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1116   score: 2.0   memory length: 100000   epsilon: 0.7851442600046643    steps: 199    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1117   score: 0.0   memory length: 100000   epsilon: 0.7848987400046696    steps: 124    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1118   score: 3.0   memory length: 100000   epsilon: 0.7844096800046803    steps: 247    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1119   score: 3.0   memory length: 100000   epsilon: 0.7839186400046909    steps: 248    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1120   score: 1.0   memory length: 100000   epsilon: 0.7836176800046974    steps: 152    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1121   score: 6.0   memory length: 100000   epsilon: 0.7829286400047124    steps: 348    lr: 4e-05     evaluation reward: 1.93\n",
      "episode: 1122   score: 8.0   memory length: 100000   epsilon: 0.7821524800047293    steps: 392    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1123   score: 3.0   memory length: 100000   epsilon: 0.781703020004739    steps: 227    lr: 4e-05     evaluation reward: 2.0\n",
      "episode: 1124   score: 0.0   memory length: 100000   epsilon: 0.7814575000047443    steps: 124    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1125   score: 4.0   memory length: 100000   epsilon: 0.780873400004757    steps: 295    lr: 4e-05     evaluation reward: 2.03\n",
      "episode: 1126   score: 0.0   memory length: 100000   epsilon: 0.7806278800047624    steps: 124    lr: 4e-05     evaluation reward: 2.0\n",
      "episode: 1127   score: 11.0   memory length: 100000   epsilon: 0.7797032200047824    steps: 467    lr: 4e-05     evaluation reward: 2.08\n",
      "episode: 1128   score: 3.0   memory length: 100000   epsilon: 0.7792478200047923    steps: 230    lr: 4e-05     evaluation reward: 2.11\n",
      "episode: 1129   score: 2.0   memory length: 100000   epsilon: 0.7788142000048017    steps: 219    lr: 4e-05     evaluation reward: 2.12\n",
      "episode: 1130   score: 2.0   memory length: 100000   epsilon: 0.7784538400048095    steps: 182    lr: 4e-05     evaluation reward: 2.11\n",
      "episode: 1131   score: 3.0   memory length: 100000   epsilon: 0.7780063600048193    steps: 226    lr: 4e-05     evaluation reward: 2.13\n",
      "episode: 1132   score: 2.0   memory length: 100000   epsilon: 0.7775687800048288    steps: 221    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1133   score: 4.0   memory length: 100000   epsilon: 0.7770124000048408    steps: 281    lr: 4e-05     evaluation reward: 2.19\n",
      "episode: 1134   score: 0.0   memory length: 100000   epsilon: 0.7767668800048462    steps: 124    lr: 4e-05     evaluation reward: 2.19\n",
      "episode: 1135   score: 2.0   memory length: 100000   epsilon: 0.7763728600048547    steps: 199    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1136   score: 3.0   memory length: 100000   epsilon: 0.7759531000048638    steps: 212    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1137   score: 2.0   memory length: 100000   epsilon: 0.7755194800048733    steps: 219    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1138   score: 0.0   memory length: 100000   epsilon: 0.7752739600048786    steps: 124    lr: 4e-05     evaluation reward: 2.18\n",
      "episode: 1139   score: 4.0   memory length: 100000   epsilon: 0.7747274800048904    steps: 276    lr: 4e-05     evaluation reward: 2.19\n",
      "episode: 1140   score: 0.0   memory length: 100000   epsilon: 0.7744819600048958    steps: 124    lr: 4e-05     evaluation reward: 2.19\n",
      "episode: 1141   score: 5.0   memory length: 100000   epsilon: 0.7737988600049106    steps: 345    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1142   score: 2.0   memory length: 100000   epsilon: 0.77336524000492    steps: 219    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1143   score: 4.0   memory length: 100000   epsilon: 0.7727831200049327    steps: 294    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1144   score: 3.0   memory length: 100000   epsilon: 0.7722960400049432    steps: 246    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1145   score: 2.0   memory length: 100000   epsilon: 0.7718644000049526    steps: 218    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1146   score: 2.0   memory length: 100000   epsilon: 0.7714703800049612    steps: 199    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1147   score: 2.0   memory length: 100000   epsilon: 0.7711120000049689    steps: 181    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1148   score: 2.0   memory length: 100000   epsilon: 0.7707179800049775    steps: 199    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1149   score: 3.0   memory length: 100000   epsilon: 0.770234860004988    steps: 244    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1150   score: 2.0   memory length: 100000   epsilon: 0.7698349000049967    steps: 202    lr: 4e-05     evaluation reward: 2.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1151   score: 2.0   memory length: 100000   epsilon: 0.7694408800050052    steps: 199    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1152   score: 0.0   memory length: 100000   epsilon: 0.7691953600050105    steps: 124    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1153   score: 1.0   memory length: 100000   epsilon: 0.768852820005018    steps: 173    lr: 4e-05     evaluation reward: 2.31\n",
      "episode: 1154   score: 2.0   memory length: 100000   epsilon: 0.7684152400050275    steps: 221    lr: 4e-05     evaluation reward: 2.31\n",
      "episode: 1155   score: 0.0   memory length: 100000   epsilon: 0.7681697200050328    steps: 124    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1156   score: 7.0   memory length: 100000   epsilon: 0.767334160005051    steps: 422    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1157   score: 3.0   memory length: 100000   epsilon: 0.7668451000050616    steps: 247    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1158   score: 0.0   memory length: 100000   epsilon: 0.7665995800050669    steps: 124    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1159   score: 0.0   memory length: 100000   epsilon: 0.7663560400050722    steps: 123    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1160   score: 2.0   memory length: 100000   epsilon: 0.7659620200050807    steps: 199    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1161   score: 1.0   memory length: 100000   epsilon: 0.7656610600050873    steps: 152    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1162   score: 4.0   memory length: 100000   epsilon: 0.7650690400051001    steps: 299    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1163   score: 0.0   memory length: 100000   epsilon: 0.7648235200051055    steps: 124    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1164   score: 2.0   memory length: 100000   epsilon: 0.764429500005114    steps: 199    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1165   score: 2.0   memory length: 100000   epsilon: 0.7640354800051226    steps: 199    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1166   score: 2.0   memory length: 100000   epsilon: 0.7636414600051311    steps: 199    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1167   score: 2.0   memory length: 100000   epsilon: 0.7632078400051405    steps: 219    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1168   score: 1.0   memory length: 100000   epsilon: 0.7629049000051471    steps: 153    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1169   score: 4.0   memory length: 100000   epsilon: 0.7623188200051598    steps: 296    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1170   score: 0.0   memory length: 100000   epsilon: 0.7620733000051652    steps: 124    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1171   score: 3.0   memory length: 100000   epsilon: 0.7615802800051759    steps: 249    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1172   score: 0.0   memory length: 100000   epsilon: 0.7613367400051811    steps: 123    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1173   score: 2.0   memory length: 100000   epsilon: 0.7609031200051906    steps: 219    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1174   score: 0.0   memory length: 100000   epsilon: 0.7606595800051958    steps: 123    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1175   score: 3.0   memory length: 100000   epsilon: 0.7602101200052056    steps: 227    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1176   score: 0.0   memory length: 100000   epsilon: 0.7599646000052109    steps: 124    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1177   score: 3.0   memory length: 100000   epsilon: 0.7594696000052217    steps: 250    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1178   score: 3.0   memory length: 100000   epsilon: 0.7589825200052323    steps: 246    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1179   score: 1.0   memory length: 100000   epsilon: 0.7586459200052396    steps: 170    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1180   score: 2.0   memory length: 100000   epsilon: 0.758212300005249    steps: 219    lr: 4e-05     evaluation reward: 2.19\n",
      "episode: 1181   score: 6.0   memory length: 100000   epsilon: 0.7574618800052653    steps: 379    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1182   score: 4.0   memory length: 100000   epsilon: 0.756877780005278    steps: 295    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1183   score: 4.0   memory length: 100000   epsilon: 0.7562441800052917    steps: 320    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1184   score: 3.0   memory length: 100000   epsilon: 0.7557511600053024    steps: 249    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1185   score: 3.0   memory length: 100000   epsilon: 0.7553017000053122    steps: 227    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1186   score: 0.0   memory length: 100000   epsilon: 0.7550581600053174    steps: 123    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1187   score: 3.0   memory length: 100000   epsilon: 0.754573060005328    steps: 245    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1188   score: 0.0   memory length: 100000   epsilon: 0.7543275400053333    steps: 124    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1189   score: 2.0   memory length: 100000   epsilon: 0.7538939200053427    steps: 219    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1190   score: 2.0   memory length: 100000   epsilon: 0.7534999000053513    steps: 199    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1191   score: 3.0   memory length: 100000   epsilon: 0.7530108400053619    steps: 247    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1192   score: 1.0   memory length: 100000   epsilon: 0.7526722600053692    steps: 171    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1193   score: 2.0   memory length: 100000   epsilon: 0.7522386400053787    steps: 219    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1194   score: 4.0   memory length: 100000   epsilon: 0.7516921600053905    steps: 276    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1195   score: 3.0   memory length: 100000   epsilon: 0.7511674600054019    steps: 265    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1196   score: 0.0   memory length: 100000   epsilon: 0.7509219400054072    steps: 124    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1197   score: 2.0   memory length: 100000   epsilon: 0.7504922800054166    steps: 217    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1198   score: 0.0   memory length: 100000   epsilon: 0.7502467600054219    steps: 124    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1199   score: 3.0   memory length: 100000   epsilon: 0.7497973000054317    steps: 227    lr: 4e-05     evaluation reward: 2.31\n",
      "episode: 1200   score: 3.0   memory length: 100000   epsilon: 0.7492666600054432    steps: 268    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1201   score: 4.0   memory length: 100000   epsilon: 0.7486370200054568    steps: 318    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1202   score: 2.0   memory length: 100000   epsilon: 0.7482430000054654    steps: 199    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1203   score: 1.0   memory length: 100000   epsilon: 0.7479064000054727    steps: 170    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1204   score: 3.0   memory length: 100000   epsilon: 0.7474153600054834    steps: 248    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1205   score: 3.0   memory length: 100000   epsilon: 0.7469659000054931    steps: 227    lr: 4e-05     evaluation reward: 2.36\n",
      "episode: 1206   score: 2.0   memory length: 100000   epsilon: 0.7465679200055018    steps: 201    lr: 4e-05     evaluation reward: 2.36\n",
      "episode: 1207   score: 1.0   memory length: 100000   epsilon: 0.7462313200055091    steps: 170    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1208   score: 4.0   memory length: 100000   epsilon: 0.7456432600055218    steps: 297    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1209   score: 8.0   memory length: 100000   epsilon: 0.7449839200055361    steps: 333    lr: 4e-05     evaluation reward: 2.39\n",
      "episode: 1210   score: 4.0   memory length: 100000   epsilon: 0.744437440005548    steps: 276    lr: 4e-05     evaluation reward: 2.43\n",
      "episode: 1211   score: 3.0   memory length: 100000   epsilon: 0.7439483800055586    steps: 247    lr: 4e-05     evaluation reward: 2.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1212   score: 3.0   memory length: 100000   epsilon: 0.7434989200055684    steps: 227    lr: 4e-05     evaluation reward: 2.41\n",
      "episode: 1213   score: 3.0   memory length: 100000   epsilon: 0.7430138200055789    steps: 245    lr: 4e-05     evaluation reward: 2.41\n",
      "episode: 1214   score: 0.0   memory length: 100000   epsilon: 0.7427683000055842    steps: 124    lr: 4e-05     evaluation reward: 2.38\n",
      "episode: 1215   score: 1.0   memory length: 100000   epsilon: 0.7424673400055908    steps: 152    lr: 4e-05     evaluation reward: 2.37\n",
      "episode: 1216   score: 0.0   memory length: 100000   epsilon: 0.7422218200055961    steps: 124    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1217   score: 4.0   memory length: 100000   epsilon: 0.7416357400056088    steps: 296    lr: 4e-05     evaluation reward: 2.39\n",
      "episode: 1218   score: 3.0   memory length: 100000   epsilon: 0.7411447000056195    steps: 248    lr: 4e-05     evaluation reward: 2.39\n",
      "episode: 1219   score: 3.0   memory length: 100000   epsilon: 0.7406972200056292    steps: 226    lr: 4e-05     evaluation reward: 2.39\n",
      "episode: 1220   score: 3.0   memory length: 100000   epsilon: 0.740247760005639    steps: 227    lr: 4e-05     evaluation reward: 2.41\n",
      "episode: 1221   score: 2.0   memory length: 100000   epsilon: 0.7398497800056476    steps: 201    lr: 4e-05     evaluation reward: 2.37\n",
      "episode: 1222   score: 0.0   memory length: 100000   epsilon: 0.7396042600056529    steps: 124    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1223   score: 1.0   memory length: 100000   epsilon: 0.7393033000056595    steps: 152    lr: 4e-05     evaluation reward: 2.27\n",
      "episode: 1224   score: 4.0   memory length: 100000   epsilon: 0.738679600005673    steps: 315    lr: 4e-05     evaluation reward: 2.31\n",
      "episode: 1225   score: 2.0   memory length: 100000   epsilon: 0.7382855800056816    steps: 199    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1226   score: 4.0   memory length: 100000   epsilon: 0.7376519800056953    steps: 320    lr: 4e-05     evaluation reward: 2.33\n",
      "episode: 1227   score: 2.0   memory length: 100000   epsilon: 0.7372579600057039    steps: 199    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1228   score: 3.0   memory length: 100000   epsilon: 0.7368085000057136    steps: 227    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1229   score: 3.0   memory length: 100000   epsilon: 0.7363590400057234    steps: 227    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1230   score: 0.0   memory length: 100000   epsilon: 0.7361135200057287    steps: 124    lr: 4e-05     evaluation reward: 2.23\n",
      "episode: 1231   score: 2.0   memory length: 100000   epsilon: 0.7356799000057381    steps: 219    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1232   score: 2.0   memory length: 100000   epsilon: 0.7352462800057475    steps: 219    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1233   score: 1.0   memory length: 100000   epsilon: 0.7349453200057541    steps: 152    lr: 4e-05     evaluation reward: 2.19\n",
      "episode: 1234   score: 3.0   memory length: 100000   epsilon: 0.7344978400057638    steps: 226    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1235   score: 5.0   memory length: 100000   epsilon: 0.7338523600057778    steps: 326    lr: 4e-05     evaluation reward: 2.25\n",
      "episode: 1236   score: 2.0   memory length: 100000   epsilon: 0.7334603200057863    steps: 198    lr: 4e-05     evaluation reward: 2.24\n",
      "episode: 1237   score: 6.0   memory length: 100000   epsilon: 0.7326802000058033    steps: 394    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1238   score: 4.0   memory length: 100000   epsilon: 0.7320505600058169    steps: 318    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1239   score: 2.0   memory length: 100000   epsilon: 0.7316525800058256    steps: 201    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1240   score: 4.0   memory length: 100000   epsilon: 0.7311457000058366    steps: 256    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1241   score: 2.0   memory length: 100000   epsilon: 0.7307873200058443    steps: 181    lr: 4e-05     evaluation reward: 2.31\n",
      "episode: 1242   score: 1.0   memory length: 100000   epsilon: 0.7304527000058516    steps: 169    lr: 4e-05     evaluation reward: 2.3\n",
      "episode: 1243   score: 3.0   memory length: 100000   epsilon: 0.7300032400058614    steps: 227    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1244   score: 3.0   memory length: 100000   epsilon: 0.729512200005872    steps: 248    lr: 4e-05     evaluation reward: 2.29\n",
      "episode: 1245   score: 5.0   memory length: 100000   epsilon: 0.7288271200058869    steps: 346    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1246   score: 2.0   memory length: 100000   epsilon: 0.7284331000058955    steps: 199    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1247   score: 4.0   memory length: 100000   epsilon: 0.7278034600059091    steps: 318    lr: 4e-05     evaluation reward: 2.34\n",
      "episode: 1248   score: 0.0   memory length: 100000   epsilon: 0.7275599200059144    steps: 123    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1249   score: 2.0   memory length: 100000   epsilon: 0.727165900005923    steps: 199    lr: 4e-05     evaluation reward: 2.31\n",
      "episode: 1250   score: 6.0   memory length: 100000   epsilon: 0.7264669600059381    steps: 353    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1251   score: 4.0   memory length: 100000   epsilon: 0.7258808800059509    steps: 296    lr: 4e-05     evaluation reward: 2.37\n",
      "episode: 1252   score: 3.0   memory length: 100000   epsilon: 0.7253957800059614    steps: 245    lr: 4e-05     evaluation reward: 2.4\n",
      "episode: 1253   score: 2.0   memory length: 100000   epsilon: 0.7250334400059693    steps: 183    lr: 4e-05     evaluation reward: 2.41\n",
      "episode: 1254   score: 3.0   memory length: 100000   epsilon: 0.724583980005979    steps: 227    lr: 4e-05     evaluation reward: 2.42\n",
      "episode: 1255   score: 3.0   memory length: 100000   epsilon: 0.7241345200059888    steps: 227    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1256   score: 2.0   memory length: 100000   epsilon: 0.7237405000059973    steps: 199    lr: 4e-05     evaluation reward: 2.4\n",
      "episode: 1257   score: 2.0   memory length: 100000   epsilon: 0.7233464800060059    steps: 199    lr: 4e-05     evaluation reward: 2.39\n",
      "episode: 1258   score: 2.0   memory length: 100000   epsilon: 0.7229168200060152    steps: 217    lr: 4e-05     evaluation reward: 2.41\n",
      "episode: 1259   score: 4.0   memory length: 100000   epsilon: 0.7223248000060281    steps: 299    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1260   score: 5.0   memory length: 100000   epsilon: 0.7216615000060425    steps: 335    lr: 4e-05     evaluation reward: 2.48\n",
      "episode: 1261   score: 0.0   memory length: 100000   epsilon: 0.7214159800060478    steps: 124    lr: 4e-05     evaluation reward: 2.47\n",
      "episode: 1262   score: 5.0   memory length: 100000   epsilon: 0.720808120006061    steps: 307    lr: 4e-05     evaluation reward: 2.48\n",
      "episode: 1263   score: 3.0   memory length: 100000   epsilon: 0.7203586600060707    steps: 227    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1264   score: 2.0   memory length: 100000   epsilon: 0.7199646400060793    steps: 199    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1265   score: 4.0   memory length: 100000   epsilon: 0.7193726200060921    steps: 299    lr: 4e-05     evaluation reward: 2.53\n",
      "episode: 1266   score: 2.0   memory length: 100000   epsilon: 0.7189786000061007    steps: 199    lr: 4e-05     evaluation reward: 2.53\n",
      "episode: 1267   score: 1.0   memory length: 100000   epsilon: 0.718642000006108    steps: 170    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1268   score: 5.0   memory length: 100000   epsilon: 0.7179945400061221    steps: 327    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1269   score: 3.0   memory length: 100000   epsilon: 0.7175450800061318    steps: 227    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1270   score: 2.0   memory length: 100000   epsilon: 0.7171510600061404    steps: 199    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1271   score: 11.0   memory length: 100000   epsilon: 0.7162679800061595    steps: 446    lr: 4e-05     evaluation reward: 2.65\n",
      "episode: 1272   score: 2.0   memory length: 100000   epsilon: 0.715875940006168    steps: 198    lr: 4e-05     evaluation reward: 2.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1273   score: 2.0   memory length: 100000   epsilon: 0.7154819200061766    steps: 199    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1274   score: 3.0   memory length: 100000   epsilon: 0.7150621600061857    steps: 212    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1275   score: 3.0   memory length: 100000   epsilon: 0.7145711200061964    steps: 248    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1276   score: 3.0   memory length: 100000   epsilon: 0.7140860200062069    steps: 245    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1277   score: 3.0   memory length: 100000   epsilon: 0.7135969600062175    steps: 247    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1278   score: 1.0   memory length: 100000   epsilon: 0.7132960000062241    steps: 152    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1279   score: 2.0   memory length: 100000   epsilon: 0.7129019800062326    steps: 199    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1280   score: 2.0   memory length: 100000   epsilon: 0.7125079600062412    steps: 199    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1281   score: 4.0   memory length: 100000   epsilon: 0.7119218800062539    steps: 296    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1282   score: 4.0   memory length: 100000   epsilon: 0.7113397600062665    steps: 294    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1283   score: 2.0   memory length: 100000   epsilon: 0.7109417800062752    steps: 201    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1284   score: 2.0   memory length: 100000   epsilon: 0.7105457800062838    steps: 200    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1285   score: 2.0   memory length: 100000   epsilon: 0.7101141400062931    steps: 218    lr: 4e-05     evaluation reward: 2.66\n",
      "episode: 1286   score: 3.0   memory length: 100000   epsilon: 0.7095835000063047    steps: 268    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1287   score: 1.0   memory length: 100000   epsilon: 0.7092825400063112    steps: 152    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1288   score: 2.0   memory length: 100000   epsilon: 0.7088885200063197    steps: 199    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1289   score: 2.0   memory length: 100000   epsilon: 0.7084964800063283    steps: 198    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1290   score: 3.0   memory length: 100000   epsilon: 0.708047020006338    steps: 227    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1291   score: 3.0   memory length: 100000   epsilon: 0.7075559800063487    steps: 248    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1292   score: 3.0   memory length: 100000   epsilon: 0.7070629600063594    steps: 249    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1293   score: 3.0   memory length: 100000   epsilon: 0.7066135000063691    steps: 227    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1294   score: 0.0   memory length: 100000   epsilon: 0.7063679800063745    steps: 124    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1295   score: 2.0   memory length: 100000   epsilon: 0.705926440006384    steps: 223    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1296   score: 4.0   memory length: 100000   epsilon: 0.705377980006396    steps: 277    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1297   score: 5.0   memory length: 100000   epsilon: 0.7047384400064098    steps: 323    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1298   score: 5.0   memory length: 100000   epsilon: 0.7040949400064238    steps: 325    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1299   score: 0.0   memory length: 100000   epsilon: 0.7038494200064291    steps: 124    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1300   score: 4.0   memory length: 100000   epsilon: 0.7032910600064413    steps: 282    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1301   score: 4.0   memory length: 100000   epsilon: 0.7027307200064534    steps: 283    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1302   score: 3.0   memory length: 100000   epsilon: 0.702241660006464    steps: 247    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1303   score: 4.0   memory length: 100000   epsilon: 0.7016595400064767    steps: 294    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1304   score: 3.0   memory length: 100000   epsilon: 0.7011744400064872    steps: 245    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1305   score: 3.0   memory length: 100000   epsilon: 0.7006834000064979    steps: 248    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1306   score: 3.0   memory length: 100000   epsilon: 0.7001884000065086    steps: 250    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1307   score: 5.0   memory length: 100000   epsilon: 0.6995765800065219    steps: 309    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1308   score: 0.0   memory length: 100000   epsilon: 0.6993310600065272    steps: 124    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1309   score: 5.0   memory length: 100000   epsilon: 0.6987212200065405    steps: 308    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1310   score: 5.0   memory length: 100000   epsilon: 0.6981133600065537    steps: 307    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1311   score: 6.0   memory length: 100000   epsilon: 0.6973906600065694    steps: 365    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1312   score: 8.0   memory length: 100000   epsilon: 0.696624400006586    steps: 387    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1313   score: 5.0   memory length: 100000   epsilon: 0.6959809000066    steps: 325    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1314   score: 2.0   memory length: 100000   epsilon: 0.6955888600066085    steps: 198    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1315   score: 3.0   memory length: 100000   epsilon: 0.69505822000662    steps: 268    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1316   score: 1.0   memory length: 100000   epsilon: 0.6947572600066265    steps: 152    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1317   score: 2.0   memory length: 100000   epsilon: 0.6943632400066351    steps: 199    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1318   score: 3.0   memory length: 100000   epsilon: 0.6938345800066466    steps: 267    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1319   score: 3.0   memory length: 100000   epsilon: 0.6933851200066563    steps: 227    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1320   score: 0.0   memory length: 100000   epsilon: 0.6931415800066616    steps: 123    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1321   score: 3.0   memory length: 100000   epsilon: 0.6926921200066714    steps: 227    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1322   score: 1.0   memory length: 100000   epsilon: 0.6923555200066787    steps: 170    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1323   score: 9.0   memory length: 100000   epsilon: 0.6915259000066967    steps: 419    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1324   score: 2.0   memory length: 100000   epsilon: 0.691094260006706    steps: 218    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1325   score: 3.0   memory length: 100000   epsilon: 0.6906012400067167    steps: 249    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1326   score: 5.0   memory length: 100000   epsilon: 0.6898825000067323    steps: 363    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1327   score: 4.0   memory length: 100000   epsilon: 0.6893399800067441    steps: 274    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1328   score: 2.0   memory length: 100000   epsilon: 0.6889459600067527    steps: 199    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1329   score: 2.0   memory length: 100000   epsilon: 0.6885836200067605    steps: 183    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1330   score: 3.0   memory length: 100000   epsilon: 0.6881341600067703    steps: 227    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1331   score: 4.0   memory length: 100000   epsilon: 0.6875837200067823    steps: 278    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1332   score: 4.0   memory length: 100000   epsilon: 0.686997640006795    steps: 296    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1333   score: 3.0   memory length: 100000   epsilon: 0.6865422400068049    steps: 230    lr: 4e-05     evaluation reward: 3.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1334   score: 2.0   memory length: 100000   epsilon: 0.6861482200068134    steps: 199    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1335   score: 3.0   memory length: 100000   epsilon: 0.685659160006824    steps: 247    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1336   score: 5.0   memory length: 100000   epsilon: 0.6849760600068389    steps: 345    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1337   score: 3.0   memory length: 100000   epsilon: 0.6844493800068503    steps: 266    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1338   score: 2.0   memory length: 100000   epsilon: 0.6840573400068588    steps: 198    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1339   score: 5.0   memory length: 100000   epsilon: 0.6834316600068724    steps: 316    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1340   score: 1.0   memory length: 100000   epsilon: 0.6831307000068789    steps: 152    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1341   score: 3.0   memory length: 100000   epsilon: 0.6826376800068896    steps: 249    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1342   score: 4.0   memory length: 100000   epsilon: 0.6820100200069033    steps: 317    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1343   score: 0.0   memory length: 100000   epsilon: 0.6817645000069086    steps: 124    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1344   score: 1.0   memory length: 100000   epsilon: 0.6814635400069151    steps: 152    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1345   score: 5.0   memory length: 100000   epsilon: 0.6808913200069275    steps: 289    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1346   score: 0.0   memory length: 100000   epsilon: 0.6806458000069329    steps: 124    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1347   score: 4.0   memory length: 100000   epsilon: 0.6800597200069456    steps: 296    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1348   score: 4.0   memory length: 100000   epsilon: 0.6794677000069584    steps: 299    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1349   score: 4.0   memory length: 100000   epsilon: 0.6789152800069704    steps: 279    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1350   score: 2.0   memory length: 100000   epsilon: 0.6785549200069783    steps: 182    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1351   score: 1.0   memory length: 100000   epsilon: 0.6782539600069848    steps: 152    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1352   score: 2.0   memory length: 100000   epsilon: 0.6778223200069942    steps: 218    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1353   score: 5.0   memory length: 100000   epsilon: 0.6772184200070073    steps: 305    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1354   score: 4.0   memory length: 100000   epsilon: 0.6766283800070201    steps: 298    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1355   score: 6.0   memory length: 100000   epsilon: 0.6759056800070358    steps: 365    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1356   score: 5.0   memory length: 100000   epsilon: 0.67525228000705    steps: 330    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1357   score: 5.0   memory length: 100000   epsilon: 0.6746087800070639    steps: 325    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1358   score: 3.0   memory length: 100000   epsilon: 0.6741256600070744    steps: 244    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1359   score: 4.0   memory length: 100000   epsilon: 0.6735772000070863    steps: 277    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1360   score: 1.0   memory length: 100000   epsilon: 0.6732782200070928    steps: 151    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1361   score: 3.0   memory length: 100000   epsilon: 0.6728287600071026    steps: 227    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1362   score: 3.0   memory length: 100000   epsilon: 0.6723357400071133    steps: 249    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1363   score: 2.0   memory length: 100000   epsilon: 0.6719437000071218    steps: 198    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1364   score: 3.0   memory length: 100000   epsilon: 0.6714506800071325    steps: 249    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1365   score: 2.0   memory length: 100000   epsilon: 0.671056660007141    steps: 199    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1366   score: 3.0   memory length: 100000   epsilon: 0.6706072000071508    steps: 227    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1367   score: 3.0   memory length: 100000   epsilon: 0.6701577400071606    steps: 227    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1368   score: 4.0   memory length: 100000   epsilon: 0.6696092800071725    steps: 277    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1369   score: 3.0   memory length: 100000   epsilon: 0.6691202200071831    steps: 247    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1370   score: 3.0   memory length: 100000   epsilon: 0.6686291800071937    steps: 248    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1371   score: 0.0   memory length: 100000   epsilon: 0.6683836600071991    steps: 124    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1372   score: 2.0   memory length: 100000   epsilon: 0.6679896400072076    steps: 199    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1373   score: 3.0   memory length: 100000   epsilon: 0.6674966200072183    steps: 249    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1374   score: 4.0   memory length: 100000   epsilon: 0.6669501400072302    steps: 276    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1375   score: 4.0   memory length: 100000   epsilon: 0.6664353400072414    steps: 260    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1376   score: 3.0   memory length: 100000   epsilon: 0.6659047000072529    steps: 268    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1377   score: 5.0   memory length: 100000   epsilon: 0.6653107000072658    steps: 300    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1378   score: 2.0   memory length: 100000   epsilon: 0.6649067800072745    steps: 204    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1379   score: 3.0   memory length: 100000   epsilon: 0.6644573200072843    steps: 227    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1380   score: 3.0   memory length: 100000   epsilon: 0.6640039000072941    steps: 229    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1381   score: 3.0   memory length: 100000   epsilon: 0.6634732600073057    steps: 268    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1382   score: 2.0   memory length: 100000   epsilon: 0.6630752800073143    steps: 201    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1383   score: 5.0   memory length: 100000   epsilon: 0.6624317800073283    steps: 325    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1384   score: 6.0   memory length: 100000   epsilon: 0.6617229400073437    steps: 358    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1385   score: 3.0   memory length: 100000   epsilon: 0.6612319000073543    steps: 248    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1386   score: 3.0   memory length: 100000   epsilon: 0.6607824400073641    steps: 227    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1387   score: 4.0   memory length: 100000   epsilon: 0.660188440007377    steps: 300    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1388   score: 2.0   memory length: 100000   epsilon: 0.6597904600073856    steps: 201    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1389   score: 4.0   memory length: 100000   epsilon: 0.6592420000073975    steps: 277    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1390   score: 4.0   memory length: 100000   epsilon: 0.6586559200074102    steps: 296    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1391   score: 2.0   memory length: 100000   epsilon: 0.6582619000074188    steps: 199    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1392   score: 4.0   memory length: 100000   epsilon: 0.6576758200074315    steps: 296    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1393   score: 2.0   memory length: 100000   epsilon: 0.65728378000744    steps: 198    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1394   score: 3.0   memory length: 100000   epsilon: 0.6568343200074498    steps: 227    lr: 4e-05     evaluation reward: 3.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1395   score: 0.0   memory length: 100000   epsilon: 0.6565888000074551    steps: 124    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1396   score: 3.0   memory length: 100000   epsilon: 0.6560938000074659    steps: 250    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1397   score: 1.0   memory length: 100000   epsilon: 0.6557928400074724    steps: 152    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1398   score: 1.0   memory length: 100000   epsilon: 0.6554562400074797    steps: 170    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1399   score: 3.0   memory length: 100000   epsilon: 0.6550067800074895    steps: 227    lr: 4e-05     evaluation reward: 3.14\n",
      "episode: 1400   score: 3.0   memory length: 100000   epsilon: 0.6545593000074992    steps: 226    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1401   score: 3.0   memory length: 100000   epsilon: 0.6541098400075089    steps: 227    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1402   score: 3.0   memory length: 100000   epsilon: 0.6536603800075187    steps: 227    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1403   score: 4.0   memory length: 100000   epsilon: 0.65313964000753    steps: 263    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1404   score: 4.0   memory length: 100000   epsilon: 0.6525575200075426    steps: 294    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1405   score: 3.0   memory length: 100000   epsilon: 0.6520268800075542    steps: 268    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1406   score: 6.0   memory length: 100000   epsilon: 0.6512804200075704    steps: 377    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1407   score: 2.0   memory length: 100000   epsilon: 0.6508864000075789    steps: 199    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1408   score: 2.0   memory length: 100000   epsilon: 0.6504547600075883    steps: 218    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1409   score: 1.0   memory length: 100000   epsilon: 0.6501538000075948    steps: 152    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1410   score: 1.0   memory length: 100000   epsilon: 0.6498528400076014    steps: 152    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1411   score: 3.0   memory length: 100000   epsilon: 0.6493241800076128    steps: 267    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1412   score: 3.0   memory length: 100000   epsilon: 0.6488331400076235    steps: 248    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1413   score: 3.0   memory length: 100000   epsilon: 0.648348040007634    steps: 245    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1414   score: 2.0   memory length: 100000   epsilon: 0.6479857000076419    steps: 183    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1415   score: 2.0   memory length: 100000   epsilon: 0.6475520800076513    steps: 219    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1416   score: 4.0   memory length: 100000   epsilon: 0.6470056000076632    steps: 276    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1417   score: 3.0   memory length: 100000   epsilon: 0.6465165400076738    steps: 247    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1418   score: 3.0   memory length: 100000   epsilon: 0.6460255000076844    steps: 248    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1419   score: 0.0   memory length: 100000   epsilon: 0.6457799800076898    steps: 124    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1420   score: 4.0   memory length: 100000   epsilon: 0.6452315200077017    steps: 277    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1421   score: 3.0   memory length: 100000   epsilon: 0.6447820600077114    steps: 227    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1422   score: 3.0   memory length: 100000   epsilon: 0.6442534000077229    steps: 267    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1423   score: 2.0   memory length: 100000   epsilon: 0.6438574000077315    steps: 200    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1424   score: 3.0   memory length: 100000   epsilon: 0.6433663600077422    steps: 248    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1425   score: 2.0   memory length: 100000   epsilon: 0.6429723400077507    steps: 199    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1426   score: 3.0   memory length: 100000   epsilon: 0.6424813000077614    steps: 248    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1427   score: 3.0   memory length: 100000   epsilon: 0.6419882800077721    steps: 249    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1428   score: 4.0   memory length: 100000   epsilon: 0.6414774400077832    steps: 258    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1429   score: 4.0   memory length: 100000   epsilon: 0.6408893800077959    steps: 297    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1430   score: 3.0   memory length: 100000   epsilon: 0.6404399200078057    steps: 227    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1431   score: 3.0   memory length: 100000   epsilon: 0.6399865000078155    steps: 229    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1432   score: 6.0   memory length: 100000   epsilon: 0.6392519200078315    steps: 371    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1433   score: 2.0   memory length: 100000   epsilon: 0.6388519600078402    steps: 202    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1434   score: 3.0   memory length: 100000   epsilon: 0.6383609200078508    steps: 248    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1435   score: 2.0   memory length: 100000   epsilon: 0.6379273000078602    steps: 219    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1436   score: 3.0   memory length: 100000   epsilon: 0.6373966600078718    steps: 268    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1437   score: 3.0   memory length: 100000   epsilon: 0.6369472000078815    steps: 227    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1438   score: 1.0   memory length: 100000   epsilon: 0.636646240007888    steps: 152    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1439   score: 0.0   memory length: 100000   epsilon: 0.6364007200078934    steps: 124    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1440   score: 2.0   memory length: 100000   epsilon: 0.6360403600079012    steps: 182    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1441   score: 1.0   memory length: 100000   epsilon: 0.6357394000079077    steps: 152    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1442   score: 5.0   memory length: 100000   epsilon: 0.6350939200079218    steps: 326    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1443   score: 6.0   memory length: 100000   epsilon: 0.6344286400079362    steps: 336    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1444   score: 5.0   memory length: 100000   epsilon: 0.6337891000079501    steps: 323    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1445   score: 5.0   memory length: 100000   epsilon: 0.6332010400079628    steps: 297    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1446   score: 1.0   memory length: 100000   epsilon: 0.6329000800079694    steps: 152    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1447   score: 5.0   memory length: 100000   epsilon: 0.6322169800079842    steps: 345    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1448   score: 2.0   memory length: 100000   epsilon: 0.631860580007992    steps: 180    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1449   score: 1.0   memory length: 100000   epsilon: 0.6315616000079984    steps: 151    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1450   score: 2.0   memory length: 100000   epsilon: 0.6311992600080063    steps: 183    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1451   score: 4.0   memory length: 100000   epsilon: 0.6306508000080182    steps: 277    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1452   score: 1.0   memory length: 100000   epsilon: 0.6303498400080247    steps: 152    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1453   score: 1.0   memory length: 100000   epsilon: 0.6300488800080313    steps: 152    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1454   score: 4.0   memory length: 100000   epsilon: 0.6294984400080432    steps: 278    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1455   score: 2.0   memory length: 100000   epsilon: 0.6290648200080526    steps: 219    lr: 4e-05     evaluation reward: 2.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1456   score: 2.0   memory length: 100000   epsilon: 0.6286648600080613    steps: 202    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1457   score: 3.0   memory length: 100000   epsilon: 0.628217380008071    steps: 226    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1458   score: 3.0   memory length: 100000   epsilon: 0.6277619800080809    steps: 230    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1459   score: 4.0   memory length: 100000   epsilon: 0.6272095600080929    steps: 279    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1460   score: 3.0   memory length: 100000   epsilon: 0.6267601000081027    steps: 227    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1461   score: 4.0   memory length: 100000   epsilon: 0.6261799600081153    steps: 293    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1462   score: 2.0   memory length: 100000   epsilon: 0.6257859400081238    steps: 199    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1463   score: 3.0   memory length: 100000   epsilon: 0.6253008400081344    steps: 245    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1464   score: 4.0   memory length: 100000   epsilon: 0.6247543600081462    steps: 276    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1465   score: 1.0   memory length: 100000   epsilon: 0.6244534000081527    steps: 152    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1466   score: 2.0   memory length: 100000   epsilon: 0.6240593800081613    steps: 199    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1467   score: 4.0   memory length: 100000   epsilon: 0.6234990400081735    steps: 283    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1468   score: 4.0   memory length: 100000   epsilon: 0.6229129600081862    steps: 296    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1469   score: 4.0   memory length: 100000   epsilon: 0.622324900008199    steps: 297    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1470   score: 6.0   memory length: 100000   epsilon: 0.6216536800082135    steps: 339    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1471   score: 2.0   memory length: 100000   epsilon: 0.6212953000082213    steps: 181    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1472   score: 2.0   memory length: 100000   epsilon: 0.620938900008229    steps: 180    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1473   score: 3.0   memory length: 100000   epsilon: 0.6204894400082388    steps: 227    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1474   score: 2.0   memory length: 100000   epsilon: 0.6201271000082467    steps: 183    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1475   score: 7.0   memory length: 100000   epsilon: 0.6193648000082632    steps: 385    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1476   score: 4.0   memory length: 100000   epsilon: 0.6188104000082753    steps: 280    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1477   score: 4.0   memory length: 100000   epsilon: 0.6183292600082857    steps: 243    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1478   score: 3.0   memory length: 100000   epsilon: 0.6178382200082964    steps: 248    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1479   score: 2.0   memory length: 100000   epsilon: 0.6174442000083049    steps: 199    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1480   score: 0.0   memory length: 100000   epsilon: 0.6171986800083102    steps: 124    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1481   score: 3.0   memory length: 100000   epsilon: 0.61674922000832    steps: 227    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1482   score: 2.0   memory length: 100000   epsilon: 0.6163868800083279    steps: 183    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1483   score: 4.0   memory length: 100000   epsilon: 0.6157592200083415    steps: 317    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1484   score: 2.0   memory length: 100000   epsilon: 0.61536520000835    steps: 199    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1485   score: 1.0   memory length: 100000   epsilon: 0.6150642400083566    steps: 152    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1486   score: 4.0   memory length: 100000   epsilon: 0.6145494400083678    steps: 260    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1487   score: 0.0   memory length: 100000   epsilon: 0.614305900008373    steps: 123    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1488   score: 6.0   memory length: 100000   epsilon: 0.6135317200083898    steps: 391    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1489   score: 1.0   memory length: 100000   epsilon: 0.6132307600083964    steps: 152    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1490   score: 2.0   memory length: 100000   epsilon: 0.6128387200084049    steps: 198    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1491   score: 4.0   memory length: 100000   epsilon: 0.6122863000084169    steps: 279    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1492   score: 5.0   memory length: 100000   epsilon: 0.61168042000843    steps: 306    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1493   score: 3.0   memory length: 100000   epsilon: 0.6112309600084398    steps: 227    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1494   score: 2.0   memory length: 100000   epsilon: 0.6108369400084483    steps: 199    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1495   score: 3.0   memory length: 100000   epsilon: 0.6103874800084581    steps: 227    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1496   score: 3.0   memory length: 100000   epsilon: 0.6098924800084689    steps: 250    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1497   score: 4.0   memory length: 100000   epsilon: 0.6093103600084815    steps: 294    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1498   score: 3.0   memory length: 100000   epsilon: 0.6088153600084922    steps: 250    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1499   score: 3.0   memory length: 100000   epsilon: 0.608365900008502    steps: 227    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1500   score: 1.0   memory length: 100000   epsilon: 0.6080649400085085    steps: 152    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1501   score: 1.0   memory length: 100000   epsilon: 0.6077639800085151    steps: 152    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1502   score: 1.0   memory length: 100000   epsilon: 0.6074630200085216    steps: 152    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1503   score: 1.0   memory length: 100000   epsilon: 0.6071620600085281    steps: 152    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1504   score: 5.0   memory length: 100000   epsilon: 0.6065165800085421    steps: 326    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1505   score: 4.0   memory length: 100000   epsilon: 0.6059285200085549    steps: 297    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1506   score: 0.0   memory length: 100000   epsilon: 0.6056849800085602    steps: 123    lr: 4e-05     evaluation reward: 2.79\n",
      "episode: 1507   score: 3.0   memory length: 100000   epsilon: 0.6051919600085709    steps: 249    lr: 4e-05     evaluation reward: 2.8\n",
      "episode: 1508   score: 3.0   memory length: 100000   epsilon: 0.6047088400085814    steps: 244    lr: 4e-05     evaluation reward: 2.81\n",
      "episode: 1509   score: 3.0   memory length: 100000   epsilon: 0.6042494800085914    steps: 232    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1510   score: 1.0   memory length: 100000   epsilon: 0.6039485200085979    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1511   score: 3.0   memory length: 100000   epsilon: 0.6034951000086077    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1512   score: 3.0   memory length: 100000   epsilon: 0.6030753400086168    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1513   score: 6.0   memory length: 100000   epsilon: 0.602330860008633    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 2.86\n",
      "episode: 1514   score: 2.0   memory length: 100000   epsilon: 0.6019368400086416    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 2.86\n",
      "episode: 1515   score: 4.0   memory length: 100000   epsilon: 0.6013567000086542    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 2.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1516   score: 2.0   memory length: 100000   epsilon: 0.600994360008662    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 2.86\n",
      "episode: 1517   score: 1.0   memory length: 100000   epsilon: 0.6006557800086694    steps: 171    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1518   score: 2.0   memory length: 100000   epsilon: 0.6002934400086772    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1519   score: 2.0   memory length: 100000   epsilon: 0.5998994200086858    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 2.85\n",
      "episode: 1520   score: 4.0   memory length: 100000   epsilon: 0.5993529400086977    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 2.85\n",
      "episode: 1521   score: 1.0   memory length: 100000   epsilon: 0.599014360008705    steps: 171    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1522   score: 1.0   memory length: 100000   epsilon: 0.5987134000087115    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 2.81\n",
      "episode: 1523   score: 4.0   memory length: 100000   epsilon: 0.5981986000087227    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1524   score: 2.0   memory length: 100000   epsilon: 0.5978045800087313    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1525   score: 3.0   memory length: 100000   epsilon: 0.5973155200087419    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1526   score: 3.0   memory length: 100000   epsilon: 0.5968581400087518    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1527   score: 3.0   memory length: 100000   epsilon: 0.5963690800087624    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1528   score: 5.0   memory length: 100000   epsilon: 0.5957830000087752    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1529   score: 4.0   memory length: 100000   epsilon: 0.5952266200087872    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1530   score: 5.0   memory length: 100000   epsilon: 0.5946167800088005    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 2.86\n",
      "episode: 1531   score: 1.0   memory length: 100000   epsilon: 0.594313840008807    steps: 153    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1532   score: 4.0   memory length: 100000   epsilon: 0.5937218200088199    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1533   score: 3.0   memory length: 100000   epsilon: 0.5932327600088305    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1534   score: 5.0   memory length: 100000   epsilon: 0.5925952000088444    steps: 322    lr: 1.6000000000000003e-05     evaluation reward: 2.85\n",
      "episode: 1535   score: 1.0   memory length: 100000   epsilon: 0.5922546400088518    steps: 172    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1536   score: 4.0   memory length: 100000   epsilon: 0.5917477600088628    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 2.85\n",
      "episode: 1537   score: 5.0   memory length: 100000   epsilon: 0.5911597000088755    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 2.87\n",
      "episode: 1538   score: 1.0   memory length: 100000   epsilon: 0.590858740008882    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 2.87\n",
      "episode: 1539   score: 0.0   memory length: 100000   epsilon: 0.5906132200088874    steps: 124    lr: 1.6000000000000003e-05     evaluation reward: 2.87\n",
      "episode: 1540   score: 3.0   memory length: 100000   epsilon: 0.5901162400088982    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 2.88\n",
      "episode: 1541   score: 3.0   memory length: 100000   epsilon: 0.5897004400089072    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 2.9\n",
      "episode: 1542   score: 3.0   memory length: 100000   epsilon: 0.5892410800089172    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 2.88\n",
      "episode: 1543   score: 1.0   memory length: 100000   epsilon: 0.5889025000089245    steps: 171    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1544   score: 4.0   memory length: 100000   epsilon: 0.5883877000089357    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1545   score: 3.0   memory length: 100000   epsilon: 0.5878907200089465    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 2.8\n",
      "episode: 1546   score: 0.0   memory length: 100000   epsilon: 0.5876452000089518    steps: 124    lr: 1.6000000000000003e-05     evaluation reward: 2.79\n",
      "episode: 1547   score: 5.0   memory length: 100000   epsilon: 0.5870432800089649    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 2.79\n",
      "episode: 1548   score: 3.0   memory length: 100000   epsilon: 0.5865938200089746    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 2.8\n",
      "episode: 1549   score: 4.0   memory length: 100000   epsilon: 0.5860473400089865    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1550   score: 1.0   memory length: 100000   epsilon: 0.5857107400089938    steps: 170    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1551   score: 2.0   memory length: 100000   epsilon: 0.5853167200090024    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 2.8\n",
      "episode: 1552   score: 1.0   memory length: 100000   epsilon: 0.5850177400090089    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 2.8\n",
      "episode: 1553   score: 6.0   memory length: 100000   epsilon: 0.584273260009025    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 2.85\n",
      "episode: 1554   score: 1.0   memory length: 100000   epsilon: 0.5839742800090315    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1555   score: 2.0   memory length: 100000   epsilon: 0.5836159000090393    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1556   score: 2.0   memory length: 100000   epsilon: 0.5832575200090471    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1557   score: 5.0   memory length: 100000   epsilon: 0.5826556000090601    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1558   score: 3.0   memory length: 100000   epsilon: 0.5822318800090693    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1559   score: 2.0   memory length: 100000   epsilon: 0.5818715200090772    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1560   score: 2.0   memory length: 100000   epsilon: 0.581509180009085    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 2.81\n",
      "episode: 1561   score: 2.0   memory length: 100000   epsilon: 0.5811488200090928    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 2.79\n",
      "episode: 1562   score: 6.0   memory length: 100000   epsilon: 0.5805033400091069    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1563   score: 1.0   memory length: 100000   epsilon: 0.5801647600091142    steps: 171    lr: 1.6000000000000003e-05     evaluation reward: 2.81\n",
      "episode: 1564   score: 4.0   memory length: 100000   epsilon: 0.5796499600091254    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 2.81\n",
      "episode: 1565   score: 3.0   memory length: 100000   epsilon: 0.5791945600091353    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1566   score: 5.0   memory length: 100000   epsilon: 0.5785471000091493    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 2.86\n",
      "episode: 1567   score: 2.0   memory length: 100000   epsilon: 0.5781847600091572    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1568   score: 4.0   memory length: 100000   epsilon: 0.5776382800091691    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1569   score: 7.0   memory length: 100000   epsilon: 0.5768205400091868    steps: 413    lr: 1.6000000000000003e-05     evaluation reward: 2.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1570   score: 1.0   memory length: 100000   epsilon: 0.5764799800091942    steps: 172    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1571   score: 2.0   memory length: 100000   epsilon: 0.5760463600092036    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1572   score: 3.0   memory length: 100000   epsilon: 0.5755553200092143    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1573   score: 2.0   memory length: 100000   epsilon: 0.5751613000092228    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1574   score: 4.0   memory length: 100000   epsilon: 0.5746227400092345    steps: 272    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1575   score: 1.0   memory length: 100000   epsilon: 0.5743217800092411    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 2.78\n",
      "episode: 1576   score: 2.0   memory length: 100000   epsilon: 0.5738842000092506    steps: 221    lr: 1.6000000000000003e-05     evaluation reward: 2.76\n",
      "episode: 1577   score: 1.0   memory length: 100000   epsilon: 0.5735832400092571    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 2.73\n",
      "episode: 1578   score: 4.0   memory length: 100000   epsilon: 0.573034780009269    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 2.74\n",
      "episode: 1579   score: 3.0   memory length: 100000   epsilon: 0.5725873000092787    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 2.75\n",
      "episode: 1580   score: 1.0   memory length: 100000   epsilon: 0.5722467400092861    steps: 172    lr: 1.6000000000000003e-05     evaluation reward: 2.76\n",
      "episode: 1581   score: 5.0   memory length: 100000   epsilon: 0.5716230400092996    steps: 315    lr: 1.6000000000000003e-05     evaluation reward: 2.78\n",
      "episode: 1582   score: 1.0   memory length: 100000   epsilon: 0.5713220800093062    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 2.77\n",
      "episode: 1583   score: 4.0   memory length: 100000   epsilon: 0.5707815400093179    steps: 273    lr: 1.6000000000000003e-05     evaluation reward: 2.77\n",
      "episode: 1584   score: 4.0   memory length: 100000   epsilon: 0.5702667400093291    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 2.79\n",
      "episode: 1585   score: 4.0   memory length: 100000   epsilon: 0.569720260009341    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1586   score: 4.0   memory length: 100000   epsilon: 0.5691995200093523    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1587   score: 4.0   memory length: 100000   epsilon: 0.5686510600093642    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 2.86\n",
      "episode: 1588   score: 1.0   memory length: 100000   epsilon: 0.5683144600093715    steps: 170    lr: 1.6000000000000003e-05     evaluation reward: 2.81\n",
      "episode: 1589   score: 4.0   memory length: 100000   epsilon: 0.5677283800093842    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1590   score: 4.0   memory length: 100000   epsilon: 0.5671819000093961    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 2.86\n",
      "episode: 1591   score: 2.0   memory length: 100000   epsilon: 0.5668215400094039    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 2.84\n",
      "episode: 1592   score: 4.0   memory length: 100000   epsilon: 0.5662770400094157    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1593   score: 2.0   memory length: 100000   epsilon: 0.5659186600094235    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 2.82\n",
      "episode: 1594   score: 3.0   memory length: 100000   epsilon: 0.5653820800094351    steps: 271    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1595   score: 3.0   memory length: 100000   epsilon: 0.5649326200094449    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1596   score: 6.0   memory length: 100000   epsilon: 0.5641861600094611    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 2.86\n",
      "episode: 1597   score: 3.0   memory length: 100000   epsilon: 0.5637367000094708    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 2.85\n",
      "episode: 1598   score: 3.0   memory length: 100000   epsilon: 0.5633189200094799    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 2.85\n",
      "episode: 1599   score: 1.0   memory length: 100000   epsilon: 0.5630179600094865    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 2.83\n",
      "episode: 1600   score: 6.0   memory length: 100000   epsilon: 0.5622893200095023    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 2.88\n",
      "episode: 1601   score: 2.0   memory length: 100000   epsilon: 0.5618953000095108    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 2.89\n",
      "episode: 1602   score: 3.0   memory length: 100000   epsilon: 0.5614319800095209    steps: 234    lr: 1.6000000000000003e-05     evaluation reward: 2.91\n",
      "episode: 1603   score: 3.0   memory length: 100000   epsilon: 0.5609429200095315    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 2.93\n",
      "episode: 1604   score: 3.0   memory length: 100000   epsilon: 0.5604895000095413    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 2.91\n",
      "episode: 1605   score: 2.0   memory length: 100000   epsilon: 0.5600954800095499    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 2.89\n",
      "episode: 1606   score: 1.0   memory length: 100000   epsilon: 0.5597569000095572    steps: 171    lr: 1.6000000000000003e-05     evaluation reward: 2.9\n",
      "episode: 1607   score: 5.0   memory length: 100000   epsilon: 0.5590718200095721    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 2.92\n",
      "episode: 1608   score: 3.0   memory length: 100000   epsilon: 0.5586223600095819    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 2.92\n",
      "episode: 1609   score: 3.0   memory length: 100000   epsilon: 0.5581313200095925    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 2.92\n",
      "episode: 1610   score: 2.0   memory length: 100000   epsilon: 0.557697700009602    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 2.93\n",
      "episode: 1611   score: 5.0   memory length: 100000   epsilon: 0.5570205400096166    steps: 342    lr: 1.6000000000000003e-05     evaluation reward: 2.95\n",
      "episode: 1612   score: 3.0   memory length: 100000   epsilon: 0.5565275200096274    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 2.95\n",
      "episode: 1613   score: 4.0   memory length: 100000   epsilon: 0.5559394600096401    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 2.93\n",
      "episode: 1614   score: 5.0   memory length: 100000   epsilon: 0.5552464600096552    steps: 350    lr: 1.6000000000000003e-05     evaluation reward: 2.96\n",
      "episode: 1615   score: 2.0   memory length: 100000   epsilon: 0.5548544200096637    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 2.94\n",
      "episode: 1616   score: 2.0   memory length: 100000   epsilon: 0.5544623800096722    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 2.94\n",
      "episode: 1617   score: 0.0   memory length: 100000   epsilon: 0.5542168600096775    steps: 124    lr: 1.6000000000000003e-05     evaluation reward: 2.93\n",
      "episode: 1618   score: 3.0   memory length: 100000   epsilon: 0.5538010600096865    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 2.94\n",
      "episode: 1619   score: 5.0   memory length: 100000   epsilon: 0.5531734000097002    steps: 317    lr: 1.6000000000000003e-05     evaluation reward: 2.97\n",
      "episode: 1620   score: 4.0   memory length: 100000   epsilon: 0.5525853400097129    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 2.97\n",
      "episode: 1621   score: 6.0   memory length: 100000   epsilon: 0.5518725400097284    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 3.02\n",
      "episode: 1622   score: 4.0   memory length: 100000   epsilon: 0.5513201200097404    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.05\n",
      "episode: 1623   score: 4.0   memory length: 100000   epsilon: 0.5508073000097515    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1624   score: 4.0   memory length: 100000   epsilon: 0.5502925000097627    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.07\n",
      "episode: 1625   score: 4.0   memory length: 100000   epsilon: 0.5497341400097748    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 3.08\n",
      "episode: 1626   score: 2.0   memory length: 100000   epsilon: 0.5493718000097827    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.07\n",
      "episode: 1627   score: 3.0   memory length: 100000   epsilon: 0.5488827400097933    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.07\n",
      "episode: 1628   score: 6.0   memory length: 100000   epsilon: 0.5481442000098093    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 3.08\n",
      "episode: 1629   score: 3.0   memory length: 100000   epsilon: 0.5476571200098199    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 3.07\n",
      "episode: 1630   score: 3.0   memory length: 100000   epsilon: 0.5471660800098306    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 3.05\n",
      "episode: 1631   score: 2.0   memory length: 100000   epsilon: 0.5468057200098384    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.06\n",
      "episode: 1632   score: 3.0   memory length: 100000   epsilon: 0.5463523000098482    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.05\n",
      "episode: 1633   score: 3.0   memory length: 100000   epsilon: 0.5458969000098581    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.05\n",
      "episode: 1634   score: 3.0   memory length: 100000   epsilon: 0.5454791200098672    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.03\n",
      "episode: 1635   score: 4.0   memory length: 100000   epsilon: 0.5449682800098783    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 3.06\n",
      "episode: 1636   score: 5.0   memory length: 100000   epsilon: 0.5443564600098916    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 3.07\n",
      "episode: 1637   score: 2.0   memory length: 100000   epsilon: 0.5439565000099003    steps: 202    lr: 1.6000000000000003e-05     evaluation reward: 3.04\n",
      "episode: 1638   score: 3.0   memory length: 100000   epsilon: 0.5434654600099109    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 3.06\n",
      "episode: 1639   score: 6.0   memory length: 100000   epsilon: 0.5428140400099251    steps: 329    lr: 1.6000000000000003e-05     evaluation reward: 3.12\n",
      "episode: 1640   score: 5.0   memory length: 100000   epsilon: 0.5422378600099376    steps: 291    lr: 1.6000000000000003e-05     evaluation reward: 3.14\n",
      "episode: 1641   score: 4.0   memory length: 100000   epsilon: 0.5416953400099493    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 3.15\n",
      "episode: 1642   score: 3.0   memory length: 100000   epsilon: 0.5412775600099584    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.15\n",
      "episode: 1643   score: 4.0   memory length: 100000   epsilon: 0.5407231600099704    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 3.18\n",
      "episode: 1644   score: 3.0   memory length: 100000   epsilon: 0.5403053800099795    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.17\n",
      "episode: 1645   score: 5.0   memory length: 100000   epsilon: 0.5396559400099936    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 3.19\n",
      "episode: 1646   score: 5.0   memory length: 100000   epsilon: 0.5390065000100077    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 3.24\n",
      "episode: 1647   score: 3.0   memory length: 100000   epsilon: 0.5385194200100183    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 3.22\n",
      "episode: 1648   score: 2.0   memory length: 100000   epsilon: 0.5381254000100268    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.21\n",
      "episode: 1649   score: 4.0   memory length: 100000   epsilon: 0.5375769400100388    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.21\n",
      "episode: 1650   score: 1.0   memory length: 100000   epsilon: 0.5372779600100452    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.21\n",
      "episode: 1651   score: 8.0   memory length: 100000   epsilon: 0.5363988400100643    steps: 444    lr: 1.6000000000000003e-05     evaluation reward: 3.27\n",
      "episode: 1652   score: 2.0   memory length: 100000   epsilon: 0.5360068000100728    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.28\n",
      "episode: 1653   score: 2.0   memory length: 100000   epsilon: 0.5355731800100823    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 3.24\n",
      "episode: 1654   score: 6.0   memory length: 100000   epsilon: 0.5348504800100979    steps: 365    lr: 1.6000000000000003e-05     evaluation reward: 3.29\n",
      "episode: 1655   score: 3.0   memory length: 100000   epsilon: 0.5343911200101079    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1656   score: 4.0   memory length: 100000   epsilon: 0.5338446400101198    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1657   score: 2.0   memory length: 100000   epsilon: 0.5334823000101276    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.29\n",
      "episode: 1658   score: 4.0   memory length: 100000   epsilon: 0.5329358200101395    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1659   score: 3.0   memory length: 100000   epsilon: 0.5324863600101493    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1660   score: 3.0   memory length: 100000   epsilon: 0.5320309600101591    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1661   score: 3.0   memory length: 100000   epsilon: 0.5316072400101683    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1662   score: 3.0   memory length: 100000   epsilon: 0.5311201600101789    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1663   score: 6.0   memory length: 100000   epsilon: 0.530425180010194    steps: 351    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1664   score: 3.0   memory length: 100000   epsilon: 0.5299757200102038    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1665   score: 5.0   memory length: 100000   epsilon: 0.5293837000102166    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 3.36\n",
      "episode: 1666   score: 4.0   memory length: 100000   epsilon: 0.5288035600102292    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1667   score: 4.0   memory length: 100000   epsilon: 0.5282570800102411    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1668   score: 5.0   memory length: 100000   epsilon: 0.5276670400102539    steps: 298    lr: 1.6000000000000003e-05     evaluation reward: 3.38\n",
      "episode: 1669   score: 3.0   memory length: 100000   epsilon: 0.5272096600102638    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1670   score: 2.0   memory length: 100000   epsilon: 0.5268116800102725    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1671   score: 1.0   memory length: 100000   epsilon: 0.5264731000102798    steps: 171    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1672   score: 4.0   memory length: 100000   epsilon: 0.5259602800102909    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1673   score: 2.0   memory length: 100000   epsilon: 0.5255999200102988    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1674   score: 4.0   memory length: 100000   epsilon: 0.5250039400103117    steps: 301    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1675   score: 3.0   memory length: 100000   epsilon: 0.5245445800103217    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1676   score: 11.0   memory length: 100000   epsilon: 0.5235684400103429    steps: 493    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1677   score: 3.0   memory length: 100000   epsilon: 0.5231150200103527    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1678   score: 2.0   memory length: 100000   epsilon: 0.5227526800103606    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1679   score: 1.0   memory length: 100000   epsilon: 0.5224517200103671    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.44\n",
      "episode: 1680   score: 3.0   memory length: 100000   epsilon: 0.5220280000103763    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1681   score: 3.0   memory length: 100000   epsilon: 0.5215785400103861    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.44\n",
      "episode: 1682   score: 3.0   memory length: 100000   epsilon: 0.521123140010396    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1683   score: 2.0   memory length: 100000   epsilon: 0.5207192200104047    steps: 204    lr: 1.6000000000000003e-05     evaluation reward: 3.44\n",
      "episode: 1684   score: 2.0   memory length: 100000   epsilon: 0.5203568800104126    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.42\n",
      "episode: 1685   score: 2.0   memory length: 100000   epsilon: 0.5199628600104211    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.4\n",
      "episode: 1686   score: 3.0   memory length: 100000   epsilon: 0.5195054800104311    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.39\n",
      "episode: 1687   score: 3.0   memory length: 100000   epsilon: 0.5190520600104409    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.38\n",
      "episode: 1688   score: 4.0   memory length: 100000   epsilon: 0.518539240010452    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.41\n",
      "episode: 1689   score: 2.0   memory length: 100000   epsilon: 0.5181472000104606    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.39\n",
      "episode: 1690   score: 4.0   memory length: 100000   epsilon: 0.5175967600104725    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.39\n",
      "episode: 1691   score: 3.0   memory length: 100000   epsilon: 0.5170958200104834    steps: 253    lr: 1.6000000000000003e-05     evaluation reward: 3.4\n",
      "episode: 1692   score: 3.0   memory length: 100000   epsilon: 0.5166780400104924    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.39\n",
      "episode: 1693   score: 6.0   memory length: 100000   epsilon: 0.5159672200105079    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 3.43\n",
      "episode: 1694   score: 4.0   memory length: 100000   epsilon: 0.5154504400105191    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.44\n",
      "episode: 1695   score: 4.0   memory length: 100000   epsilon: 0.5149376200105302    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.45\n",
      "episode: 1696   score: 2.0   memory length: 100000   epsilon: 0.5145752800105381    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.41\n",
      "episode: 1697   score: 6.0   memory length: 100000   epsilon: 0.5138506000105538    steps: 366    lr: 1.6000000000000003e-05     evaluation reward: 3.44\n",
      "episode: 1698   score: 3.0   memory length: 100000   epsilon: 0.5133932200105638    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.44\n",
      "episode: 1699   score: 4.0   memory length: 100000   epsilon: 0.5128368400105758    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 3.47\n",
      "episode: 1700   score: 5.0   memory length: 100000   epsilon: 0.5121874000105899    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1701   score: 4.0   memory length: 100000   epsilon: 0.511676560010601    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1702   score: 1.0   memory length: 100000   epsilon: 0.5113756000106076    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1703   score: 5.0   memory length: 100000   epsilon: 0.5107816000106205    steps: 300    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1704   score: 5.0   memory length: 100000   epsilon: 0.5101975000106331    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1705   score: 2.0   memory length: 100000   epsilon: 0.509835160010641    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1706   score: 4.0   memory length: 100000   epsilon: 0.5092787800106531    steps: 281    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1707   score: 3.0   memory length: 100000   epsilon: 0.5087481400106646    steps: 268    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1708   score: 5.0   memory length: 100000   epsilon: 0.5080947400106788    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1709   score: 3.0   memory length: 100000   epsilon: 0.5076037000106894    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1710   score: 3.0   memory length: 100000   epsilon: 0.5071542400106992    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1711   score: 3.0   memory length: 100000   epsilon: 0.5066988400107091    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1712   score: 5.0   memory length: 100000   epsilon: 0.5060692000107228    steps: 318    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1713   score: 8.0   memory length: 100000   epsilon: 0.5052039400107415    steps: 437    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1714   score: 7.0   memory length: 100000   epsilon: 0.5044555000107578    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1715   score: 2.0   memory length: 100000   epsilon: 0.5040218800107672    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1716   score: 6.0   memory length: 100000   epsilon: 0.5033526400107817    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1717   score: 1.0   memory length: 100000   epsilon: 0.5030516800107883    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1718   score: 5.0   memory length: 100000   epsilon: 0.5024418400108015    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1719   score: 3.0   memory length: 100000   epsilon: 0.5019864400108114    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1720   score: 3.0   memory length: 100000   epsilon: 0.5015666800108205    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1721   score: 2.0   memory length: 100000   epsilon: 0.5012043400108284    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1722   score: 4.0   memory length: 100000   epsilon: 0.5006558800108403    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1723   score: 3.0   memory length: 100000   epsilon: 0.500160880010851    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1724   score: 5.0   memory length: 100000   epsilon: 0.49951540001085143    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1725   score: 3.0   memory length: 100000   epsilon: 0.4990560400108485    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1726   score: 3.0   memory length: 100000   epsilon: 0.49860262001084565    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1727   score: 5.0   memory length: 100000   epsilon: 0.4979967400108418    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1728   score: 5.0   memory length: 100000   epsilon: 0.49733740001083765    steps: 333    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1729   score: 3.0   memory length: 100000   epsilon: 0.49685032001083457    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1730   score: 4.0   memory length: 100000   epsilon: 0.4962979000108311    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1731   score: 3.0   memory length: 100000   epsilon: 0.4958425000108282    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1732   score: 4.0   memory length: 100000   epsilon: 0.49532770001082493    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1733   score: 4.0   memory length: 100000   epsilon: 0.4947733000108214    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1734   score: 2.0   memory length: 100000   epsilon: 0.4943773000108189    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1735   score: 5.0   memory length: 100000   epsilon: 0.4937912200108152    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1736   score: 3.0   memory length: 100000   epsilon: 0.49333582001081233    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.63\n",
      "episode: 1737   score: 3.0   memory length: 100000   epsilon: 0.49288240001080946    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1738   score: 4.0   memory length: 100000   epsilon: 0.49229632001080575    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1739   score: 2.0   memory length: 100000   epsilon: 0.49190230001080326    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1740   score: 4.0   memory length: 100000   epsilon: 0.4913538400107998    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1741   score: 4.0   memory length: 100000   epsilon: 0.4907737000107961    steps: 293    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1742   score: 3.0   memory length: 100000   epsilon: 0.4903242400107933    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1743   score: 6.0   memory length: 100000   epsilon: 0.48960946001078876    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1744   score: 3.0   memory length: 100000   epsilon: 0.4891857400107861    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1745   score: 3.0   memory length: 100000   epsilon: 0.48875806001078337    steps: 216    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1746   score: 3.0   memory length: 100000   epsilon: 0.4883343400107807    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1747   score: 1.0   memory length: 100000   epsilon: 0.48799774001077856    steps: 170    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1748   score: 3.0   memory length: 100000   epsilon: 0.4875463000107757    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1749   score: 5.0   memory length: 100000   epsilon: 0.4869265600107718    steps: 313    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1750   score: 3.0   memory length: 100000   epsilon: 0.4864256200107686    steps: 253    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1751   score: 2.0   memory length: 100000   epsilon: 0.4860276400107661    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1752   score: 4.0   memory length: 100000   epsilon: 0.48551482001076285    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1753   score: 3.0   memory length: 100000   epsilon: 0.48502576001075975    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1754   score: 1.0   memory length: 100000   epsilon: 0.48472480001075785    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1755   score: 4.0   memory length: 100000   epsilon: 0.48417040001075434    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1756   score: 2.0   memory length: 100000   epsilon: 0.48377836001075186    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1757   score: 3.0   memory length: 100000   epsilon: 0.483322960010749    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1758   score: 1.0   memory length: 100000   epsilon: 0.4830239800107471    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1759   score: 3.0   memory length: 100000   epsilon: 0.4826002600107444    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1760   score: 3.0   memory length: 100000   epsilon: 0.4821112000107413    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1761   score: 3.0   memory length: 100000   epsilon: 0.4816181800107382    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1762   score: 2.0   memory length: 100000   epsilon: 0.4812241600107357    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1763   score: 4.0   memory length: 100000   epsilon: 0.480638080010732    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1764   score: 3.0   memory length: 100000   epsilon: 0.48014308001072886    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1765   score: 3.0   memory length: 100000   epsilon: 0.47968372001072596    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 3.44\n",
      "episode: 1766   score: 2.0   memory length: 100000   epsilon: 0.47932138001072366    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.42\n",
      "episode: 1767   score: 2.0   memory length: 100000   epsilon: 0.4789610200107214    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.4\n",
      "episode: 1768   score: 3.0   memory length: 100000   epsilon: 0.4785076000107185    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.38\n",
      "episode: 1769   score: 4.0   memory length: 100000   epsilon: 0.47792548001071483    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 3.39\n",
      "episode: 1770   score: 3.0   memory length: 100000   epsilon: 0.47750176001071215    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.4\n",
      "episode: 1771   score: 1.0   memory length: 100000   epsilon: 0.47720080001071025    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.4\n",
      "episode: 1772   score: 5.0   memory length: 100000   epsilon: 0.47661472001070654    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.41\n",
      "episode: 1773   score: 2.0   memory length: 100000   epsilon: 0.47622070001070405    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.41\n",
      "episode: 1774   score: 3.0   memory length: 100000   epsilon: 0.47576530001070116    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.4\n",
      "episode: 1775   score: 3.0   memory length: 100000   epsilon: 0.47530594001069826    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 3.4\n",
      "episode: 1776   score: 3.0   memory length: 100000   epsilon: 0.4748564800106954    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1777   score: 3.0   memory length: 100000   epsilon: 0.47443672001069276    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1778   score: 2.0   memory length: 100000   epsilon: 0.4740446800106903    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1779   score: 6.0   memory length: 100000   epsilon: 0.4733061400106856    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1780   score: 2.0   memory length: 100000   epsilon: 0.4729438000106833    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.36\n",
      "episode: 1781   score: 4.0   memory length: 100000   epsilon: 0.47236168001067963    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1782   score: 3.0   memory length: 100000   epsilon: 0.47187658001067656    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1783   score: 5.0   memory length: 100000   epsilon: 0.4712548600106726    steps: 314    lr: 1.6000000000000003e-05     evaluation reward: 3.4\n",
      "episode: 1784   score: 5.0   memory length: 100000   epsilon: 0.47056186001066824    steps: 350    lr: 1.6000000000000003e-05     evaluation reward: 3.43\n",
      "episode: 1785   score: 1.0   memory length: 100000   epsilon: 0.47026288001066635    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1786   score: 4.0   memory length: 100000   epsilon: 0.46967878001066266    steps: 295    lr: 1.6000000000000003e-05     evaluation reward: 3.43\n",
      "episode: 1787   score: 5.0   memory length: 100000   epsilon: 0.4689877600106583    steps: 349    lr: 1.6000000000000003e-05     evaluation reward: 3.45\n",
      "episode: 1788   score: 5.0   memory length: 100000   epsilon: 0.4683403000106542    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1789   score: 4.0   memory length: 100000   epsilon: 0.46779382001065073    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1790   score: 9.0   memory length: 100000   epsilon: 0.4671087400106464    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1791   score: 6.0   memory length: 100000   epsilon: 0.46638802001064184    steps: 364    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1792   score: 3.0   memory length: 100000   epsilon: 0.46589896001063874    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1793   score: 5.0   memory length: 100000   epsilon: 0.4652138800106344    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1794   score: 4.0   memory length: 100000   epsilon: 0.4646258200106307    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1795   score: 8.0   memory length: 100000   epsilon: 0.4639922200106267    steps: 320    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1796   score: 3.0   memory length: 100000   epsilon: 0.4635348400106238    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1797   score: 7.0   memory length: 100000   epsilon: 0.4627289800106187    steps: 407    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1798   score: 4.0   memory length: 100000   epsilon: 0.4622557600106157    steps: 239    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1799   score: 4.0   memory length: 100000   epsilon: 0.46169542001061215    steps: 283    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1800   score: 4.0   memory length: 100000   epsilon: 0.4611509200106087    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1801   score: 4.0   memory length: 100000   epsilon: 0.460562860010605    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1802   score: 6.0   memory length: 100000   epsilon: 0.4598896600106007    steps: 340    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1803   score: 4.0   memory length: 100000   epsilon: 0.45933922001059724    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1804   score: 6.0   memory length: 100000   epsilon: 0.45868978001059313    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1805   score: 4.0   memory length: 100000   epsilon: 0.45813736001058963    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.68\n",
      "episode: 1806   score: 4.0   memory length: 100000   epsilon: 0.45758890001058616    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.68\n",
      "episode: 1807   score: 4.0   memory length: 100000   epsilon: 0.4570424200105827    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1808   score: 5.0   memory length: 100000   epsilon: 0.45640288001057866    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1809   score: 7.0   memory length: 100000   epsilon: 0.4555732600105734    steps: 419    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1810   score: 4.0   memory length: 100000   epsilon: 0.45499114001056973    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1811   score: 2.0   memory length: 100000   epsilon: 0.45462880001056744    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1812   score: 5.0   memory length: 100000   epsilon: 0.45398530001056336    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1813   score: 3.0   memory length: 100000   epsilon: 0.45349030001056023    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 3.68\n",
      "episode: 1814   score: 5.0   memory length: 100000   epsilon: 0.4528725400105563    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1815   score: 3.0   memory length: 100000   epsilon: 0.4524547600105537    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1816   score: 2.0   memory length: 100000   epsilon: 0.4520963800105514    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.63\n",
      "episode: 1817   score: 4.0   memory length: 100000   epsilon: 0.45158158001054816    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1818   score: 5.0   memory length: 100000   epsilon: 0.45097768001054434    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1819   score: 8.0   memory length: 100000   epsilon: 0.4501936000105394    steps: 396    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1820   score: 4.0   memory length: 100000   epsilon: 0.4496411800105359    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.72\n",
      "episode: 1821   score: 4.0   memory length: 100000   epsilon: 0.44908678001053237    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1822   score: 3.0   memory length: 100000   epsilon: 0.4485977200105293    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1823   score: 5.0   memory length: 100000   epsilon: 0.44799184001052544    steps: 306    lr: 1.6000000000000003e-05     evaluation reward: 3.75\n",
      "episode: 1824   score: 4.0   memory length: 100000   epsilon: 0.4473978400105217    steps: 300    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1825   score: 5.0   memory length: 100000   epsilon: 0.446815720010518    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 3.76\n",
      "episode: 1826   score: 4.0   memory length: 100000   epsilon: 0.44626726001051453    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n",
      "episode: 1827   score: 6.0   memory length: 100000   epsilon: 0.4456019800105103    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 3.78\n",
      "episode: 1828   score: 5.0   memory length: 100000   epsilon: 0.44494066001050614    steps: 334    lr: 1.6000000000000003e-05     evaluation reward: 3.78\n",
      "episode: 1829   score: 5.0   memory length: 100000   epsilon: 0.44429320001050204    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1830   score: 3.0   memory length: 100000   epsilon: 0.4438754200104994    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.79\n",
      "episode: 1831   score: 7.0   memory length: 100000   epsilon: 0.4431923200104951    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1832   score: 3.0   memory length: 100000   epsilon: 0.44274286001049223    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1833   score: 5.0   memory length: 100000   epsilon: 0.4421567800104885    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1834   score: 4.0   memory length: 100000   epsilon: 0.4416439600104853    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1835   score: 4.0   memory length: 100000   epsilon: 0.4410895600104818    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1836   score: 4.0   memory length: 100000   epsilon: 0.4405391200104783    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1837   score: 4.0   memory length: 100000   epsilon: 0.4399886800104748    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1838   score: 3.0   memory length: 100000   epsilon: 0.4394976400104717    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1839   score: 3.0   memory length: 100000   epsilon: 0.4390085800104686    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1840   score: 3.0   memory length: 100000   epsilon: 0.43859080001046596    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1841   score: 6.0   memory length: 100000   epsilon: 0.43790968001046165    steps: 344    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1842   score: 4.0   memory length: 100000   epsilon: 0.4373968600104584    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1843   score: 3.0   memory length: 100000   epsilon: 0.43697314001045573    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1844   score: 3.0   memory length: 100000   epsilon: 0.4365217000104529    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1845   score: 3.0   memory length: 100000   epsilon: 0.43606630001045    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1846   score: 1.0   memory length: 100000   epsilon: 0.4357653400104481    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1847   score: 3.0   memory length: 100000   epsilon: 0.4353099400104452    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1848   score: 4.0   memory length: 100000   epsilon: 0.4347575200104417    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1849   score: 5.0   memory length: 100000   epsilon: 0.43414966001043787    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1850   score: 5.0   memory length: 100000   epsilon: 0.433535860010434    steps: 310    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1851   score: 3.0   memory length: 100000   epsilon: 0.4331121400104313    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1852   score: 3.0   memory length: 100000   epsilon: 0.43265872001042843    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1853   score: 4.0   memory length: 100000   epsilon: 0.4321459000104252    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1854   score: 7.0   memory length: 100000   epsilon: 0.4313440000104201    steps: 405    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1855   score: 2.0   memory length: 100000   epsilon: 0.43098364001041783    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.93\n",
      "episode: 1856   score: 2.0   memory length: 100000   epsilon: 0.43062130001041554    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.93\n",
      "episode: 1857   score: 4.0   memory length: 100000   epsilon: 0.4301045200104123    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.94\n",
      "episode: 1858   score: 2.0   memory length: 100000   epsilon: 0.42970654001040975    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1859   score: 2.0   memory length: 100000   epsilon: 0.4293481600104075    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.94\n",
      "episode: 1860   score: 2.0   memory length: 100000   epsilon: 0.4289858200104052    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 3.93\n",
      "episode: 1861   score: 1.0   memory length: 100000   epsilon: 0.4286848600104033    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1862   score: 6.0   memory length: 100000   epsilon: 0.42799780001039894    steps: 347    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1863   score: 3.0   memory length: 100000   epsilon: 0.42757408001039626    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.94\n",
      "episode: 1864   score: 2.0   memory length: 100000   epsilon: 0.42718006001039377    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.93\n",
      "episode: 1865   score: 5.0   memory length: 100000   epsilon: 0.4265702200103899    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1866   score: 3.0   memory length: 100000   epsilon: 0.42611482001038703    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.96\n",
      "episode: 1867   score: 5.0   memory length: 100000   epsilon: 0.42550498001038317    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1868   score: 4.0   memory length: 100000   epsilon: 0.4249882000103799    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 4.0\n",
      "episode: 1869   score: 3.0   memory length: 100000   epsilon: 0.42456646001037723    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1870   score: 3.0   memory length: 100000   epsilon: 0.42410908001037434    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1871   score: 2.0   memory length: 100000   epsilon: 0.4237111000103718    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 4.0\n",
      "episode: 1872   score: 3.0   memory length: 100000   epsilon: 0.42325372001036893    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1873   score: 13.0   memory length: 100000   epsilon: 0.42214888001036194    steps: 558    lr: 1.6000000000000003e-05     evaluation reward: 4.09\n",
      "episode: 1874   score: 3.0   memory length: 100000   epsilon: 0.4217291200103593    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 4.09\n",
      "episode: 1875   score: 7.0   memory length: 100000   epsilon: 0.4208777200103539    steps: 430    lr: 1.6000000000000003e-05     evaluation reward: 4.13\n",
      "episode: 1876   score: 6.0   memory length: 100000   epsilon: 0.4201669000103494    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1877   score: 1.0   memory length: 100000   epsilon: 0.41983030001034727    steps: 170    lr: 1.6000000000000003e-05     evaluation reward: 4.14\n",
      "episode: 1878   score: 4.0   memory length: 100000   epsilon: 0.41935312001034425    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 4.16\n",
      "episode: 1879   score: 3.0   memory length: 100000   epsilon: 0.4188997000103414    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.13\n",
      "episode: 1880   score: 2.0   memory length: 100000   epsilon: 0.4185373600103391    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 4.13\n",
      "episode: 1881   score: 3.0   memory length: 100000   epsilon: 0.41811760001033643    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 4.12\n",
      "episode: 1882   score: 3.0   memory length: 100000   epsilon: 0.4176661600103336    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.12\n",
      "episode: 1883   score: 1.0   memory length: 100000   epsilon: 0.4173671800103317    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n",
      "episode: 1884   score: 4.0   memory length: 100000   epsilon: 0.4168167400103282    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1885   score: 3.0   memory length: 100000   epsilon: 0.41636332001032533    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.09\n",
      "episode: 1886   score: 7.0   memory length: 100000   epsilon: 0.4155950800103205    steps: 388    lr: 1.6000000000000003e-05     evaluation reward: 4.12\n",
      "episode: 1887   score: 3.0   memory length: 100000   epsilon: 0.41514562001031763    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 4.1\n",
      "episode: 1888   score: 3.0   memory length: 100000   epsilon: 0.4146941800103148    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n",
      "episode: 1889   score: 4.0   memory length: 100000   epsilon: 0.41418334001031154    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n",
      "episode: 1890   score: 7.0   memory length: 100000   epsilon: 0.4134210400103067    steps: 385    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1891   score: 7.0   memory length: 100000   epsilon: 0.41263696001030176    steps: 396    lr: 1.6000000000000003e-05     evaluation reward: 4.07\n",
      "episode: 1892   score: 4.0   memory length: 100000   epsilon: 0.4120786000102982    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n",
      "episode: 1893   score: 5.0   memory length: 100000   epsilon: 0.41151034001029463    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1894   score: 4.0   memory length: 100000   epsilon: 0.41095792001029113    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 4.08\n",
      "episode: 1895   score: 6.0   memory length: 100000   epsilon: 0.4102867000102869    steps: 339    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1896   score: 3.0   memory length: 100000   epsilon: 0.409833280010284    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1897   score: 4.0   memory length: 100000   epsilon: 0.409356100010281    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 4.03\n",
      "episode: 1898   score: 6.0   memory length: 100000   epsilon: 0.40869874001027684    steps: 332    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1899   score: 5.0   memory length: 100000   epsilon: 0.40812850001027323    steps: 288    lr: 1.6000000000000003e-05     evaluation reward: 4.06\n",
      "episode: 1900   score: 2.0   memory length: 100000   epsilon: 0.40776616001027094    steps: 183    lr: 1.6000000000000003e-05     evaluation reward: 4.04\n",
      "episode: 1901   score: 5.0   memory length: 100000   epsilon: 0.4071880000102673    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 4.05\n",
      "episode: 1902   score: 2.0   memory length: 100000   epsilon: 0.40675438001026454    steps: 219    lr: 1.6000000000000003e-05     evaluation reward: 4.01\n",
      "episode: 1903   score: 2.0   memory length: 100000   epsilon: 0.40639600001026227    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1904   score: 3.0   memory length: 100000   epsilon: 0.4059406000102594    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 3.96\n",
      "episode: 1905   score: 4.0   memory length: 100000   epsilon: 0.40542580001025613    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 3.96\n",
      "episode: 1906   score: 7.0   memory length: 100000   epsilon: 0.4046635000102513    steps: 385    lr: 6.400000000000001e-06     evaluation reward: 3.99\n",
      "episode: 1907   score: 4.0   memory length: 100000   epsilon: 0.40411504001024784    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 3.99\n",
      "episode: 1908   score: 3.0   memory length: 100000   epsilon: 0.4036180600102447    steps: 251    lr: 6.400000000000001e-06     evaluation reward: 3.97\n",
      "episode: 1909   score: 3.0   memory length: 100000   epsilon: 0.40320226001024206    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 3.93\n",
      "episode: 1910   score: 2.0   memory length: 100000   epsilon: 0.40280428001023955    steps: 201    lr: 6.400000000000001e-06     evaluation reward: 3.91\n",
      "episode: 1911   score: 3.0   memory length: 100000   epsilon: 0.4023548200102367    steps: 227    lr: 6.400000000000001e-06     evaluation reward: 3.92\n",
      "episode: 1912   score: 3.0   memory length: 100000   epsilon: 0.401931100010234    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 3.9\n",
      "episode: 1913   score: 4.0   memory length: 100000   epsilon: 0.4013885800102306    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 3.91\n",
      "episode: 1914   score: 4.0   memory length: 100000   epsilon: 0.40090744001022754    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 3.9\n",
      "episode: 1915   score: 8.0   memory length: 100000   epsilon: 0.4000798000102223    steps: 418    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1916   score: 4.0   memory length: 100000   epsilon: 0.39956500001021905    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 3.97\n",
      "episode: 1917   score: 4.0   memory length: 100000   epsilon: 0.3990205000102156    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 3.97\n",
      "episode: 1918   score: 5.0   memory length: 100000   epsilon: 0.3983671000102115    steps: 330    lr: 6.400000000000001e-06     evaluation reward: 3.97\n",
      "episode: 1919   score: 3.0   memory length: 100000   epsilon: 0.3979097200102086    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 3.92\n",
      "episode: 1920   score: 5.0   memory length: 100000   epsilon: 0.3972998800102047    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 3.93\n",
      "episode: 1921   score: 4.0   memory length: 100000   epsilon: 0.39678508001020146    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 3.93\n",
      "episode: 1922   score: 5.0   memory length: 100000   epsilon: 0.39610000001019713    steps: 346    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1923   score: 4.0   memory length: 100000   epsilon: 0.39555154001019366    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 3.94\n",
      "episode: 1924   score: 3.0   memory length: 100000   epsilon: 0.395129800010191    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 3.93\n",
      "episode: 1925   score: 4.0   memory length: 100000   epsilon: 0.39461500001018773    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 3.92\n",
      "episode: 1926   score: 3.0   memory length: 100000   epsilon: 0.39408040001018435    steps: 270    lr: 6.400000000000001e-06     evaluation reward: 3.91\n",
      "episode: 1927   score: 3.0   memory length: 100000   epsilon: 0.39365668001018167    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 3.88\n",
      "episode: 1928   score: 2.0   memory length: 100000   epsilon: 0.3932230600101789    steps: 219    lr: 6.400000000000001e-06     evaluation reward: 3.85\n",
      "episode: 1929   score: 4.0   memory length: 100000   epsilon: 0.39270826001017567    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 3.84\n",
      "episode: 1930   score: 4.0   memory length: 100000   epsilon: 0.3921914800101724    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 3.85\n",
      "episode: 1931   score: 2.0   memory length: 100000   epsilon: 0.39183310001017013    steps: 181    lr: 6.400000000000001e-06     evaluation reward: 3.8\n",
      "episode: 1932   score: 2.0   memory length: 100000   epsilon: 0.39147076001016784    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 3.79\n",
      "episode: 1933   score: 6.0   memory length: 100000   epsilon: 0.3907817200101635    steps: 348    lr: 6.400000000000001e-06     evaluation reward: 3.8\n",
      "episode: 1934   score: 6.0   memory length: 100000   epsilon: 0.3901402000101594    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 3.82\n",
      "episode: 1935   score: 5.0   memory length: 100000   epsilon: 0.3895006600101554    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 3.83\n",
      "episode: 1936   score: 2.0   memory length: 100000   epsilon: 0.3891422800101531    steps: 181    lr: 6.400000000000001e-06     evaluation reward: 3.81\n",
      "episode: 1937   score: 4.0   memory length: 100000   epsilon: 0.38859580001014965    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 3.81\n",
      "episode: 1938   score: 5.0   memory length: 100000   epsilon: 0.388019620010146    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 3.83\n",
      "episode: 1939   score: 5.0   memory length: 100000   epsilon: 0.38744344001014236    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 3.85\n",
      "episode: 1940   score: 3.0   memory length: 100000   epsilon: 0.38695240001013925    steps: 248    lr: 6.400000000000001e-06     evaluation reward: 3.85\n",
      "episode: 1941   score: 4.0   memory length: 100000   epsilon: 0.38640196001013577    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 3.83\n",
      "episode: 1942   score: 3.0   memory length: 100000   epsilon: 0.38594260001013286    steps: 232    lr: 6.400000000000001e-06     evaluation reward: 3.82\n",
      "episode: 1943   score: 3.0   memory length: 100000   epsilon: 0.38551492001013016    steps: 216    lr: 6.400000000000001e-06     evaluation reward: 3.82\n",
      "episode: 1944   score: 5.0   memory length: 100000   epsilon: 0.3849347800101265    steps: 293    lr: 6.400000000000001e-06     evaluation reward: 3.84\n",
      "episode: 1945   score: 7.0   memory length: 100000   epsilon: 0.3841586200101216    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 3.88\n",
      "episode: 1946   score: 9.0   memory length: 100000   epsilon: 0.3833092000101162    steps: 429    lr: 6.400000000000001e-06     evaluation reward: 3.96\n",
      "episode: 1947   score: 4.0   memory length: 100000   epsilon: 0.38282410001011313    steps: 245    lr: 6.400000000000001e-06     evaluation reward: 3.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1948   score: 3.0   memory length: 100000   epsilon: 0.3824043400101105    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 3.96\n",
      "episode: 1949   score: 3.0   memory length: 100000   epsilon: 0.3819509200101076    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 3.94\n",
      "episode: 1950   score: 4.0   memory length: 100000   epsilon: 0.38135692001010385    steps: 300    lr: 6.400000000000001e-06     evaluation reward: 3.93\n",
      "episode: 1951   score: 6.0   memory length: 100000   epsilon: 0.38069362001009965    steps: 335    lr: 6.400000000000001e-06     evaluation reward: 3.96\n",
      "episode: 1952   score: 3.0   memory length: 100000   epsilon: 0.3802421800100968    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 3.96\n",
      "episode: 1953   score: 6.0   memory length: 100000   epsilon: 0.37958680001009265    steps: 331    lr: 6.400000000000001e-06     evaluation reward: 3.98\n",
      "episode: 1954   score: 2.0   memory length: 100000   epsilon: 0.3792284200100904    steps: 181    lr: 6.400000000000001e-06     evaluation reward: 3.93\n",
      "episode: 1955   score: 4.0   memory length: 100000   epsilon: 0.3786779800100869    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1956   score: 2.0   memory length: 100000   epsilon: 0.3782800000100844    steps: 201    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1957   score: 3.0   memory length: 100000   epsilon: 0.3778206400100815    steps: 232    lr: 6.400000000000001e-06     evaluation reward: 3.94\n",
      "episode: 1958   score: 5.0   memory length: 100000   epsilon: 0.37725040001007787    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 3.97\n",
      "episode: 1959   score: 5.0   memory length: 100000   epsilon: 0.376636600010074    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 4.0\n",
      "episode: 1960   score: 2.0   memory length: 100000   epsilon: 0.3762782200100717    steps: 181    lr: 6.400000000000001e-06     evaluation reward: 4.0\n",
      "episode: 1961   score: 2.0   memory length: 100000   epsilon: 0.3759158800100694    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 4.01\n",
      "episode: 1962   score: 1.0   memory length: 100000   epsilon: 0.37561690001006753    steps: 151    lr: 6.400000000000001e-06     evaluation reward: 3.96\n",
      "episode: 1963   score: 6.0   memory length: 100000   epsilon: 0.37490806001006305    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 3.99\n",
      "episode: 1964   score: 4.0   memory length: 100000   epsilon: 0.3743140600100593    steps: 300    lr: 6.400000000000001e-06     evaluation reward: 4.01\n",
      "episode: 1965   score: 2.0   memory length: 100000   epsilon: 0.373951720010057    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 3.98\n",
      "episode: 1966   score: 4.0   memory length: 100000   epsilon: 0.3734012800100535    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 3.99\n",
      "episode: 1967   score: 6.0   memory length: 100000   epsilon: 0.3727320400100493    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 4.0\n",
      "episode: 1968   score: 2.0   memory length: 100000   epsilon: 0.372369700010047    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 3.98\n",
      "episode: 1969   score: 9.0   memory length: 100000   epsilon: 0.3714846400100414    steps: 447    lr: 6.400000000000001e-06     evaluation reward: 4.04\n",
      "episode: 1970   score: 5.0   memory length: 100000   epsilon: 0.3708569800100374    steps: 317    lr: 6.400000000000001e-06     evaluation reward: 4.06\n",
      "episode: 1971   score: 4.0   memory length: 100000   epsilon: 0.370312480010034    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 4.08\n",
      "episode: 1972   score: 3.0   memory length: 100000   epsilon: 0.3698927200100313    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 4.08\n",
      "episode: 1973   score: 3.0   memory length: 100000   epsilon: 0.3694749400100287    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 3.98\n",
      "episode: 1974   score: 6.0   memory length: 100000   epsilon: 0.3688354000100246    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 4.01\n",
      "episode: 1975   score: 5.0   memory length: 100000   epsilon: 0.3681483400100203    steps: 347    lr: 6.400000000000001e-06     evaluation reward: 3.99\n",
      "episode: 1976   score: 2.0   memory length: 100000   epsilon: 0.36775036001001776    steps: 201    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1977   score: 5.0   memory length: 100000   epsilon: 0.36713458001001387    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 3.99\n",
      "episode: 1978   score: 3.0   memory length: 100000   epsilon: 0.366685120010011    steps: 227    lr: 6.400000000000001e-06     evaluation reward: 3.98\n",
      "episode: 1979   score: 3.0   memory length: 100000   epsilon: 0.36626536001000837    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 3.98\n",
      "episode: 1980   score: 5.0   memory length: 100000   epsilon: 0.36566146001000455    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 4.01\n",
      "episode: 1981   score: 1.0   memory length: 100000   epsilon: 0.36536050001000264    steps: 152    lr: 6.400000000000001e-06     evaluation reward: 3.99\n",
      "episode: 1982   score: 4.0   memory length: 100000   epsilon: 0.3648140200099992    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 4.0\n",
      "episode: 1983   score: 8.0   memory length: 100000   epsilon: 0.3640457800099943    steps: 388    lr: 6.400000000000001e-06     evaluation reward: 4.07\n",
      "episode: 1984   score: 4.0   memory length: 100000   epsilon: 0.36356266000999127    steps: 244    lr: 6.400000000000001e-06     evaluation reward: 4.07\n",
      "episode: 1985   score: 6.0   memory length: 100000   epsilon: 0.3628122400099865    steps: 379    lr: 6.400000000000001e-06     evaluation reward: 4.1\n",
      "episode: 1986   score: 5.0   memory length: 100000   epsilon: 0.3622400200099829    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 4.08\n",
      "episode: 1987   score: 4.0   memory length: 100000   epsilon: 0.36175690000997984    steps: 244    lr: 6.400000000000001e-06     evaluation reward: 4.09\n",
      "episode: 1988   score: 4.0   memory length: 100000   epsilon: 0.36120448000997635    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 4.1\n",
      "episode: 1989   score: 7.0   memory length: 100000   epsilon: 0.3604263400099714    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 4.13\n",
      "episode: 1990   score: 2.0   memory length: 100000   epsilon: 0.36006400000996913    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 4.08\n",
      "episode: 1991   score: 7.0   memory length: 100000   epsilon: 0.35924824000996397    steps: 412    lr: 6.400000000000001e-06     evaluation reward: 4.08\n",
      "episode: 1992   score: 3.0   memory length: 100000   epsilon: 0.3587948200099611    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 4.07\n",
      "episode: 1993   score: 4.0   memory length: 100000   epsilon: 0.3582424000099576    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 4.06\n",
      "episode: 1994   score: 6.0   memory length: 100000   epsilon: 0.35748802000995283    steps: 381    lr: 6.400000000000001e-06     evaluation reward: 4.08\n",
      "episode: 1995   score: 2.0   memory length: 100000   epsilon: 0.3570900400099503    steps: 201    lr: 6.400000000000001e-06     evaluation reward: 4.04\n",
      "episode: 1996   score: 3.0   memory length: 100000   epsilon: 0.35666632000994763    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 4.04\n",
      "episode: 1997   score: 5.0   memory length: 100000   epsilon: 0.3560644000099438    steps: 304    lr: 6.400000000000001e-06     evaluation reward: 4.05\n",
      "episode: 1998   score: 5.0   memory length: 100000   epsilon: 0.35545060000993994    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 4.04\n",
      "episode: 1999   score: 3.0   memory length: 100000   epsilon: 0.3550308400099373    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 4.02\n",
      "episode: 2000   score: 5.0   memory length: 100000   epsilon: 0.35445466000993364    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 4.05\n",
      "episode: 2001   score: 2.0   memory length: 100000   epsilon: 0.3540566800099311    steps: 201    lr: 6.400000000000001e-06     evaluation reward: 4.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2002   score: 4.0   memory length: 100000   epsilon: 0.35350624000992764    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 4.04\n",
      "episode: 2003   score: 5.0   memory length: 100000   epsilon: 0.3529003600099238    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 4.07\n",
      "episode: 2004   score: 6.0   memory length: 100000   epsilon: 0.3521915200099193    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 4.1\n",
      "episode: 2005   score: 4.0   memory length: 100000   epsilon: 0.3516787000099161    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 4.1\n",
      "episode: 2006   score: 5.0   memory length: 100000   epsilon: 0.35109262000991237    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 4.08\n",
      "episode: 2007   score: 3.0   memory length: 100000   epsilon: 0.3506748400099097    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 4.07\n",
      "episode: 2008   score: 6.0   memory length: 100000   epsilon: 0.3499937200099054    steps: 344    lr: 6.400000000000001e-06     evaluation reward: 4.1\n",
      "episode: 2009   score: 3.0   memory length: 100000   epsilon: 0.34954030000990255    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 4.1\n",
      "episode: 2010   score: 5.0   memory length: 100000   epsilon: 0.348976000009899    steps: 285    lr: 6.400000000000001e-06     evaluation reward: 4.13\n",
      "episode: 2011   score: 8.0   memory length: 100000   epsilon: 0.3480790600098933    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 4.18\n",
      "episode: 2012   score: 5.0   memory length: 100000   epsilon: 0.3474652600098894    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 4.2\n",
      "episode: 2013   score: 5.0   memory length: 100000   epsilon: 0.3468435400098855    steps: 314    lr: 6.400000000000001e-06     evaluation reward: 4.21\n",
      "episode: 2014   score: 4.0   memory length: 100000   epsilon: 0.34628518000988195    steps: 282    lr: 6.400000000000001e-06     evaluation reward: 4.21\n",
      "episode: 2015   score: 3.0   memory length: 100000   epsilon: 0.3458614600098793    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 4.16\n",
      "episode: 2016   score: 2.0   memory length: 100000   epsilon: 0.3454674400098768    steps: 199    lr: 6.400000000000001e-06     evaluation reward: 4.14\n",
      "episode: 2017   score: 4.0   memory length: 100000   epsilon: 0.3449605600098736    steps: 256    lr: 6.400000000000001e-06     evaluation reward: 4.14\n",
      "episode: 2018   score: 2.0   memory length: 100000   epsilon: 0.3446002000098713    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 4.11\n",
      "episode: 2019   score: 3.0   memory length: 100000   epsilon: 0.34410322000986815    steps: 251    lr: 6.400000000000001e-06     evaluation reward: 4.11\n",
      "episode: 2020   score: 5.0   memory length: 100000   epsilon: 0.3435290200098645    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 4.11\n",
      "episode: 2021   score: 4.0   memory length: 100000   epsilon: 0.34301422000986126    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.11\n",
      "episode: 2022   score: 5.0   memory length: 100000   epsilon: 0.34244200000985764    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 4.11\n",
      "episode: 2023   score: 4.0   memory length: 100000   epsilon: 0.34189354000985417    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.11\n",
      "episode: 2024   score: 6.0   memory length: 100000   epsilon: 0.3412480600098501    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 4.14\n",
      "episode: 2025   score: 6.0   memory length: 100000   epsilon: 0.3405412000098456    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 4.16\n",
      "episode: 2026   score: 5.0   memory length: 100000   epsilon: 0.33992938000984174    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 4.18\n",
      "episode: 2027   score: 6.0   memory length: 100000   epsilon: 0.3392601400098375    steps: 338    lr: 6.400000000000001e-06     evaluation reward: 4.21\n",
      "episode: 2028   score: 5.0   memory length: 100000   epsilon: 0.3386463400098336    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 4.24\n",
      "episode: 2029   score: 5.0   memory length: 100000   epsilon: 0.33800086000982954    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 4.25\n",
      "episode: 2030   score: 3.0   memory length: 100000   epsilon: 0.33751378000982646    steps: 246    lr: 6.400000000000001e-06     evaluation reward: 4.24\n",
      "episode: 2031   score: 3.0   memory length: 100000   epsilon: 0.3370603600098236    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 4.25\n",
      "episode: 2032   score: 12.0   memory length: 100000   epsilon: 0.3359674000098167    steps: 552    lr: 6.400000000000001e-06     evaluation reward: 4.35\n",
      "episode: 2033   score: 5.0   memory length: 100000   epsilon: 0.3353258800098126    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 4.34\n",
      "episode: 2034   score: 2.0   memory length: 100000   epsilon: 0.3349635400098103    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 4.3\n",
      "episode: 2035   score: 3.0   memory length: 100000   epsilon: 0.33451012000980745    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 4.28\n",
      "episode: 2036   score: 4.0   memory length: 100000   epsilon: 0.3339933400098042    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 4.3\n",
      "episode: 2037   score: 6.0   memory length: 100000   epsilon: 0.3332686600097996    steps: 366    lr: 6.400000000000001e-06     evaluation reward: 4.32\n",
      "episode: 2038   score: 4.0   memory length: 100000   epsilon: 0.33275584000979636    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 4.31\n",
      "episode: 2039   score: 4.0   memory length: 100000   epsilon: 0.3322430200097931    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 4.3\n",
      "episode: 2040   score: 3.0   memory length: 100000   epsilon: 0.33178762000979023    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 4.3\n",
      "episode: 2041   score: 5.0   memory length: 100000   epsilon: 0.3311797600097864    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 4.31\n",
      "episode: 2042   score: 5.0   memory length: 100000   epsilon: 0.3305362600097823    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 4.33\n",
      "episode: 2043   score: 6.0   memory length: 100000   epsilon: 0.3297917800097776    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 4.36\n",
      "episode: 2044   score: 5.0   memory length: 100000   epsilon: 0.3291106600097733    steps: 344    lr: 6.400000000000001e-06     evaluation reward: 4.36\n",
      "episode: 2045   score: 5.0   memory length: 100000   epsilon: 0.32853844000976967    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 4.34\n",
      "episode: 2046   score: 4.0   memory length: 100000   epsilon: 0.3280256200097664    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 4.29\n",
      "episode: 2047   score: 3.0   memory length: 100000   epsilon: 0.327487060009763    steps: 272    lr: 6.400000000000001e-06     evaluation reward: 4.28\n",
      "episode: 2048   score: 5.0   memory length: 100000   epsilon: 0.3268792000097592    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 4.3\n",
      "episode: 2049   score: 3.0   memory length: 100000   epsilon: 0.32642974000975633    steps: 227    lr: 6.400000000000001e-06     evaluation reward: 4.3\n",
      "episode: 2050   score: 5.0   memory length: 100000   epsilon: 0.32581792000975246    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 4.31\n",
      "episode: 2051   score: 6.0   memory length: 100000   epsilon: 0.3251922400097485    steps: 316    lr: 6.400000000000001e-06     evaluation reward: 4.31\n",
      "episode: 2052   score: 3.0   memory length: 100000   epsilon: 0.3247031800097454    steps: 247    lr: 6.400000000000001e-06     evaluation reward: 4.31\n",
      "episode: 2053   score: 5.0   memory length: 100000   epsilon: 0.324164620009742    steps: 272    lr: 6.400000000000001e-06     evaluation reward: 4.3\n",
      "episode: 2054   score: 4.0   memory length: 100000   epsilon: 0.3236082400097385    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 4.32\n",
      "episode: 2055   score: 6.0   memory length: 100000   epsilon: 0.322903360009734    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 4.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2056   score: 3.0   memory length: 100000   epsilon: 0.3224875600097314    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 4.35\n",
      "episode: 2057   score: 7.0   memory length: 100000   epsilon: 0.32162626000972594    steps: 435    lr: 6.400000000000001e-06     evaluation reward: 4.39\n",
      "episode: 2058   score: 2.0   memory length: 100000   epsilon: 0.32126392000972365    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 4.36\n",
      "episode: 2059   score: 5.0   memory length: 100000   epsilon: 0.3206560600097198    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 4.36\n",
      "episode: 2060   score: 6.0   memory length: 100000   epsilon: 0.3200105800097157    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 4.4\n",
      "episode: 2061   score: 3.0   memory length: 100000   epsilon: 0.3195155800097126    steps: 250    lr: 6.400000000000001e-06     evaluation reward: 4.41\n",
      "episode: 2062   score: 7.0   memory length: 100000   epsilon: 0.3187572400097078    steps: 383    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2063   score: 7.0   memory length: 100000   epsilon: 0.3179870200097029    steps: 389    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2064   score: 1.0   memory length: 100000   epsilon: 0.317688040009701    steps: 151    lr: 6.400000000000001e-06     evaluation reward: 4.45\n",
      "episode: 2065   score: 5.0   memory length: 100000   epsilon: 0.3170386000096969    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2066   score: 6.0   memory length: 100000   epsilon: 0.316258480009692    steps: 394    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2067   score: 4.0   memory length: 100000   epsilon: 0.3157753600096889    steps: 244    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2068   score: 6.0   memory length: 100000   epsilon: 0.315001180009684    steps: 391    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2069   score: 2.0   memory length: 100000   epsilon: 0.3146032000096815    steps: 201    lr: 6.400000000000001e-06     evaluation reward: 4.45\n",
      "episode: 2070   score: 6.0   memory length: 100000   epsilon: 0.31381714000967653    steps: 397    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 2071   score: 5.0   memory length: 100000   epsilon: 0.3132073000096727    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2072   score: 7.0   memory length: 100000   epsilon: 0.3124351000096678    steps: 390    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 2073   score: 6.0   memory length: 100000   epsilon: 0.3117282400096633    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 4.54\n",
      "episode: 2074   score: 3.0   memory length: 100000   epsilon: 0.3112669000096604    steps: 233    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 2075   score: 4.0   memory length: 100000   epsilon: 0.3107184400096569    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2076   score: 4.0   memory length: 100000   epsilon: 0.3102373000096539    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2077   score: 3.0   memory length: 100000   epsilon: 0.309779920009651    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2078   score: 5.0   memory length: 100000   epsilon: 0.3091978000096473    steps: 294    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2079   score: 3.0   memory length: 100000   epsilon: 0.3087740800096446    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2080   score: 3.0   memory length: 100000   epsilon: 0.30828502000964153    steps: 247    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2081   score: 2.0   memory length: 100000   epsilon: 0.30792268000963924    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 2082   score: 4.0   memory length: 100000   epsilon: 0.30743758000963617    steps: 245    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 2083   score: 5.0   memory length: 100000   epsilon: 0.3068257600096323    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2084   score: 5.0   memory length: 100000   epsilon: 0.3062099800096284    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 4.49\n",
      "episode: 2085   score: 3.0   memory length: 100000   epsilon: 0.30576052000962556    steps: 227    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 2086   score: 6.0   memory length: 100000   epsilon: 0.30509524000962135    steps: 336    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2087   score: 4.0   memory length: 100000   epsilon: 0.30458638000961813    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2088   score: 4.0   memory length: 100000   epsilon: 0.3040795000096149    steps: 256    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2089   score: 6.0   memory length: 100000   epsilon: 0.30341818000961074    steps: 334    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 2090   score: 4.0   memory length: 100000   epsilon: 0.3028736800096073    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2091   score: 7.0   memory length: 100000   epsilon: 0.30203218000960197    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2092   score: 2.0   memory length: 100000   epsilon: 0.30163420000959945    steps: 201    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2093   score: 3.0   memory length: 100000   epsilon: 0.3012184000095968    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 2094   score: 4.0   memory length: 100000   epsilon: 0.3007372600095938    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 4.44\n",
      "episode: 2095   score: 3.0   memory length: 100000   epsilon: 0.3002858200095909    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 4.45\n",
      "episode: 2096   score: 4.0   memory length: 100000   epsilon: 0.2997413200095875    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 2097   score: 3.0   memory length: 100000   epsilon: 0.2992859200095846    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 4.44\n",
      "episode: 2098   score: 2.0   memory length: 100000   epsilon: 0.2989235800095823    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 4.41\n",
      "episode: 2099   score: 10.0   memory length: 100000   epsilon: 0.29818702000957764    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2100   score: 6.0   memory length: 100000   epsilon: 0.29750590000957333    steps: 344    lr: 6.400000000000001e-06     evaluation reward: 4.49\n",
      "episode: 2101   score: 3.0   memory length: 100000   epsilon: 0.29705050000957045    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2102   score: 4.0   memory length: 100000   epsilon: 0.296506000009567    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2103   score: 6.0   memory length: 100000   epsilon: 0.29571994000956203    steps: 397    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 2104   score: 5.0   memory length: 100000   epsilon: 0.2950705000095579    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2105   score: 6.0   memory length: 100000   epsilon: 0.2944052200095537    steps: 336    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2106   score: 2.0   memory length: 100000   epsilon: 0.2940092200095512    steps: 200    lr: 6.400000000000001e-06     evaluation reward: 4.49\n",
      "episode: 2107   score: 4.0   memory length: 100000   epsilon: 0.29349442000954795    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2108   score: 4.0   memory length: 100000   epsilon: 0.2929479400095445    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2109   score: 4.0   memory length: 100000   epsilon: 0.292393540009541    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 4.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2110   score: 4.0   memory length: 100000   epsilon: 0.2918411200095375    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2111   score: 4.0   memory length: 100000   epsilon: 0.29129464000953403    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 4.44\n",
      "episode: 2112   score: 4.0   memory length: 100000   epsilon: 0.2907481600095306    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 4.43\n",
      "episode: 2113   score: 5.0   memory length: 100000   epsilon: 0.29020564000952714    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 4.43\n",
      "episode: 2114   score: 3.0   memory length: 100000   epsilon: 0.2897878600095245    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 4.42\n",
      "episode: 2115   score: 6.0   memory length: 100000   epsilon: 0.28903744000951975    steps: 379    lr: 6.400000000000001e-06     evaluation reward: 4.45\n",
      "episode: 2116   score: 4.0   memory length: 100000   epsilon: 0.2885226400095165    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2117   score: 6.0   memory length: 100000   epsilon: 0.2877781600095118    steps: 376    lr: 6.400000000000001e-06     evaluation reward: 4.49\n",
      "episode: 2118   score: 5.0   memory length: 100000   epsilon: 0.2871683200095079    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2119   score: 6.0   memory length: 100000   epsilon: 0.2864218600095032    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 4.55\n",
      "episode: 2120   score: 2.0   memory length: 100000   epsilon: 0.28606348000950094    steps: 181    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2121   score: 4.0   memory length: 100000   epsilon: 0.2855506600094977    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2122   score: 9.0   memory length: 100000   epsilon: 0.284653720009492    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 4.56\n",
      "episode: 2123   score: 2.0   memory length: 100000   epsilon: 0.2842913800094897    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 4.54\n",
      "episode: 2124   score: 6.0   memory length: 100000   epsilon: 0.28369144000948593    steps: 303    lr: 6.400000000000001e-06     evaluation reward: 4.54\n",
      "episode: 2125   score: 4.0   memory length: 100000   epsilon: 0.2832162400094829    steps: 240    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2126   score: 2.0   memory length: 100000   epsilon: 0.2828182600094804    steps: 201    lr: 6.400000000000001e-06     evaluation reward: 4.49\n",
      "episode: 2127   score: 4.0   memory length: 100000   epsilon: 0.2822321800094767    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2128   score: 4.0   memory length: 100000   epsilon: 0.2817589600094737    steps: 239    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 2129   score: 5.0   memory length: 100000   epsilon: 0.2811451600094698    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 2130   score: 4.0   memory length: 100000   epsilon: 0.28059670000946635    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2131   score: 7.0   memory length: 100000   epsilon: 0.2797967800094613    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 2132   score: 6.0   memory length: 100000   epsilon: 0.2790701200094567    steps: 367    lr: 6.400000000000001e-06     evaluation reward: 4.45\n",
      "episode: 2133   score: 6.0   memory length: 100000   epsilon: 0.2783632600094522    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 2134   score: 4.0   memory length: 100000   epsilon: 0.27781282000944874    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2135   score: 1.0   memory length: 100000   epsilon: 0.27751186000944683    steps: 152    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 2136   score: 5.0   memory length: 100000   epsilon: 0.276907960009443    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2137   score: 4.0   memory length: 100000   epsilon: 0.27636346000943957    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 4.45\n",
      "episode: 2138   score: 4.0   memory length: 100000   epsilon: 0.2758130200094361    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 4.45\n",
      "episode: 2139   score: 3.0   memory length: 100000   epsilon: 0.27539524000943344    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 4.44\n",
      "episode: 2140   score: 5.0   memory length: 100000   epsilon: 0.27478342000942957    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 2141   score: 6.0   memory length: 100000   epsilon: 0.2739973600094246    steps: 397    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2142   score: 7.0   memory length: 100000   epsilon: 0.27326476000941996    steps: 370    lr: 6.400000000000001e-06     evaluation reward: 4.49\n",
      "episode: 2143   score: 8.0   memory length: 100000   epsilon: 0.2723698000094143    steps: 452    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 2144   score: 4.0   memory length: 100000   epsilon: 0.27182332000941084    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2145   score: 3.0   memory length: 100000   epsilon: 0.2713263400094077    steps: 251    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2146   score: 3.0   memory length: 100000   epsilon: 0.270902620009405    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2147   score: 5.0   memory length: 100000   epsilon: 0.2702551600094009    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 4.49\n",
      "episode: 2148   score: 6.0   memory length: 100000   epsilon: 0.2696037400093968    steps: 329    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2149   score: 5.0   memory length: 100000   epsilon: 0.2690156800093931    steps: 297    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2150   score: 5.0   memory length: 100000   epsilon: 0.26840782000938923    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2151   score: 4.0   memory length: 100000   epsilon: 0.26785738000938575    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2152   score: 4.0   memory length: 100000   epsilon: 0.2673762400093827    steps: 243    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 2153   score: 6.0   memory length: 100000   epsilon: 0.26670502000937846    steps: 339    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2154   score: 6.0   memory length: 100000   epsilon: 0.26603974000937425    steps: 336    lr: 6.400000000000001e-06     evaluation reward: 4.54\n",
      "episode: 2155   score: 7.0   memory length: 100000   epsilon: 0.2652378400093692    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 4.55\n",
      "episode: 2156   score: 4.0   memory length: 100000   epsilon: 0.2647230400093659    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.56\n",
      "episode: 2157   score: 5.0   memory length: 100000   epsilon: 0.26411122000936205    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 4.54\n",
      "episode: 2158   score: 3.0   memory length: 100000   epsilon: 0.2636182000093589    steps: 249    lr: 6.400000000000001e-06     evaluation reward: 4.55\n",
      "episode: 2159   score: 7.0   memory length: 100000   epsilon: 0.26285986000935413    steps: 383    lr: 6.400000000000001e-06     evaluation reward: 4.57\n",
      "episode: 2160   score: 2.0   memory length: 100000   epsilon: 0.26246782000935165    steps: 198    lr: 6.400000000000001e-06     evaluation reward: 4.53\n",
      "episode: 2161   score: 6.0   memory length: 100000   epsilon: 0.26182234000934757    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 4.56\n",
      "episode: 2162   score: 4.0   memory length: 100000   epsilon: 0.26134516000934455    steps: 241    lr: 6.400000000000001e-06     evaluation reward: 4.53\n",
      "episode: 2163   score: 5.0   memory length: 100000   epsilon: 0.2606917600093404    steps: 330    lr: 6.400000000000001e-06     evaluation reward: 4.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2164   score: 3.0   memory length: 100000   epsilon: 0.26026804000933773    steps: 214    lr: 6.400000000000001e-06     evaluation reward: 4.53\n",
      "episode: 2165   score: 3.0   memory length: 100000   epsilon: 0.25984630000933506    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 2166   score: 7.0   memory length: 100000   epsilon: 0.25935724000933197    steps: 247    lr: 6.400000000000001e-06     evaluation reward: 4.52\n",
      "episode: 2167   score: 3.0   memory length: 100000   epsilon: 0.2589058000093291    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 4.51\n",
      "episode: 2168   score: 4.0   memory length: 100000   epsilon: 0.25836130000932567    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 4.49\n",
      "episode: 2169   score: 3.0   memory length: 100000   epsilon: 0.25786828000932255    steps: 249    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2170   score: 4.0   memory length: 100000   epsilon: 0.25731190000931903    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2171   score: 4.0   memory length: 100000   epsilon: 0.256834720009316    steps: 241    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2172   score: 4.0   memory length: 100000   epsilon: 0.25631992000931275    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.44\n",
      "episode: 2173   score: 5.0   memory length: 100000   epsilon: 0.25570612000930887    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 4.43\n",
      "episode: 2174   score: 4.0   memory length: 100000   epsilon: 0.25519528000930564    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 4.44\n",
      "episode: 2175   score: 6.0   memory length: 100000   epsilon: 0.2545240600093014    steps: 339    lr: 6.400000000000001e-06     evaluation reward: 4.46\n",
      "episode: 2176   score: 3.0   memory length: 100000   epsilon: 0.2540369800092983    steps: 246    lr: 6.400000000000001e-06     evaluation reward: 4.45\n",
      "episode: 2177   score: 6.0   memory length: 100000   epsilon: 0.25333210000929385    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 4.48\n",
      "episode: 2178   score: 7.0   memory length: 100000   epsilon: 0.2525975200092892    steps: 371    lr: 6.400000000000001e-06     evaluation reward: 4.5\n",
      "episode: 2179   score: 8.0   memory length: 100000   epsilon: 0.251773840009284    steps: 416    lr: 6.400000000000001e-06     evaluation reward: 4.55\n",
      "episode: 2180   score: 3.0   memory length: 100000   epsilon: 0.2513521000092813    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 4.55\n",
      "episode: 2181   score: 14.0   memory length: 100000   epsilon: 0.2505244600092761    steps: 418    lr: 6.400000000000001e-06     evaluation reward: 4.67\n",
      "episode: 2182   score: 4.0   memory length: 100000   epsilon: 0.24997006000927258    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 4.67\n",
      "episode: 2183   score: 4.0   memory length: 100000   epsilon: 0.2494216000092691    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.66\n",
      "episode: 2184   score: 3.0   memory length: 100000   epsilon: 0.24900184000926645    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 4.64\n",
      "episode: 2185   score: 4.0   memory length: 100000   epsilon: 0.24847714000926313    steps: 265    lr: 6.400000000000001e-06     evaluation reward: 4.65\n",
      "episode: 2186   score: 4.0   memory length: 100000   epsilon: 0.24789304000925944    steps: 295    lr: 6.400000000000001e-06     evaluation reward: 4.63\n",
      "episode: 2187   score: 3.0   memory length: 100000   epsilon: 0.2473980400092563    steps: 250    lr: 6.400000000000001e-06     evaluation reward: 4.62\n",
      "episode: 2188   score: 5.0   memory length: 100000   epsilon: 0.24679018000925246    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 4.63\n",
      "episode: 2189   score: 7.0   memory length: 100000   epsilon: 0.24598432000924736    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 4.64\n",
      "episode: 2190   score: 4.0   memory length: 100000   epsilon: 0.2454358600092439    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.64\n",
      "episode: 2191   score: 3.0   memory length: 100000   epsilon: 0.244980460009241    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 4.6\n",
      "episode: 2192   score: 4.0   memory length: 100000   epsilon: 0.24446764000923776    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 4.62\n",
      "episode: 2193   score: 2.0   memory length: 100000   epsilon: 0.24410530000923547    steps: 183    lr: 6.400000000000001e-06     evaluation reward: 4.61\n",
      "episode: 2194   score: 4.0   memory length: 100000   epsilon: 0.2435865400092322    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 4.61\n",
      "episode: 2195   score: 5.0   memory length: 100000   epsilon: 0.2429707600092283    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 4.63\n",
      "episode: 2196   score: 6.0   memory length: 100000   epsilon: 0.24226588000922383    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 4.65\n",
      "episode: 2197   score: 4.0   memory length: 100000   epsilon: 0.2417867200092208    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 4.66\n",
      "episode: 2198   score: 5.0   memory length: 100000   epsilon: 0.24125212000921742    steps: 270    lr: 6.400000000000001e-06     evaluation reward: 4.69\n",
      "episode: 2199   score: 4.0   memory length: 100000   epsilon: 0.2407452400092142    steps: 256    lr: 6.400000000000001e-06     evaluation reward: 4.63\n",
      "episode: 2200   score: 5.0   memory length: 100000   epsilon: 0.24013342000921034    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 4.62\n",
      "episode: 2201   score: 4.0   memory length: 100000   epsilon: 0.23961862000920708    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.63\n",
      "episode: 2202   score: 4.0   memory length: 100000   epsilon: 0.2390681800092036    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 4.63\n",
      "episode: 2203   score: 6.0   memory length: 100000   epsilon: 0.23836726000919917    steps: 354    lr: 6.400000000000001e-06     evaluation reward: 4.63\n",
      "episode: 2204   score: 4.0   memory length: 100000   epsilon: 0.23789404000919617    steps: 239    lr: 6.400000000000001e-06     evaluation reward: 4.62\n",
      "episode: 2205   score: 7.0   memory length: 100000   epsilon: 0.23708620000919106    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 4.63\n",
      "episode: 2206   score: 5.0   memory length: 100000   epsilon: 0.23643874000918697    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 4.66\n",
      "episode: 2207   score: 9.0   memory length: 100000   epsilon: 0.23548240000918091    steps: 483    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2208   score: 4.0   memory length: 100000   epsilon: 0.23493790000917747    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2209   score: 3.0   memory length: 100000   epsilon: 0.2345161600091748    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 4.7\n",
      "episode: 2210   score: 6.0   memory length: 100000   epsilon: 0.23384296000917054    steps: 340    lr: 6.400000000000001e-06     evaluation reward: 4.72\n",
      "episode: 2211   score: 3.0   memory length: 100000   epsilon: 0.23334994000916742    steps: 249    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2212   score: 4.0   memory length: 100000   epsilon: 0.23279752000916393    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2213   score: 5.0   memory length: 100000   epsilon: 0.23215006000915983    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2214   score: 4.0   memory length: 100000   epsilon: 0.2316214000091565    steps: 267    lr: 6.400000000000001e-06     evaluation reward: 4.72\n",
      "episode: 2215   score: 9.0   memory length: 100000   epsilon: 0.23074822000915096    steps: 441    lr: 6.400000000000001e-06     evaluation reward: 4.75\n",
      "episode: 2216   score: 3.0   memory length: 100000   epsilon: 0.23029084000914807    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 4.74\n",
      "episode: 2217   score: 1.0   memory length: 100000   epsilon: 0.22998988000914616    steps: 152    lr: 6.400000000000001e-06     evaluation reward: 4.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2218   score: 3.0   memory length: 100000   epsilon: 0.22953250000914327    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 4.67\n",
      "episode: 2219   score: 7.0   memory length: 100000   epsilon: 0.2287325800091382    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 4.68\n",
      "episode: 2220   score: 5.0   memory length: 100000   epsilon: 0.22812670000913438    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2221   score: 5.0   memory length: 100000   epsilon: 0.2275129000091305    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 4.72\n",
      "episode: 2222   score: 6.0   memory length: 100000   epsilon: 0.2268496000091263    steps: 335    lr: 6.400000000000001e-06     evaluation reward: 4.69\n",
      "episode: 2223   score: 3.0   memory length: 100000   epsilon: 0.22639618000912343    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 4.7\n",
      "episode: 2224   score: 7.0   memory length: 100000   epsilon: 0.22561210000911847    steps: 396    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2225   score: 4.0   memory length: 100000   epsilon: 0.2250953200091152    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2226   score: 10.0   memory length: 100000   epsilon: 0.2240201800091084    steps: 543    lr: 6.400000000000001e-06     evaluation reward: 4.79\n",
      "episode: 2227   score: 3.0   memory length: 100000   epsilon: 0.22360042000910574    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 4.78\n",
      "episode: 2228   score: 4.0   memory length: 100000   epsilon: 0.2231212600091027    steps: 242    lr: 6.400000000000001e-06     evaluation reward: 4.78\n",
      "episode: 2229   score: 5.0   memory length: 100000   epsilon: 0.22251736000909889    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 4.78\n",
      "episode: 2230   score: 3.0   memory length: 100000   epsilon: 0.22206790000909604    steps: 227    lr: 6.400000000000001e-06     evaluation reward: 4.77\n",
      "episode: 2231   score: 4.0   memory length: 100000   epsilon: 0.22154914000909276    steps: 262    lr: 6.400000000000001e-06     evaluation reward: 4.74\n",
      "episode: 2232   score: 6.0   memory length: 100000   epsilon: 0.22087198000908848    steps: 342    lr: 6.400000000000001e-06     evaluation reward: 4.74\n",
      "episode: 2233   score: 5.0   memory length: 100000   epsilon: 0.22027204000908468    steps: 303    lr: 6.400000000000001e-06     evaluation reward: 4.73\n",
      "episode: 2234   score: 7.0   memory length: 100000   epsilon: 0.21947212000907962    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 4.76\n",
      "episode: 2235   score: 4.0   memory length: 100000   epsilon: 0.21892564000907616    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 4.79\n",
      "episode: 2236   score: 2.0   memory length: 100000   epsilon: 0.21856528000907388    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 4.76\n",
      "episode: 2237   score: 8.0   memory length: 100000   epsilon: 0.2176683400090682    steps: 453    lr: 6.400000000000001e-06     evaluation reward: 4.8\n",
      "episode: 2238   score: 6.0   memory length: 100000   epsilon: 0.21696346000906375    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 4.82\n",
      "episode: 2239   score: 4.0   memory length: 100000   epsilon: 0.2164486600090605    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.83\n",
      "episode: 2240   score: 3.0   memory length: 100000   epsilon: 0.21602692000905782    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 4.81\n",
      "episode: 2241   score: 7.0   memory length: 100000   epsilon: 0.2152507600090529    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 4.82\n",
      "episode: 2242   score: 4.0   memory length: 100000   epsilon: 0.21473596000904965    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.79\n",
      "episode: 2243   score: 8.0   memory length: 100000   epsilon: 0.21389842000904435    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 4.79\n",
      "episode: 2244   score: 4.0   memory length: 100000   epsilon: 0.2133836200090411    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.79\n",
      "episode: 2245   score: 3.0   memory length: 100000   epsilon: 0.21296584000903845    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 4.79\n",
      "episode: 2246   score: 6.0   memory length: 100000   epsilon: 0.2122312600090338    steps: 371    lr: 6.400000000000001e-06     evaluation reward: 4.82\n",
      "episode: 2247   score: 4.0   memory length: 100000   epsilon: 0.21171646000903055    steps: 260    lr: 6.400000000000001e-06     evaluation reward: 4.81\n",
      "episode: 2248   score: 3.0   memory length: 100000   epsilon: 0.21122146000902742    steps: 250    lr: 6.400000000000001e-06     evaluation reward: 4.78\n",
      "episode: 2249   score: 4.0   memory length: 100000   epsilon: 0.21066904000902392    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 4.77\n",
      "episode: 2250   score: 4.0   memory length: 100000   epsilon: 0.2101582000090207    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 4.76\n",
      "episode: 2251   score: 4.0   memory length: 100000   epsilon: 0.20960380000901718    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 4.76\n",
      "episode: 2252   score: 4.0   memory length: 100000   epsilon: 0.20912662000901416    steps: 241    lr: 6.400000000000001e-06     evaluation reward: 4.76\n",
      "episode: 2253   score: 4.0   memory length: 100000   epsilon: 0.20854054000901046    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 4.74\n",
      "episode: 2254   score: 7.0   memory length: 100000   epsilon: 0.20778022000900565    steps: 384    lr: 2.560000000000001e-06     evaluation reward: 4.75\n",
      "episode: 2255   score: 6.0   memory length: 100000   epsilon: 0.20704366000900098    steps: 372    lr: 2.560000000000001e-06     evaluation reward: 4.74\n",
      "episode: 2256   score: 10.0   memory length: 100000   epsilon: 0.20621602000899575    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 4.8\n",
      "episode: 2257   score: 3.0   memory length: 100000   epsilon: 0.20576062000899287    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 4.78\n",
      "episode: 2258   score: 6.0   memory length: 100000   epsilon: 0.20505970000898843    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 4.81\n",
      "episode: 2259   score: 6.0   memory length: 100000   epsilon: 0.20431918000898375    steps: 374    lr: 2.560000000000001e-06     evaluation reward: 4.8\n",
      "episode: 2260   score: 8.0   memory length: 100000   epsilon: 0.20340838000897798    steps: 460    lr: 2.560000000000001e-06     evaluation reward: 4.86\n",
      "episode: 2261   score: 4.0   memory length: 100000   epsilon: 0.20293120000897497    steps: 241    lr: 2.560000000000001e-06     evaluation reward: 4.84\n",
      "episode: 2262   score: 4.0   memory length: 100000   epsilon: 0.20245204000897193    steps: 242    lr: 2.560000000000001e-06     evaluation reward: 4.84\n",
      "episode: 2263   score: 5.0   memory length: 100000   epsilon: 0.2018758600089683    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 4.84\n",
      "episode: 2264   score: 3.0   memory length: 100000   epsilon: 0.20145808000896565    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 4.84\n",
      "episode: 2265   score: 7.0   memory length: 100000   epsilon: 0.20071756000896096    steps: 374    lr: 2.560000000000001e-06     evaluation reward: 4.88\n",
      "episode: 2266   score: 3.0   memory length: 100000   epsilon: 0.20029978000895832    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 4.84\n",
      "episode: 2267   score: 7.0   memory length: 100000   epsilon: 0.19949986000895326    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 4.88\n",
      "episode: 2268   score: 8.0   memory length: 100000   epsilon: 0.19861084000894763    steps: 449    lr: 2.560000000000001e-06     evaluation reward: 4.92\n",
      "episode: 2269   score: 6.0   memory length: 100000   epsilon: 0.197878240008943    steps: 370    lr: 2.560000000000001e-06     evaluation reward: 4.95\n",
      "episode: 2270   score: 4.0   memory length: 100000   epsilon: 0.1973575000089397    steps: 263    lr: 2.560000000000001e-06     evaluation reward: 4.95\n",
      "episode: 2271   score: 3.0   memory length: 100000   epsilon: 0.1969001200089368    steps: 231    lr: 2.560000000000001e-06     evaluation reward: 4.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2272   score: 4.0   memory length: 100000   epsilon: 0.19634968000893332    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 4.94\n",
      "episode: 2273   score: 6.0   memory length: 100000   epsilon: 0.19564084000892884    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 4.95\n",
      "episode: 2274   score: 3.0   memory length: 100000   epsilon: 0.19517950000892592    steps: 233    lr: 2.560000000000001e-06     evaluation reward: 4.94\n",
      "episode: 2275   score: 6.0   memory length: 100000   epsilon: 0.1944825400089215    steps: 352    lr: 2.560000000000001e-06     evaluation reward: 4.94\n",
      "episode: 2276   score: 3.0   memory length: 100000   epsilon: 0.19403308000891867    steps: 227    lr: 2.560000000000001e-06     evaluation reward: 4.94\n",
      "episode: 2277   score: 4.0   memory length: 100000   epsilon: 0.19348264000891519    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 4.92\n",
      "episode: 2278   score: 5.0   memory length: 100000   epsilon: 0.19283914000891111    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 4.9\n",
      "episode: 2279   score: 6.0   memory length: 100000   epsilon: 0.19216198000890683    steps: 342    lr: 2.560000000000001e-06     evaluation reward: 4.88\n",
      "episode: 2280   score: 5.0   memory length: 100000   epsilon: 0.19159372000890323    steps: 287    lr: 2.560000000000001e-06     evaluation reward: 4.9\n",
      "episode: 2281   score: 4.0   memory length: 100000   epsilon: 0.19111654000890022    steps: 241    lr: 2.560000000000001e-06     evaluation reward: 4.8\n",
      "episode: 2282   score: 6.0   memory length: 100000   epsilon: 0.190451260008896    steps: 336    lr: 2.560000000000001e-06     evaluation reward: 4.82\n",
      "episode: 2283   score: 6.0   memory length: 100000   epsilon: 0.18970480000889128    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 4.84\n",
      "episode: 2284   score: 6.0   memory length: 100000   epsilon: 0.18897814000888669    steps: 367    lr: 2.560000000000001e-06     evaluation reward: 4.87\n",
      "episode: 2285   score: 3.0   memory length: 100000   epsilon: 0.1885227400088838    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 4.86\n",
      "episode: 2286   score: 4.0   memory length: 100000   epsilon: 0.18800794000888055    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 4.86\n",
      "episode: 2287   score: 5.0   memory length: 100000   epsilon: 0.18740206000887671    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 4.88\n",
      "episode: 2288   score: 4.0   memory length: 100000   epsilon: 0.18685360000887324    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 4.87\n",
      "episode: 2289   score: 9.0   memory length: 100000   epsilon: 0.18595468000886756    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 4.89\n",
      "episode: 2290   score: 5.0   memory length: 100000   epsilon: 0.185394340008864    steps: 283    lr: 2.560000000000001e-06     evaluation reward: 4.9\n",
      "episode: 2291   score: 3.0   memory length: 100000   epsilon: 0.18493894000886113    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 4.9\n",
      "episode: 2292   score: 3.0   memory length: 100000   epsilon: 0.18448156000885824    steps: 231    lr: 2.560000000000001e-06     evaluation reward: 4.89\n",
      "episode: 2293   score: 9.0   memory length: 100000   epsilon: 0.1835588800088524    steps: 466    lr: 2.560000000000001e-06     evaluation reward: 4.96\n",
      "episode: 2294   score: 7.0   memory length: 100000   epsilon: 0.18274906000884727    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 4.99\n",
      "episode: 2295   score: 7.0   memory length: 100000   epsilon: 0.18194914000884221    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2296   score: 5.0   memory length: 100000   epsilon: 0.18134128000883837    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2297   score: 4.0   memory length: 100000   epsilon: 0.1808264800088351    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2298   score: 6.0   memory length: 100000   epsilon: 0.1801612000088309    steps: 336    lr: 2.560000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2299   score: 4.0   memory length: 100000   epsilon: 0.17968402000882788    steps: 241    lr: 2.560000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2300   score: 6.0   memory length: 100000   epsilon: 0.1790227000088237    steps: 334    lr: 2.560000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2301   score: 4.0   memory length: 100000   epsilon: 0.17847424000882023    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2302   score: 2.0   memory length: 100000   epsilon: 0.17811190000881794    steps: 183    lr: 2.560000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2303   score: 9.0   memory length: 100000   epsilon: 0.17719714000881215    steps: 462    lr: 2.560000000000001e-06     evaluation reward: 5.03\n",
      "episode: 2304   score: 7.0   memory length: 100000   epsilon: 0.17643880000880735    steps: 383    lr: 2.560000000000001e-06     evaluation reward: 5.06\n",
      "episode: 2305   score: 3.0   memory length: 100000   epsilon: 0.17598142000880446    steps: 231    lr: 2.560000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2306   score: 6.0   memory length: 100000   epsilon: 0.1753260400088003    steps: 331    lr: 2.560000000000001e-06     evaluation reward: 5.03\n",
      "episode: 2307   score: 5.0   memory length: 100000   epsilon: 0.1746746200087962    steps: 329    lr: 2.560000000000001e-06     evaluation reward: 4.99\n",
      "episode: 2308   score: 7.0   memory length: 100000   epsilon: 0.17392816000879147    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2309   score: 5.0   memory length: 100000   epsilon: 0.17327674000878734    steps: 329    lr: 2.560000000000001e-06     evaluation reward: 5.04\n",
      "episode: 2310   score: 5.0   memory length: 100000   epsilon: 0.17262730000878324    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 5.03\n",
      "episode: 2311   score: 4.0   memory length: 100000   epsilon: 0.17211844000878002    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.04\n",
      "episode: 2312   score: 7.0   memory length: 100000   epsilon: 0.17131852000877495    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2313   score: 5.0   memory length: 100000   epsilon: 0.17070472000877107    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2314   score: 3.0   memory length: 100000   epsilon: 0.1702493200087682    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 5.06\n",
      "episode: 2315   score: 5.0   memory length: 100000   epsilon: 0.16963750000876432    steps: 309    lr: 2.560000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2316   score: 3.0   memory length: 100000   epsilon: 0.16918012000876143    steps: 231    lr: 2.560000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2317   score: 3.0   memory length: 100000   epsilon: 0.16872472000875854    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 5.04\n",
      "episode: 2318   score: 4.0   memory length: 100000   epsilon: 0.1682099200087553    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 5.05\n",
      "episode: 2319   score: 3.0   memory length: 100000   epsilon: 0.1677545200087524    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2320   score: 4.0   memory length: 100000   epsilon: 0.1672001200087489    steps: 280    lr: 2.560000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2321   score: 5.0   memory length: 100000   epsilon: 0.16659226000874505    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2322   score: 2.0   memory length: 100000   epsilon: 0.16619626000874255    steps: 200    lr: 2.560000000000001e-06     evaluation reward: 4.96\n",
      "episode: 2323   score: 7.0   memory length: 100000   epsilon: 0.16543594000873774    steps: 384    lr: 2.560000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2324   score: 5.0   memory length: 100000   epsilon: 0.1648617400087341    steps: 290    lr: 2.560000000000001e-06     evaluation reward: 4.98\n",
      "episode: 2325   score: 4.0   memory length: 100000   epsilon: 0.16438456000873108    steps: 241    lr: 2.560000000000001e-06     evaluation reward: 4.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2326   score: 4.0   memory length: 100000   epsilon: 0.16387570000872786    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 4.92\n",
      "episode: 2327   score: 3.0   memory length: 100000   epsilon: 0.1634539600087252    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 4.92\n",
      "episode: 2328   score: 6.0   memory length: 100000   epsilon: 0.1627094800087205    steps: 376    lr: 2.560000000000001e-06     evaluation reward: 4.94\n",
      "episode: 2329   score: 5.0   memory length: 100000   epsilon: 0.16208380000871653    steps: 316    lr: 2.560000000000001e-06     evaluation reward: 4.94\n",
      "episode: 2330   score: 4.0   memory length: 100000   epsilon: 0.16156900000871327    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 4.95\n",
      "episode: 2331   score: 6.0   memory length: 100000   epsilon: 0.16089976000870904    steps: 338    lr: 2.560000000000001e-06     evaluation reward: 4.97\n",
      "episode: 2332   score: 7.0   memory length: 100000   epsilon: 0.16005232000870367    steps: 428    lr: 2.560000000000001e-06     evaluation reward: 4.98\n",
      "episode: 2333   score: 5.0   memory length: 100000   epsilon: 0.15948010000870005    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 4.98\n",
      "episode: 2334   score: 6.0   memory length: 100000   epsilon: 0.15869008000869506    steps: 399    lr: 2.560000000000001e-06     evaluation reward: 4.97\n",
      "episode: 2335   score: 5.0   memory length: 100000   epsilon: 0.1580822200086912    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 4.98\n",
      "episode: 2336   score: 6.0   memory length: 100000   epsilon: 0.15735952000868664    steps: 365    lr: 2.560000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2337   score: 6.0   memory length: 100000   epsilon: 0.15665068000868215    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2338   score: 6.0   memory length: 100000   epsilon: 0.15598342000867793    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2339   score: 8.0   memory length: 100000   epsilon: 0.15508054000867222    steps: 456    lr: 2.560000000000001e-06     evaluation reward: 5.04\n",
      "episode: 2340   score: 5.0   memory length: 100000   epsilon: 0.1544627800086683    steps: 312    lr: 2.560000000000001e-06     evaluation reward: 5.06\n",
      "episode: 2341   score: 5.0   memory length: 100000   epsilon: 0.1538588800086645    steps: 305    lr: 2.560000000000001e-06     evaluation reward: 5.04\n",
      "episode: 2342   score: 4.0   memory length: 100000   epsilon: 0.15337576000866143    steps: 244    lr: 2.560000000000001e-06     evaluation reward: 5.04\n",
      "episode: 2343   score: 3.0   memory length: 100000   epsilon: 0.15296194000865881    steps: 209    lr: 2.560000000000001e-06     evaluation reward: 4.99\n",
      "episode: 2344   score: 6.0   memory length: 100000   epsilon: 0.15228874000865456    steps: 340    lr: 2.560000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2345   score: 8.0   memory length: 100000   epsilon: 0.15153040000864976    steps: 383    lr: 2.560000000000001e-06     evaluation reward: 5.06\n",
      "episode: 2346   score: 4.0   memory length: 100000   epsilon: 0.1509502600086461    steps: 293    lr: 2.560000000000001e-06     evaluation reward: 5.04\n",
      "episode: 2347   score: 6.0   memory length: 100000   epsilon: 0.15020182000864135    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 5.06\n",
      "episode: 2348   score: 4.0   memory length: 100000   epsilon: 0.14964544000863783    steps: 281    lr: 2.560000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2349   score: 5.0   memory length: 100000   epsilon: 0.1490712400086342    steps: 290    lr: 2.560000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2350   score: 4.0   memory length: 100000   epsilon: 0.14855842000863095    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2351   score: 9.0   memory length: 100000   epsilon: 0.14783572000862638    steps: 365    lr: 2.560000000000001e-06     evaluation reward: 5.13\n",
      "episode: 2352   score: 5.0   memory length: 100000   epsilon: 0.14722984000862255    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 5.14\n",
      "episode: 2353   score: 7.0   memory length: 100000   epsilon: 0.1463863600086172    steps: 426    lr: 2.560000000000001e-06     evaluation reward: 5.17\n",
      "episode: 2354   score: 2.0   memory length: 100000   epsilon: 0.14602600000861493    steps: 182    lr: 2.560000000000001e-06     evaluation reward: 5.12\n",
      "episode: 2355   score: 5.0   memory length: 100000   epsilon: 0.14537656000861082    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 5.11\n",
      "episode: 2356   score: 6.0   memory length: 100000   epsilon: 0.1446756400086064    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2357   score: 4.0   memory length: 100000   epsilon: 0.1441232200086029    steps: 279    lr: 2.560000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2358   score: 5.0   memory length: 100000   epsilon: 0.14351734000859906    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2359   score: 5.0   memory length: 100000   epsilon: 0.14290354000859518    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 5.06\n",
      "episode: 2360   score: 7.0   memory length: 100000   epsilon: 0.14215708000859045    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 5.05\n",
      "episode: 2361   score: 6.0   memory length: 100000   epsilon: 0.14145022000858598    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2362   score: 7.0   memory length: 100000   epsilon: 0.14067010000858104    steps: 394    lr: 2.560000000000001e-06     evaluation reward: 5.1\n",
      "episode: 2363   score: 4.0   memory length: 100000   epsilon: 0.14011966000857756    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 5.09\n",
      "episode: 2364   score: 6.0   memory length: 100000   epsilon: 0.13945834000857338    steps: 334    lr: 2.560000000000001e-06     evaluation reward: 5.12\n",
      "episode: 2365   score: 5.0   memory length: 100000   epsilon: 0.13885048000856953    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 5.1\n",
      "episode: 2366   score: 12.0   memory length: 100000   epsilon: 0.1377238600085624    steps: 569    lr: 2.560000000000001e-06     evaluation reward: 5.19\n",
      "episode: 2367   score: 5.0   memory length: 100000   epsilon: 0.1371080800085585    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 5.17\n",
      "episode: 2368   score: 3.0   memory length: 100000   epsilon: 0.13665268000855563    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 5.12\n",
      "episode: 2369   score: 5.0   memory length: 100000   epsilon: 0.13603690000855173    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 5.11\n",
      "episode: 2370   score: 3.0   memory length: 100000   epsilon: 0.1356191200085491    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 5.1\n",
      "episode: 2371   score: 4.0   memory length: 100000   epsilon: 0.13510630000854584    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 5.11\n",
      "episode: 2372   score: 6.0   memory length: 100000   epsilon: 0.13440142000854138    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 5.13\n",
      "episode: 2373   score: 3.0   memory length: 100000   epsilon: 0.1339440400085385    steps: 231    lr: 2.560000000000001e-06     evaluation reward: 5.1\n",
      "episode: 2374   score: 4.0   memory length: 100000   epsilon: 0.13338568000853496    steps: 282    lr: 2.560000000000001e-06     evaluation reward: 5.11\n",
      "episode: 2375   score: 4.0   memory length: 100000   epsilon: 0.13291048000853195    steps: 240    lr: 2.560000000000001e-06     evaluation reward: 5.09\n",
      "episode: 2376   score: 5.0   memory length: 100000   epsilon: 0.13234024000852834    steps: 288    lr: 2.560000000000001e-06     evaluation reward: 5.11\n",
      "episode: 2377   score: 8.0   memory length: 100000   epsilon: 0.1317185200085244    steps: 314    lr: 2.560000000000001e-06     evaluation reward: 5.15\n",
      "episode: 2378   score: 3.0   memory length: 100000   epsilon: 0.13130272000852178    steps: 210    lr: 2.560000000000001e-06     evaluation reward: 5.13\n",
      "episode: 2379   score: 3.0   memory length: 100000   epsilon: 0.1308473200085189    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 5.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2380   score: 3.0   memory length: 100000   epsilon: 0.13039984000851607    steps: 226    lr: 2.560000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2381   score: 5.0   memory length: 100000   epsilon: 0.12979198000851222    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 5.09\n",
      "episode: 2382   score: 9.0   memory length: 100000   epsilon: 0.12882772000850612    steps: 487    lr: 2.560000000000001e-06     evaluation reward: 5.12\n",
      "episode: 2383   score: 5.0   memory length: 100000   epsilon: 0.128178280008502    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 5.11\n",
      "episode: 2384   score: 4.0   memory length: 100000   epsilon: 0.1276694200084988    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.09\n",
      "episode: 2385   score: 3.0   memory length: 100000   epsilon: 0.12724768000849612    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 5.09\n",
      "episode: 2386   score: 5.0   memory length: 100000   epsilon: 0.126598240008492    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 5.1\n",
      "episode: 2387   score: 3.0   memory length: 100000   epsilon: 0.12618244000848938    steps: 210    lr: 2.560000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2388   score: 8.0   memory length: 100000   epsilon: 0.12539242000848438    steps: 399    lr: 2.560000000000001e-06     evaluation reward: 5.12\n",
      "episode: 2389   score: 4.0   memory length: 100000   epsilon: 0.12488158000848198    steps: 258    lr: 2.560000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2390   score: 5.0   memory length: 100000   epsilon: 0.1242737200084824    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2391   score: 3.0   memory length: 100000   epsilon: 0.12385594000848268    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2392   score: 6.0   memory length: 100000   epsilon: 0.12315106000848317    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 5.1\n",
      "episode: 2393   score: 4.0   memory length: 100000   epsilon: 0.12267190000848349    steps: 242    lr: 2.560000000000001e-06     evaluation reward: 5.05\n",
      "episode: 2394   score: 5.0   memory length: 100000   epsilon: 0.1220660200084839    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 5.03\n",
      "episode: 2395   score: 10.0   memory length: 100000   epsilon: 0.12140866000848435    steps: 332    lr: 2.560000000000001e-06     evaluation reward: 5.06\n",
      "episode: 2396   score: 4.0   memory length: 100000   epsilon: 0.1208958400084847    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 5.05\n",
      "episode: 2397   score: 7.0   memory length: 100000   epsilon: 0.1201672000084852    steps: 368    lr: 2.560000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2398   score: 3.0   memory length: 100000   epsilon: 0.11971378000848551    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 5.05\n",
      "episode: 2399   score: 6.0   memory length: 100000   epsilon: 0.11904058000848597    steps: 340    lr: 2.560000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2400   score: 3.0   memory length: 100000   epsilon: 0.11858914000848628    steps: 228    lr: 2.560000000000001e-06     evaluation reward: 5.04\n",
      "episode: 2401   score: 6.0   memory length: 100000   epsilon: 0.11788426000848676    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 5.06\n",
      "episode: 2402   score: 4.0   memory length: 100000   epsilon: 0.11733580000848713    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2403   score: 3.0   memory length: 100000   epsilon: 0.11691802000848742    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2404   score: 5.0   memory length: 100000   epsilon: 0.11626660000848786    steps: 329    lr: 2.560000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2405   score: 4.0   memory length: 100000   epsilon: 0.11575774000848821    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2406   score: 5.0   memory length: 100000   epsilon: 0.11515582000848862    steps: 304    lr: 2.560000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2407   score: 6.0   memory length: 100000   epsilon: 0.11449054000848907    steps: 336    lr: 2.560000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2408   score: 3.0   memory length: 100000   epsilon: 0.11407276000848936    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 4.97\n",
      "episode: 2409   score: 5.0   memory length: 100000   epsilon: 0.11354212000848972    steps: 268    lr: 2.560000000000001e-06     evaluation reward: 4.97\n",
      "episode: 2410   score: 4.0   memory length: 100000   epsilon: 0.11306692000849004    steps: 240    lr: 2.560000000000001e-06     evaluation reward: 4.96\n",
      "episode: 2411   score: 5.0   memory length: 100000   epsilon: 0.11245906000849046    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 4.97\n",
      "episode: 2412   score: 4.0   memory length: 100000   epsilon: 0.11190466000849084    steps: 280    lr: 2.560000000000001e-06     evaluation reward: 4.94\n",
      "episode: 2413   score: 10.0   memory length: 100000   epsilon: 0.11086516000849155    steps: 525    lr: 2.560000000000001e-06     evaluation reward: 4.99\n",
      "episode: 2414   score: 7.0   memory length: 100000   epsilon: 0.11006524000849209    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 5.03\n",
      "episode: 2415   score: 4.0   memory length: 100000   epsilon: 0.10955242000849244    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2416   score: 3.0   memory length: 100000   epsilon: 0.10909900000849275    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2417   score: 4.0   memory length: 100000   epsilon: 0.10855054000849312    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 5.03\n",
      "episode: 2418   score: 8.0   memory length: 100000   epsilon: 0.1080060400084935    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2419   score: 4.0   memory length: 100000   epsilon: 0.10745956000849387    steps: 276    lr: 2.560000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2420   score: 4.0   memory length: 100000   epsilon: 0.10694476000849422    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2421   score: 7.0   memory length: 100000   epsilon: 0.10622008000849471    steps: 366    lr: 2.560000000000001e-06     evaluation reward: 5.1\n",
      "episode: 2422   score: 4.0   memory length: 100000   epsilon: 0.10571122000849506    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.12\n",
      "episode: 2423   score: 9.0   memory length: 100000   epsilon: 0.10482022000849567    steps: 450    lr: 2.560000000000001e-06     evaluation reward: 5.14\n",
      "episode: 2424   score: 6.0   memory length: 100000   epsilon: 0.10412326000849614    steps: 352    lr: 2.560000000000001e-06     evaluation reward: 5.15\n",
      "episode: 2425   score: 3.0   memory length: 100000   epsilon: 0.10367182000849645    steps: 228    lr: 2.560000000000001e-06     evaluation reward: 5.14\n",
      "episode: 2426   score: 10.0   memory length: 100000   epsilon: 0.10267984000849713    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 5.2\n",
      "episode: 2427   score: 4.0   memory length: 100000   epsilon: 0.1021274200084975    steps: 279    lr: 2.560000000000001e-06     evaluation reward: 5.21\n",
      "episode: 2428   score: 3.0   memory length: 100000   epsilon: 0.10170964000849779    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 5.18\n",
      "episode: 2429   score: 4.0   memory length: 100000   epsilon: 0.10119880000849814    steps: 258    lr: 2.560000000000001e-06     evaluation reward: 5.17\n",
      "episode: 2430   score: 8.0   memory length: 100000   epsilon: 0.10040878000849868    steps: 399    lr: 2.560000000000001e-06     evaluation reward: 5.21\n",
      "episode: 2431   score: 3.0   memory length: 100000   epsilon: 0.09999298000849896    steps: 210    lr: 2.560000000000001e-06     evaluation reward: 5.18\n",
      "episode: 2432   score: 4.0   memory length: 100000   epsilon: 0.09944452000849933    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 5.15\n",
      "episode: 2433   score: 10.0   memory length: 100000   epsilon: 0.09848224000849999    steps: 486    lr: 2.560000000000001e-06     evaluation reward: 5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2434   score: 6.0   memory length: 100000   epsilon: 0.09781696000850044    steps: 336    lr: 2.560000000000001e-06     evaluation reward: 5.2\n",
      "episode: 2435   score: 8.0   memory length: 100000   epsilon: 0.09695566000850103    steps: 435    lr: 2.560000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2436   score: 6.0   memory length: 100000   epsilon: 0.09624880000850151    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2437   score: 6.0   memory length: 100000   epsilon: 0.095532040008502    steps: 362    lr: 2.560000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2438   score: 6.0   memory length: 100000   epsilon: 0.09489844000850244    steps: 320    lr: 2.560000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2439   score: 5.0   memory length: 100000   epsilon: 0.09433216000850282    steps: 286    lr: 2.560000000000001e-06     evaluation reward: 5.2\n",
      "episode: 2440   score: 7.0   memory length: 100000   epsilon: 0.09356986000850334    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 5.22\n",
      "episode: 2441   score: 7.0   memory length: 100000   epsilon: 0.09280756000850386    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 5.24\n",
      "episode: 2442   score: 5.0   memory length: 100000   epsilon: 0.0921561400085043    steps: 329    lr: 2.560000000000001e-06     evaluation reward: 5.25\n",
      "episode: 2443   score: 8.0   memory length: 100000   epsilon: 0.09130078000850489    steps: 432    lr: 2.560000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2444   score: 8.0   memory length: 100000   epsilon: 0.09046324000850546    steps: 423    lr: 2.560000000000001e-06     evaluation reward: 5.32\n",
      "episode: 2445   score: 6.0   memory length: 100000   epsilon: 0.08976232000850594    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2446   score: 3.0   memory length: 100000   epsilon: 0.08934454000850622    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 5.29\n",
      "episode: 2447   score: 5.0   memory length: 100000   epsilon: 0.08870104000850666    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 5.28\n",
      "episode: 2448   score: 4.0   memory length: 100000   epsilon: 0.08815060000850704    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 5.28\n",
      "episode: 2449   score: 4.0   memory length: 100000   epsilon: 0.08763778000850739    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 5.27\n",
      "episode: 2450   score: 6.0   memory length: 100000   epsilon: 0.08699428000850783    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 5.29\n",
      "episode: 2451   score: 5.0   memory length: 100000   epsilon: 0.08638048000850825    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 5.25\n",
      "episode: 2452   score: 4.0   memory length: 100000   epsilon: 0.08587162000850859    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.24\n",
      "episode: 2453   score: 5.0   memory length: 100000   epsilon: 0.08526178000850901    steps: 308    lr: 2.560000000000001e-06     evaluation reward: 5.22\n",
      "episode: 2454   score: 4.0   memory length: 100000   epsilon: 0.08475292000850936    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.24\n",
      "episode: 2455   score: 8.0   memory length: 100000   epsilon: 0.08391736000850993    steps: 422    lr: 2.560000000000001e-06     evaluation reward: 5.27\n",
      "episode: 2456   score: 5.0   memory length: 100000   epsilon: 0.08323822000851039    steps: 343    lr: 2.560000000000001e-06     evaluation reward: 5.26\n",
      "episode: 2457   score: 6.0   memory length: 100000   epsilon: 0.08258878000851083    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 5.28\n",
      "episode: 2458   score: 6.0   memory length: 100000   epsilon: 0.08191558000851129    steps: 340    lr: 2.560000000000001e-06     evaluation reward: 5.29\n",
      "episode: 2459   score: 6.0   memory length: 100000   epsilon: 0.08124634000851175    steps: 338    lr: 2.560000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2460   score: 6.0   memory length: 100000   epsilon: 0.08062462000851217    steps: 314    lr: 2.560000000000001e-06     evaluation reward: 5.29\n",
      "episode: 2461   score: 4.0   memory length: 100000   epsilon: 0.08011576000851252    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.27\n",
      "episode: 2462   score: 3.0   memory length: 100000   epsilon: 0.07966036000851283    steps: 230    lr: 2.560000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2463   score: 9.0   memory length: 100000   epsilon: 0.07878718000851342    steps: 441    lr: 2.560000000000001e-06     evaluation reward: 5.28\n",
      "episode: 2464   score: 6.0   memory length: 100000   epsilon: 0.07812784000851387    steps: 333    lr: 2.560000000000001e-06     evaluation reward: 5.28\n",
      "episode: 2465   score: 6.0   memory length: 100000   epsilon: 0.07745860000851433    steps: 338    lr: 2.560000000000001e-06     evaluation reward: 5.29\n",
      "episode: 2466   score: 8.0   memory length: 100000   epsilon: 0.07687450000851473    steps: 295    lr: 2.560000000000001e-06     evaluation reward: 5.25\n",
      "episode: 2467   score: 7.0   memory length: 100000   epsilon: 0.07613596000851523    steps: 373    lr: 2.560000000000001e-06     evaluation reward: 5.27\n",
      "episode: 2468   score: 6.0   memory length: 100000   epsilon: 0.07550236000851566    steps: 320    lr: 2.560000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2469   score: 7.0   memory length: 100000   epsilon: 0.07477174000851616    steps: 369    lr: 2.560000000000001e-06     evaluation reward: 5.32\n",
      "episode: 2470   score: 5.0   memory length: 100000   epsilon: 0.07419952000851655    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 5.34\n",
      "episode: 2471   score: 5.0   memory length: 100000   epsilon: 0.07363126000851694    steps: 287    lr: 2.560000000000001e-06     evaluation reward: 5.35\n",
      "episode: 2472   score: 5.0   memory length: 100000   epsilon: 0.07302934000851735    steps: 304    lr: 2.560000000000001e-06     evaluation reward: 5.34\n",
      "episode: 2473   score: 7.0   memory length: 100000   epsilon: 0.07233040000851783    steps: 353    lr: 2.560000000000001e-06     evaluation reward: 5.38\n",
      "episode: 2474   score: 14.0   memory length: 100000   epsilon: 0.07102360000851872    steps: 660    lr: 2.560000000000001e-06     evaluation reward: 5.48\n",
      "episode: 2475   score: 3.0   memory length: 100000   epsilon: 0.070607800008519    steps: 210    lr: 2.560000000000001e-06     evaluation reward: 5.47\n",
      "episode: 2476   score: 5.0   memory length: 100000   epsilon: 0.06999994000851942    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 5.47\n",
      "episode: 2477   score: 5.0   memory length: 100000   epsilon: 0.06945544000851979    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 5.44\n",
      "episode: 2478   score: 5.0   memory length: 100000   epsilon: 0.06883966000852021    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 5.46\n",
      "episode: 2479   score: 5.0   memory length: 100000   epsilon: 0.0682694200085206    steps: 288    lr: 2.560000000000001e-06     evaluation reward: 5.48\n",
      "episode: 2480   score: 4.0   memory length: 100000   epsilon: 0.06772096000852097    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 5.49\n",
      "episode: 2481   score: 5.0   memory length: 100000   epsilon: 0.06714874000852136    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 5.49\n",
      "episode: 2482   score: 9.0   memory length: 100000   epsilon: 0.06615676000852204    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 5.49\n",
      "episode: 2483   score: 4.0   memory length: 100000   epsilon: 0.06563998000852239    steps: 261    lr: 2.560000000000001e-06     evaluation reward: 5.48\n",
      "episode: 2484   score: 5.0   memory length: 100000   epsilon: 0.06498262000852284    steps: 332    lr: 2.560000000000001e-06     evaluation reward: 5.49\n",
      "episode: 2485   score: 7.0   memory length: 100000   epsilon: 0.06425398000852334    steps: 368    lr: 2.560000000000001e-06     evaluation reward: 5.53\n",
      "episode: 2486   score: 9.0   memory length: 100000   epsilon: 0.06331150000852398    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 5.57\n",
      "episode: 2487   score: 4.0   memory length: 100000   epsilon: 0.06279670000852433    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 5.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2488   score: 5.0   memory length: 100000   epsilon: 0.06222844000852472    steps: 287    lr: 2.560000000000001e-06     evaluation reward: 5.55\n",
      "episode: 2489   score: 4.0   memory length: 100000   epsilon: 0.0616720600085251    steps: 281    lr: 2.560000000000001e-06     evaluation reward: 5.55\n",
      "episode: 2490   score: 6.0   memory length: 100000   epsilon: 0.060971140008525576    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 5.56\n",
      "episode: 2491   score: 5.0   memory length: 100000   epsilon: 0.06032170000852602    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 5.58\n",
      "episode: 2492   score: 10.0   memory length: 100000   epsilon: 0.05954554000852655    steps: 392    lr: 2.560000000000001e-06     evaluation reward: 5.62\n",
      "episode: 2493   score: 8.0   memory length: 100000   epsilon: 0.058773340008527075    steps: 390    lr: 2.560000000000001e-06     evaluation reward: 5.66\n",
      "episode: 2494   score: 8.0   memory length: 100000   epsilon: 0.057965500008527626    steps: 408    lr: 2.560000000000001e-06     evaluation reward: 5.69\n",
      "episode: 2495   score: 4.0   memory length: 100000   epsilon: 0.05745862000852797    steps: 256    lr: 2.560000000000001e-06     evaluation reward: 5.63\n",
      "episode: 2496   score: 10.0   memory length: 100000   epsilon: 0.05675572000852845    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 5.69\n",
      "episode: 2497   score: 3.0   memory length: 100000   epsilon: 0.056341900008528734    steps: 209    lr: 2.560000000000001e-06     evaluation reward: 5.65\n",
      "episode: 2498   score: 8.0   memory length: 100000   epsilon: 0.05555980000852927    steps: 395    lr: 2.560000000000001e-06     evaluation reward: 5.7\n",
      "episode: 2499   score: 6.0   memory length: 100000   epsilon: 0.05484700000852975    steps: 360    lr: 2.560000000000001e-06     evaluation reward: 5.7\n",
      "episode: 2500   score: 6.0   memory length: 100000   epsilon: 0.054215380008530184    steps: 319    lr: 2.560000000000001e-06     evaluation reward: 5.73\n",
      "episode: 2501   score: 7.0   memory length: 100000   epsilon: 0.05344912000853071    steps: 387    lr: 2.560000000000001e-06     evaluation reward: 5.74\n",
      "episode: 2502   score: 8.0   memory length: 100000   epsilon: 0.05263930000853126    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 5.78\n",
      "episode: 2503   score: 6.0   memory length: 100000   epsilon: 0.05196808000853172    steps: 339    lr: 2.560000000000001e-06     evaluation reward: 5.81\n",
      "episode: 2504   score: 9.0   memory length: 100000   epsilon: 0.051063220008532334    steps: 457    lr: 2.560000000000001e-06     evaluation reward: 5.85\n",
      "episode: 2505   score: 5.0   memory length: 100000   epsilon: 0.050491000008532724    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 5.86\n",
      "episode: 2506   score: 6.0   memory length: 100000   epsilon: 0.04986730000853315    steps: 315    lr: 2.560000000000001e-06     evaluation reward: 5.87\n",
      "episode: 2507   score: 7.0   memory length: 100000   epsilon: 0.04913074000853365    steps: 372    lr: 2.560000000000001e-06     evaluation reward: 5.88\n",
      "episode: 2508   score: 6.0   memory length: 100000   epsilon: 0.04846348000853411    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 5.91\n",
      "episode: 2509   score: 9.0   memory length: 100000   epsilon: 0.04762198000853468    steps: 425    lr: 2.560000000000001e-06     evaluation reward: 5.95\n",
      "episode: 2510   score: 6.0   memory length: 100000   epsilon: 0.04698838000853511    steps: 320    lr: 2.560000000000001e-06     evaluation reward: 5.97\n",
      "episode: 2511   score: 7.0   memory length: 100000   epsilon: 0.04622608000853563    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 5.99\n",
      "episode: 2512   score: 6.0   memory length: 100000   epsilon: 0.04551724000853612    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 6.01\n",
      "episode: 2513   score: 8.0   memory length: 100000   epsilon: 0.04472524000853666    steps: 400    lr: 2.560000000000001e-06     evaluation reward: 5.99\n",
      "episode: 2514   score: 3.0   memory length: 100000   epsilon: 0.04430746000853694    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 5.95\n",
      "episode: 2515   score: 5.0   memory length: 100000   epsilon: 0.04374118000853733    steps: 286    lr: 2.560000000000001e-06     evaluation reward: 5.96\n",
      "episode: 2516   score: 4.0   memory length: 100000   epsilon: 0.04322836000853768    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 5.97\n",
      "episode: 2517   score: 5.0   memory length: 100000   epsilon: 0.042662080008538064    steps: 286    lr: 2.560000000000001e-06     evaluation reward: 5.98\n",
      "episode: 2518   score: 5.0   memory length: 100000   epsilon: 0.04208392000853846    steps: 292    lr: 2.560000000000001e-06     evaluation reward: 5.95\n",
      "episode: 2519   score: 4.0   memory length: 100000   epsilon: 0.041533480008538834    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 5.95\n",
      "episode: 2520   score: 4.0   memory length: 100000   epsilon: 0.04102264000853918    steps: 258    lr: 2.560000000000001e-06     evaluation reward: 5.95\n",
      "episode: 2521   score: 6.0   memory length: 100000   epsilon: 0.04035538000853964    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 5.94\n",
      "episode: 2522   score: 7.0   memory length: 100000   epsilon: 0.03963268000854013    steps: 365    lr: 2.560000000000001e-06     evaluation reward: 5.97\n",
      "episode: 2523   score: 8.0   memory length: 100000   epsilon: 0.03891196000854062    steps: 364    lr: 2.560000000000001e-06     evaluation reward: 5.96\n",
      "episode: 2524   score: 4.0   memory length: 100000   epsilon: 0.03839914000854097    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 5.94\n",
      "episode: 2525   score: 5.0   memory length: 100000   epsilon: 0.037791280008541386    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 5.96\n",
      "episode: 2526   score: 6.0   memory length: 100000   epsilon: 0.03712204000854184    steps: 338    lr: 2.560000000000001e-06     evaluation reward: 5.92\n",
      "episode: 2527   score: 6.0   memory length: 100000   epsilon: 0.03636766000854236    steps: 381    lr: 2.560000000000001e-06     evaluation reward: 5.94\n",
      "episode: 2528   score: 5.0   memory length: 100000   epsilon: 0.03575782000854277    steps: 308    lr: 2.560000000000001e-06     evaluation reward: 5.96\n",
      "episode: 2529   score: 6.0   memory length: 100000   epsilon: 0.035052940008543254    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 5.98\n",
      "episode: 2530   score: 4.0   memory length: 100000   epsilon: 0.03450250000854363    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 5.94\n",
      "episode: 2531   score: 4.0   memory length: 100000   epsilon: 0.03398770000854398    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 5.95\n",
      "episode: 2532   score: 4.0   memory length: 100000   epsilon: 0.03347290000854433    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 5.95\n",
      "episode: 2533   score: 7.0   memory length: 100000   epsilon: 0.03277000000854481    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 5.92\n",
      "episode: 2534   score: 5.0   memory length: 100000   epsilon: 0.0322017400085452    steps: 287    lr: 2.560000000000001e-06     evaluation reward: 5.91\n",
      "episode: 2535   score: 5.0   memory length: 100000   epsilon: 0.03159784000854561    steps: 305    lr: 2.560000000000001e-06     evaluation reward: 5.88\n",
      "episode: 2536   score: 6.0   memory length: 100000   epsilon: 0.03092662000854607    steps: 339    lr: 2.560000000000001e-06     evaluation reward: 5.88\n",
      "episode: 2537   score: 4.0   memory length: 100000   epsilon: 0.030417760008546416    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.86\n",
      "episode: 2538   score: 11.0   memory length: 100000   epsilon: 0.02966536000854693    steps: 380    lr: 2.560000000000001e-06     evaluation reward: 5.91\n",
      "episode: 2539   score: 8.0   memory length: 100000   epsilon: 0.0288298000085475    steps: 422    lr: 2.560000000000001e-06     evaluation reward: 5.94\n",
      "episode: 2540   score: 6.0   memory length: 100000   epsilon: 0.028120960008547982    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 5.93\n",
      "episode: 2541   score: 5.0   memory length: 100000   epsilon: 0.027513100008548397    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 5.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2542   score: 6.0   memory length: 100000   epsilon: 0.02687752000854883    steps: 321    lr: 2.560000000000001e-06     evaluation reward: 5.92\n",
      "episode: 2543   score: 9.0   memory length: 100000   epsilon: 0.025962760008549454    steps: 462    lr: 2.560000000000001e-06     evaluation reward: 5.93\n",
      "episode: 2544   score: 7.0   memory length: 100000   epsilon: 0.025200460008549974    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 5.92\n",
      "episode: 2545   score: 4.0   memory length: 100000   epsilon: 0.024689620008550323    steps: 258    lr: 2.560000000000001e-06     evaluation reward: 5.9\n",
      "episode: 2546   score: 9.0   memory length: 100000   epsilon: 0.02370952000855099    steps: 495    lr: 2.560000000000001e-06     evaluation reward: 5.96\n",
      "episode: 2547   score: 8.0   memory length: 100000   epsilon: 0.02280268000855161    steps: 458    lr: 2.560000000000001e-06     evaluation reward: 5.99\n",
      "episode: 2548   score: 5.0   memory length: 100000   epsilon: 0.022194820008552024    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 6.0\n",
      "episode: 2549   score: 4.0   memory length: 100000   epsilon: 0.021678040008552377    steps: 261    lr: 2.560000000000001e-06     evaluation reward: 6.0\n",
      "episode: 2550   score: 4.0   memory length: 100000   epsilon: 0.0212028400085527    steps: 240    lr: 2.560000000000001e-06     evaluation reward: 5.98\n",
      "episode: 2551   score: 5.0   memory length: 100000   epsilon: 0.02063458000855309    steps: 287    lr: 2.560000000000001e-06     evaluation reward: 5.98\n",
      "episode: 2552   score: 7.0   memory length: 100000   epsilon: 0.019931680008553568    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 6.01\n",
      "episode: 2553   score: 7.0   memory length: 100000   epsilon: 0.019207000008554062    steps: 366    lr: 2.560000000000001e-06     evaluation reward: 6.03\n",
      "episode: 2554   score: 7.0   memory length: 100000   epsilon: 0.01840510000855461    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 6.06\n",
      "episode: 2555   score: 7.0   memory length: 100000   epsilon: 0.01772794000855507    steps: 342    lr: 2.560000000000001e-06     evaluation reward: 6.05\n",
      "episode: 2556   score: 4.0   memory length: 100000   epsilon: 0.017183440008555442    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 6.04\n",
      "episode: 2557   score: 9.0   memory length: 100000   epsilon: 0.016240960008556085    steps: 476    lr: 2.560000000000001e-06     evaluation reward: 6.07\n",
      "episode: 2558   score: 3.0   memory length: 100000   epsilon: 0.015783580008556397    steps: 231    lr: 2.560000000000001e-06     evaluation reward: 6.04\n",
      "episode: 2559   score: 5.0   memory length: 100000   epsilon: 0.015215320008556426    steps: 287    lr: 2.560000000000001e-06     evaluation reward: 6.03\n",
      "episode: 2560   score: 6.0   memory length: 100000   epsilon: 0.014548060008556296    steps: 337    lr: 2.560000000000001e-06     evaluation reward: 6.03\n",
      "episode: 2561   score: 3.0   memory length: 100000   epsilon: 0.014130280008556215    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 6.02\n",
      "episode: 2562   score: 7.0   memory length: 100000   epsilon: 0.013369960008556067    steps: 384    lr: 2.560000000000001e-06     evaluation reward: 6.06\n",
      "episode: 2563   score: 6.0   memory length: 100000   epsilon: 0.012748240008555947    steps: 314    lr: 2.560000000000001e-06     evaluation reward: 6.03\n",
      "episode: 2564   score: 5.0   memory length: 100000   epsilon: 0.012104740008555822    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 6.02\n",
      "episode: 2565   score: 5.0   memory length: 100000   epsilon: 0.011536480008555712    steps: 287    lr: 2.560000000000001e-06     evaluation reward: 6.01\n",
      "episode: 2566   score: 5.0   memory length: 100000   epsilon: 0.010922680008555593    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 5.98\n",
      "episode: 2567   score: 5.0   memory length: 100000   epsilon: 0.010308880008555473    steps: 310    lr: 2.560000000000001e-06     evaluation reward: 5.96\n",
      "episode: 2568   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 279    lr: 1.0240000000000005e-06     evaluation reward: 5.94\n",
      "episode: 2569   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 292    lr: 1.0240000000000005e-06     evaluation reward: 5.92\n",
      "episode: 2570   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 286    lr: 1.0240000000000005e-06     evaluation reward: 5.92\n",
      "episode: 2571   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 290    lr: 1.0240000000000005e-06     evaluation reward: 5.92\n",
      "episode: 2572   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 5.92\n",
      "episode: 2573   score: 11.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 520    lr: 1.0240000000000005e-06     evaluation reward: 5.96\n",
      "episode: 2574   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 398    lr: 1.0240000000000005e-06     evaluation reward: 5.88\n",
      "episode: 2575   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 5.9\n",
      "episode: 2576   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 437    lr: 1.0240000000000005e-06     evaluation reward: 5.93\n",
      "episode: 2577   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 290    lr: 1.0240000000000005e-06     evaluation reward: 5.93\n",
      "episode: 2578   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 5.93\n",
      "episode: 2579   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 291    lr: 1.0240000000000005e-06     evaluation reward: 5.93\n",
      "episode: 2580   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 5.94\n",
      "episode: 2581   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 355    lr: 1.0240000000000005e-06     evaluation reward: 5.95\n",
      "episode: 2582   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 258    lr: 1.0240000000000005e-06     evaluation reward: 5.9\n",
      "episode: 2583   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 345    lr: 1.0240000000000005e-06     evaluation reward: 5.93\n",
      "episode: 2584   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 326    lr: 1.0240000000000005e-06     evaluation reward: 5.93\n",
      "episode: 2585   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 258    lr: 1.0240000000000005e-06     evaluation reward: 5.9\n",
      "episode: 2586   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 286    lr: 1.0240000000000005e-06     evaluation reward: 5.86\n",
      "episode: 2587   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 280    lr: 1.0240000000000005e-06     evaluation reward: 5.86\n",
      "episode: 2588   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 280    lr: 1.0240000000000005e-06     evaluation reward: 5.85\n",
      "episode: 2589   score: 12.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 480    lr: 1.0240000000000005e-06     evaluation reward: 5.93\n",
      "episode: 2590   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 280    lr: 1.0240000000000005e-06     evaluation reward: 5.91\n",
      "episode: 2591   score: 12.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 480    lr: 1.0240000000000005e-06     evaluation reward: 5.98\n",
      "episode: 2592   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 5.93\n",
      "episode: 2593   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 336    lr: 1.0240000000000005e-06     evaluation reward: 5.91\n",
      "episode: 2594   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 291    lr: 1.0240000000000005e-06     evaluation reward: 5.88\n",
      "episode: 2595   score: 12.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 480    lr: 1.0240000000000005e-06     evaluation reward: 5.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2596   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 5.91\n",
      "episode: 2597   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 420    lr: 1.0240000000000005e-06     evaluation reward: 5.96\n",
      "episode: 2598   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 367    lr: 1.0240000000000005e-06     evaluation reward: 5.95\n",
      "episode: 2599   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 5.94\n",
      "episode: 2600   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 308    lr: 1.0240000000000005e-06     evaluation reward: 5.93\n",
      "episode: 2601   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 308    lr: 1.0240000000000005e-06     evaluation reward: 5.91\n",
      "episode: 2602   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 308    lr: 1.0240000000000005e-06     evaluation reward: 5.88\n",
      "episode: 2603   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 286    lr: 1.0240000000000005e-06     evaluation reward: 5.87\n",
      "episode: 2604   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 308    lr: 1.0240000000000005e-06     evaluation reward: 5.83\n",
      "episode: 2605   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 308    lr: 1.0240000000000005e-06     evaluation reward: 5.83\n",
      "episode: 2606   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 338    lr: 1.0240000000000005e-06     evaluation reward: 5.83\n",
      "episode: 2607   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 278    lr: 1.0240000000000005e-06     evaluation reward: 5.8\n",
      "episode: 2608   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 308    lr: 1.0240000000000005e-06     evaluation reward: 5.79\n",
      "episode: 2609   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 5.75\n",
      "episode: 2610   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 394    lr: 1.0240000000000005e-06     evaluation reward: 5.77\n",
      "episode: 2611   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 229    lr: 1.0240000000000005e-06     evaluation reward: 5.73\n",
      "episode: 2612   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 5.72\n",
      "episode: 2613   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 1.0240000000000005e-06     evaluation reward: 5.69\n",
      "episode: 2614   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 356    lr: 1.0240000000000005e-06     evaluation reward: 5.72\n",
      "episode: 2615   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 311    lr: 1.0240000000000005e-06     evaluation reward: 5.72\n",
      "episode: 2616   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 385    lr: 1.0240000000000005e-06     evaluation reward: 5.75\n",
      "episode: 2617   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 258    lr: 1.0240000000000005e-06     evaluation reward: 5.74\n",
      "episode: 2618   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 493    lr: 1.0240000000000005e-06     evaluation reward: 5.79\n",
      "episode: 2619   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 409    lr: 1.0240000000000005e-06     evaluation reward: 5.83\n",
      "episode: 2620   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 278    lr: 1.0240000000000005e-06     evaluation reward: 5.83\n",
      "episode: 2621   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 375    lr: 1.0240000000000005e-06     evaluation reward: 5.83\n",
      "episode: 2622   score: 12.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 556    lr: 1.0240000000000005e-06     evaluation reward: 5.88\n",
      "episode: 2623   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 339    lr: 1.0240000000000005e-06     evaluation reward: 5.86\n",
      "episode: 2624   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 422    lr: 1.0240000000000005e-06     evaluation reward: 5.9\n",
      "episode: 2625   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 5.9\n",
      "episode: 2626   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 280    lr: 1.0240000000000005e-06     evaluation reward: 5.88\n",
      "episode: 2627   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 438    lr: 1.0240000000000005e-06     evaluation reward: 5.91\n",
      "episode: 2628   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 377    lr: 1.0240000000000005e-06     evaluation reward: 5.92\n",
      "episode: 2629   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 434    lr: 1.0240000000000005e-06     evaluation reward: 5.94\n",
      "episode: 2630   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 5.95\n",
      "episode: 2631   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 396    lr: 1.0240000000000005e-06     evaluation reward: 5.97\n",
      "episode: 2632   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 1.0240000000000005e-06     evaluation reward: 5.98\n",
      "episode: 2633   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 498    lr: 1.0240000000000005e-06     evaluation reward: 6.0\n",
      "episode: 2634   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 6.0\n",
      "episode: 2635   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 6.0\n",
      "episode: 2636   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 5.99\n",
      "episode: 2637   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 359    lr: 1.0240000000000005e-06     evaluation reward: 6.01\n",
      "episode: 2638   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 422    lr: 1.0240000000000005e-06     evaluation reward: 5.98\n",
      "episode: 2639   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 387    lr: 1.0240000000000005e-06     evaluation reward: 5.97\n",
      "episode: 2640   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 438    lr: 1.0240000000000005e-06     evaluation reward: 5.99\n",
      "episode: 2641   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 311    lr: 1.0240000000000005e-06     evaluation reward: 5.99\n",
      "episode: 2642   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 407    lr: 1.0240000000000005e-06     evaluation reward: 6.0\n",
      "episode: 2643   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 382    lr: 1.0240000000000005e-06     evaluation reward: 5.98\n",
      "episode: 2644   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 5.96\n",
      "episode: 2645   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 5.97\n",
      "episode: 2646   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 374    lr: 1.0240000000000005e-06     evaluation reward: 5.95\n",
      "episode: 2647   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 5.92\n",
      "episode: 2648   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 286    lr: 1.0240000000000005e-06     evaluation reward: 5.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2649   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 280    lr: 1.0240000000000005e-06     evaluation reward: 5.92\n",
      "episode: 2650   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 424    lr: 1.0240000000000005e-06     evaluation reward: 5.96\n",
      "episode: 2651   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 5.96\n",
      "episode: 2652   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 295    lr: 1.0240000000000005e-06     evaluation reward: 5.93\n",
      "episode: 2653   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 422    lr: 1.0240000000000005e-06     evaluation reward: 5.94\n",
      "episode: 2654   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 338    lr: 1.0240000000000005e-06     evaluation reward: 5.93\n",
      "episode: 2655   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 311    lr: 1.0240000000000005e-06     evaluation reward: 5.91\n",
      "episode: 2656   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 422    lr: 1.0240000000000005e-06     evaluation reward: 5.95\n",
      "episode: 2657   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 462    lr: 1.0240000000000005e-06     evaluation reward: 5.94\n",
      "episode: 2658   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 459    lr: 1.0240000000000005e-06     evaluation reward: 6.0\n",
      "episode: 2659   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 522    lr: 1.0240000000000005e-06     evaluation reward: 6.05\n",
      "episode: 2660   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 369    lr: 1.0240000000000005e-06     evaluation reward: 6.06\n",
      "episode: 2661   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 287    lr: 1.0240000000000005e-06     evaluation reward: 6.08\n",
      "episode: 2662   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 368    lr: 1.0240000000000005e-06     evaluation reward: 6.08\n",
      "episode: 2663   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 344    lr: 1.0240000000000005e-06     evaluation reward: 6.07\n",
      "episode: 2664   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 385    lr: 1.0240000000000005e-06     evaluation reward: 6.09\n",
      "episode: 2665   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 440    lr: 1.0240000000000005e-06     evaluation reward: 6.12\n",
      "episode: 2666   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 309    lr: 1.0240000000000005e-06     evaluation reward: 6.12\n",
      "episode: 2667   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 386    lr: 1.0240000000000005e-06     evaluation reward: 6.14\n",
      "episode: 2668   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 356    lr: 1.0240000000000005e-06     evaluation reward: 6.16\n",
      "episode: 2669   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 304    lr: 1.0240000000000005e-06     evaluation reward: 6.16\n",
      "episode: 2670   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 369    lr: 1.0240000000000005e-06     evaluation reward: 6.17\n",
      "episode: 2671   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 422    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2672   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 354    lr: 1.0240000000000005e-06     evaluation reward: 6.21\n",
      "episode: 2673   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 395    lr: 1.0240000000000005e-06     evaluation reward: 6.18\n",
      "episode: 2674   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 422    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2675   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 385    lr: 1.0240000000000005e-06     evaluation reward: 6.22\n",
      "episode: 2676   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 320    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2677   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 455    lr: 1.0240000000000005e-06     evaluation reward: 6.23\n",
      "episode: 2678   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 345    lr: 1.0240000000000005e-06     evaluation reward: 6.23\n",
      "episode: 2679   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 276    lr: 1.0240000000000005e-06     evaluation reward: 6.22\n",
      "episode: 2680   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 346    lr: 1.0240000000000005e-06     evaluation reward: 6.24\n",
      "episode: 2681   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 276    lr: 1.0240000000000005e-06     evaluation reward: 6.22\n",
      "episode: 2682   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 386    lr: 1.0240000000000005e-06     evaluation reward: 6.25\n",
      "episode: 2683   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 274    lr: 1.0240000000000005e-06     evaluation reward: 6.22\n",
      "episode: 2684   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 374    lr: 1.0240000000000005e-06     evaluation reward: 6.23\n",
      "episode: 2685   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 325    lr: 1.0240000000000005e-06     evaluation reward: 6.24\n",
      "episode: 2686   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 276    lr: 1.0240000000000005e-06     evaluation reward: 6.23\n",
      "episode: 2687   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 356    lr: 1.0240000000000005e-06     evaluation reward: 6.25\n",
      "episode: 2688   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 371    lr: 1.0240000000000005e-06     evaluation reward: 6.28\n",
      "episode: 2689   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 426    lr: 1.0240000000000005e-06     evaluation reward: 6.23\n",
      "episode: 2690   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 349    lr: 1.0240000000000005e-06     evaluation reward: 6.25\n",
      "episode: 2691   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 438    lr: 1.0240000000000005e-06     evaluation reward: 6.21\n",
      "episode: 2692   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 258    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2693   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 6.19\n",
      "episode: 2694   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 276    lr: 1.0240000000000005e-06     evaluation reward: 6.18\n",
      "episode: 2695   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 6.11\n",
      "episode: 2696   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 230    lr: 1.0240000000000005e-06     evaluation reward: 6.09\n",
      "episode: 2697   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 258    lr: 1.0240000000000005e-06     evaluation reward: 6.05\n",
      "episode: 2698   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 351    lr: 1.0240000000000005e-06     evaluation reward: 6.04\n",
      "episode: 2699   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 474    lr: 1.0240000000000005e-06     evaluation reward: 6.08\n",
      "episode: 2700   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 261    lr: 1.0240000000000005e-06     evaluation reward: 6.07\n",
      "episode: 2701   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 401    lr: 1.0240000000000005e-06     evaluation reward: 6.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2702   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 369    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2703   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2704   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 325    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2705   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 348    lr: 1.0240000000000005e-06     evaluation reward: 6.11\n",
      "episode: 2706   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2707   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 6.11\n",
      "episode: 2708   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 329    lr: 1.0240000000000005e-06     evaluation reward: 6.11\n",
      "episode: 2709   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 276    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2710   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 378    lr: 1.0240000000000005e-06     evaluation reward: 6.08\n",
      "episode: 2711   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 338    lr: 1.0240000000000005e-06     evaluation reward: 6.11\n",
      "episode: 2712   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 472    lr: 1.0240000000000005e-06     evaluation reward: 6.15\n",
      "episode: 2713   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 393    lr: 1.0240000000000005e-06     evaluation reward: 6.17\n",
      "episode: 2714   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 6.16\n",
      "episode: 2715   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 323    lr: 1.0240000000000005e-06     evaluation reward: 6.16\n",
      "episode: 2716   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 276    lr: 1.0240000000000005e-06     evaluation reward: 6.13\n",
      "episode: 2717   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 354    lr: 1.0240000000000005e-06     evaluation reward: 6.15\n",
      "episode: 2718   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 521    lr: 1.0240000000000005e-06     evaluation reward: 6.14\n",
      "episode: 2719   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 378    lr: 1.0240000000000005e-06     evaluation reward: 6.12\n",
      "episode: 2720   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 291    lr: 1.0240000000000005e-06     evaluation reward: 6.13\n",
      "episode: 2721   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 386    lr: 1.0240000000000005e-06     evaluation reward: 6.14\n",
      "episode: 2722   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 422    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2723   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 338    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2724   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 356    lr: 1.0240000000000005e-06     evaluation reward: 6.08\n",
      "episode: 2725   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 375    lr: 1.0240000000000005e-06     evaluation reward: 6.09\n",
      "episode: 2726   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 357    lr: 1.0240000000000005e-06     evaluation reward: 6.11\n",
      "episode: 2727   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 325    lr: 1.0240000000000005e-06     evaluation reward: 6.07\n",
      "episode: 2728   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 438    lr: 1.0240000000000005e-06     evaluation reward: 6.09\n",
      "episode: 2729   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 409    lr: 1.0240000000000005e-06     evaluation reward: 6.08\n",
      "episode: 2730   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 6.11\n",
      "episode: 2731   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 280    lr: 1.0240000000000005e-06     evaluation reward: 6.09\n",
      "episode: 2732   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 355    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2733   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2734   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 376    lr: 1.0240000000000005e-06     evaluation reward: 6.12\n",
      "episode: 2735   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 280    lr: 1.0240000000000005e-06     evaluation reward: 6.11\n",
      "episode: 2736   score: 12.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 454    lr: 1.0240000000000005e-06     evaluation reward: 6.18\n",
      "episode: 2737   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 247    lr: 1.0240000000000005e-06     evaluation reward: 6.15\n",
      "episode: 2738   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 341    lr: 1.0240000000000005e-06     evaluation reward: 6.13\n",
      "episode: 2739   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 369    lr: 1.0240000000000005e-06     evaluation reward: 6.12\n",
      "episode: 2740   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 479    lr: 1.0240000000000005e-06     evaluation reward: 6.14\n",
      "episode: 2741   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 6.14\n",
      "episode: 2742   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 6.12\n",
      "episode: 2743   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 347    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2744   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2745   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 291    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2746   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 344    lr: 1.0240000000000005e-06     evaluation reward: 6.08\n",
      "episode: 2747   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 312    lr: 1.0240000000000005e-06     evaluation reward: 6.08\n",
      "episode: 2748   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 295    lr: 1.0240000000000005e-06     evaluation reward: 6.07\n",
      "episode: 2749   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 341    lr: 1.0240000000000005e-06     evaluation reward: 6.09\n",
      "episode: 2750   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 478    lr: 1.0240000000000005e-06     evaluation reward: 6.09\n",
      "episode: 2751   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 378    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2752   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 372    lr: 1.0240000000000005e-06     evaluation reward: 6.12\n",
      "episode: 2753   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 375    lr: 1.0240000000000005e-06     evaluation reward: 6.1\n",
      "episode: 2754   score: 12.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 542    lr: 1.0240000000000005e-06     evaluation reward: 6.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2755   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 375    lr: 1.0240000000000005e-06     evaluation reward: 6.17\n",
      "episode: 2756   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 347    lr: 1.0240000000000005e-06     evaluation reward: 6.14\n",
      "episode: 2757   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 424    lr: 1.0240000000000005e-06     evaluation reward: 6.13\n",
      "episode: 2758   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 466    lr: 1.0240000000000005e-06     evaluation reward: 6.13\n",
      "episode: 2759   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 294    lr: 1.0240000000000005e-06     evaluation reward: 6.07\n",
      "episode: 2760   score: 13.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 492    lr: 1.0240000000000005e-06     evaluation reward: 6.13\n",
      "episode: 2761   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 393    lr: 1.0240000000000005e-06     evaluation reward: 6.14\n",
      "episode: 2762   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 469    lr: 1.0240000000000005e-06     evaluation reward: 6.15\n",
      "episode: 2763   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 393    lr: 1.0240000000000005e-06     evaluation reward: 6.16\n",
      "episode: 2764   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 373    lr: 1.0240000000000005e-06     evaluation reward: 6.16\n",
      "episode: 2765   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 324    lr: 1.0240000000000005e-06     evaluation reward: 6.13\n",
      "episode: 2766   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 323    lr: 1.0240000000000005e-06     evaluation reward: 6.13\n",
      "episode: 2767   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 365    lr: 1.0240000000000005e-06     evaluation reward: 6.13\n",
      "episode: 2768   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 302    lr: 1.0240000000000005e-06     evaluation reward: 6.11\n",
      "episode: 2769   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 400    lr: 1.0240000000000005e-06     evaluation reward: 6.13\n",
      "episode: 2770   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 323    lr: 1.0240000000000005e-06     evaluation reward: 6.12\n",
      "episode: 2771   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 505    lr: 1.0240000000000005e-06     evaluation reward: 6.13\n",
      "episode: 2772   score: 11.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 462    lr: 1.0240000000000005e-06     evaluation reward: 6.18\n",
      "episode: 2773   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 276    lr: 1.0240000000000005e-06     evaluation reward: 6.14\n",
      "episode: 2774   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 373    lr: 1.0240000000000005e-06     evaluation reward: 6.12\n",
      "episode: 2775   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 424    lr: 1.0240000000000005e-06     evaluation reward: 6.12\n",
      "episode: 2776   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 360    lr: 1.0240000000000005e-06     evaluation reward: 6.12\n",
      "episode: 2777   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 348    lr: 1.0240000000000005e-06     evaluation reward: 6.09\n",
      "episode: 2778   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 290    lr: 1.0240000000000005e-06     evaluation reward: 6.09\n",
      "episode: 2779   score: 16.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 419    lr: 1.0240000000000005e-06     evaluation reward: 6.21\n",
      "episode: 2780   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 292    lr: 1.0240000000000005e-06     evaluation reward: 6.19\n",
      "episode: 2781   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 398    lr: 1.0240000000000005e-06     evaluation reward: 6.23\n",
      "episode: 2782   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 279    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2783   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 276    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2784   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 325    lr: 1.0240000000000005e-06     evaluation reward: 6.19\n",
      "episode: 2785   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 324    lr: 1.0240000000000005e-06     evaluation reward: 6.19\n",
      "episode: 2786   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 338    lr: 1.0240000000000005e-06     evaluation reward: 6.21\n",
      "episode: 2787   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2788   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 430    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2789   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 489    lr: 1.0240000000000005e-06     evaluation reward: 6.22\n",
      "episode: 2790   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 347    lr: 1.0240000000000005e-06     evaluation reward: 6.21\n",
      "episode: 2791   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 376    lr: 1.0240000000000005e-06     evaluation reward: 6.19\n",
      "episode: 2792   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 363    lr: 1.0240000000000005e-06     evaluation reward: 6.21\n",
      "episode: 2793   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 358    lr: 1.0240000000000005e-06     evaluation reward: 6.22\n",
      "episode: 2794   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 1.0240000000000005e-06     evaluation reward: 6.23\n",
      "episode: 2795   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 458    lr: 1.0240000000000005e-06     evaluation reward: 6.27\n",
      "episode: 2796   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 392    lr: 1.0240000000000005e-06     evaluation reward: 6.3\n",
      "episode: 2797   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 450    lr: 1.0240000000000005e-06     evaluation reward: 6.34\n",
      "episode: 2798   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 353    lr: 1.0240000000000005e-06     evaluation reward: 6.34\n",
      "episode: 2799   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 375    lr: 1.0240000000000005e-06     evaluation reward: 6.31\n",
      "episode: 2800   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 6.32\n",
      "episode: 2801   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 6.3\n",
      "episode: 2802   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 356    lr: 1.0240000000000005e-06     evaluation reward: 6.3\n",
      "episode: 2803   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 301    lr: 1.0240000000000005e-06     evaluation reward: 6.29\n",
      "episode: 2804   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 411    lr: 1.0240000000000005e-06     evaluation reward: 6.31\n",
      "episode: 2805   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 508    lr: 1.0240000000000005e-06     evaluation reward: 6.35\n",
      "episode: 2806   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 378    lr: 1.0240000000000005e-06     evaluation reward: 6.36\n",
      "episode: 2807   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 358    lr: 1.0240000000000005e-06     evaluation reward: 6.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2808   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 323    lr: 1.0240000000000005e-06     evaluation reward: 6.37\n",
      "episode: 2809   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 297    lr: 1.0240000000000005e-06     evaluation reward: 6.37\n",
      "episode: 2810   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 455    lr: 1.0240000000000005e-06     evaluation reward: 6.39\n",
      "episode: 2811   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 363    lr: 1.0240000000000005e-06     evaluation reward: 6.39\n",
      "episode: 2812   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 231    lr: 1.0240000000000005e-06     evaluation reward: 6.33\n",
      "episode: 2813   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 325    lr: 1.0240000000000005e-06     evaluation reward: 6.31\n",
      "episode: 2814   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 302    lr: 1.0240000000000005e-06     evaluation reward: 6.3\n",
      "episode: 2815   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 405    lr: 1.0240000000000005e-06     evaluation reward: 6.33\n",
      "episode: 2816   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 376    lr: 1.0240000000000005e-06     evaluation reward: 6.35\n",
      "episode: 2817   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 231    lr: 1.0240000000000005e-06     evaluation reward: 6.32\n",
      "episode: 2818   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 1.0240000000000005e-06     evaluation reward: 6.28\n",
      "episode: 2819   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 231    lr: 1.0240000000000005e-06     evaluation reward: 6.25\n",
      "episode: 2820   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 314    lr: 1.0240000000000005e-06     evaluation reward: 6.25\n",
      "episode: 2821   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 322    lr: 1.0240000000000005e-06     evaluation reward: 6.24\n",
      "episode: 2822   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 427    lr: 1.0240000000000005e-06     evaluation reward: 6.24\n",
      "episode: 2823   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 302    lr: 1.0240000000000005e-06     evaluation reward: 6.22\n",
      "episode: 2824   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 1.0240000000000005e-06     evaluation reward: 6.21\n",
      "episode: 2825   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 444    lr: 1.0240000000000005e-06     evaluation reward: 6.23\n",
      "episode: 2826   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 302    lr: 1.0240000000000005e-06     evaluation reward: 6.21\n",
      "episode: 2827   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 324    lr: 1.0240000000000005e-06     evaluation reward: 6.21\n",
      "episode: 2828   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 376    lr: 1.0240000000000005e-06     evaluation reward: 6.19\n",
      "episode: 2829   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 364    lr: 1.0240000000000005e-06     evaluation reward: 6.18\n",
      "episode: 2830   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 278    lr: 1.0240000000000005e-06     evaluation reward: 6.14\n",
      "episode: 2831   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 432    lr: 1.0240000000000005e-06     evaluation reward: 6.19\n",
      "episode: 2832   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 297    lr: 1.0240000000000005e-06     evaluation reward: 6.17\n",
      "episode: 2833   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 523    lr: 1.0240000000000005e-06     evaluation reward: 6.18\n",
      "episode: 2834   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 1.0240000000000005e-06     evaluation reward: 6.16\n",
      "episode: 2835   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 430    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2836   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 507    lr: 1.0240000000000005e-06     evaluation reward: 6.18\n",
      "episode: 2837   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 353    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2838   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 393    lr: 1.0240000000000005e-06     evaluation reward: 6.22\n",
      "episode: 2839   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 350    lr: 1.0240000000000005e-06     evaluation reward: 6.21\n",
      "episode: 2840   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 6.16\n",
      "episode: 2841   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 436    lr: 1.0240000000000005e-06     evaluation reward: 6.19\n",
      "episode: 2842   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 377    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2843   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 325    lr: 1.0240000000000005e-06     evaluation reward: 6.2\n",
      "episode: 2844   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 410    lr: 1.0240000000000005e-06     evaluation reward: 6.22\n",
      "episode: 2845   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 336    lr: 1.0240000000000005e-06     evaluation reward: 6.24\n",
      "episode: 2846   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 274    lr: 1.0240000000000005e-06     evaluation reward: 6.24\n",
      "episode: 2847   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 1.0240000000000005e-06     evaluation reward: 6.24\n",
      "episode: 2848   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 278    lr: 1.0240000000000005e-06     evaluation reward: 6.24\n",
      "episode: 2849   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 386    lr: 1.0240000000000005e-06     evaluation reward: 6.25\n",
      "episode: 2850   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 6.22\n",
      "episode: 2851   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 344    lr: 4.0960000000000023e-07     evaluation reward: 6.21\n",
      "episode: 2852   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 6.2\n",
      "episode: 2853   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 309    lr: 4.0960000000000023e-07     evaluation reward: 6.19\n",
      "episode: 2854   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 297    lr: 4.0960000000000023e-07     evaluation reward: 6.11\n",
      "episode: 2855   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 320    lr: 4.0960000000000023e-07     evaluation reward: 6.11\n",
      "episode: 2856   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 6.11\n",
      "episode: 2857   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 6.09\n",
      "episode: 2858   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 6.05\n",
      "episode: 2859   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 6.06\n",
      "episode: 2860   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 297    lr: 4.0960000000000023e-07     evaluation reward: 5.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2861   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.96\n",
      "episode: 2862   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 376    lr: 4.0960000000000023e-07     evaluation reward: 5.94\n",
      "episode: 2863   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 377    lr: 4.0960000000000023e-07     evaluation reward: 5.94\n",
      "episode: 2864   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 231    lr: 4.0960000000000023e-07     evaluation reward: 5.9\n",
      "episode: 2865   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.9\n",
      "episode: 2866   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 376    lr: 4.0960000000000023e-07     evaluation reward: 5.91\n",
      "episode: 2867   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 5.89\n",
      "episode: 2868   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 5.9\n",
      "episode: 2869   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 5.88\n",
      "episode: 2870   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 297    lr: 4.0960000000000023e-07     evaluation reward: 5.87\n",
      "episode: 2871   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.83\n",
      "episode: 2872   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 5.77\n",
      "episode: 2873   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.78\n",
      "episode: 2874   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 5.77\n",
      "episode: 2875   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 345    lr: 4.0960000000000023e-07     evaluation reward: 5.75\n",
      "episode: 2876   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 363    lr: 4.0960000000000023e-07     evaluation reward: 5.75\n",
      "episode: 2877   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 364    lr: 4.0960000000000023e-07     evaluation reward: 5.77\n",
      "episode: 2878   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 433    lr: 4.0960000000000023e-07     evaluation reward: 5.8\n",
      "episode: 2879   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 302    lr: 4.0960000000000023e-07     evaluation reward: 5.68\n",
      "episode: 2880   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.68\n",
      "episode: 2881   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 353    lr: 4.0960000000000023e-07     evaluation reward: 5.66\n",
      "episode: 2882   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 370    lr: 4.0960000000000023e-07     evaluation reward: 5.69\n",
      "episode: 2883   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 4.0960000000000023e-07     evaluation reward: 5.7\n",
      "episode: 2884   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 297    lr: 4.0960000000000023e-07     evaluation reward: 5.69\n",
      "episode: 2885   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 5.69\n",
      "episode: 2886   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 379    lr: 4.0960000000000023e-07     evaluation reward: 5.69\n",
      "episode: 2887   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 356    lr: 4.0960000000000023e-07     evaluation reward: 5.7\n",
      "episode: 2888   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 259    lr: 4.0960000000000023e-07     evaluation reward: 5.67\n",
      "episode: 2889   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.63\n",
      "episode: 2890   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 5.63\n",
      "episode: 2891   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 332    lr: 4.0960000000000023e-07     evaluation reward: 5.63\n",
      "episode: 2892   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 450    lr: 4.0960000000000023e-07     evaluation reward: 5.66\n",
      "episode: 2893   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 298    lr: 4.0960000000000023e-07     evaluation reward: 5.64\n",
      "episode: 2894   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.64\n",
      "episode: 2895   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 451    lr: 4.0960000000000023e-07     evaluation reward: 5.62\n",
      "episode: 2896   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 354    lr: 4.0960000000000023e-07     evaluation reward: 5.62\n",
      "episode: 2897   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 288    lr: 4.0960000000000023e-07     evaluation reward: 5.59\n",
      "episode: 2898   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 298    lr: 4.0960000000000023e-07     evaluation reward: 5.57\n",
      "episode: 2899   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 279    lr: 4.0960000000000023e-07     evaluation reward: 5.55\n",
      "episode: 2900   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 309    lr: 4.0960000000000023e-07     evaluation reward: 5.55\n",
      "episode: 2901   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.55\n",
      "episode: 2902   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 292    lr: 4.0960000000000023e-07     evaluation reward: 5.54\n",
      "episode: 2903   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 380    lr: 4.0960000000000023e-07     evaluation reward: 5.56\n",
      "episode: 2904   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 351    lr: 4.0960000000000023e-07     evaluation reward: 5.55\n",
      "episode: 2905   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 309    lr: 4.0960000000000023e-07     evaluation reward: 5.5\n",
      "episode: 2906   score: 13.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 456    lr: 4.0960000000000023e-07     evaluation reward: 5.57\n",
      "episode: 2907   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 243    lr: 4.0960000000000023e-07     evaluation reward: 5.55\n",
      "episode: 2908   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 393    lr: 4.0960000000000023e-07     evaluation reward: 5.58\n",
      "episode: 2909   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.59\n",
      "episode: 2910   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.56\n",
      "episode: 2911   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 320    lr: 4.0960000000000023e-07     evaluation reward: 5.56\n",
      "episode: 2912   score: 11.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 455    lr: 4.0960000000000023e-07     evaluation reward: 5.64\n",
      "episode: 2913   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 292    lr: 4.0960000000000023e-07     evaluation reward: 5.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2914   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.65\n",
      "episode: 2915   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 344    lr: 4.0960000000000023e-07     evaluation reward: 5.62\n",
      "episode: 2916   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 376    lr: 4.0960000000000023e-07     evaluation reward: 5.62\n",
      "episode: 2917   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 403    lr: 4.0960000000000023e-07     evaluation reward: 5.67\n",
      "episode: 2918   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.67\n",
      "episode: 2919   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 278    lr: 4.0960000000000023e-07     evaluation reward: 5.68\n",
      "episode: 2920   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 368    lr: 4.0960000000000023e-07     evaluation reward: 5.7\n",
      "episode: 2921   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 476    lr: 4.0960000000000023e-07     evaluation reward: 5.73\n",
      "episode: 2922   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 441    lr: 4.0960000000000023e-07     evaluation reward: 5.73\n",
      "episode: 2923   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 440    lr: 4.0960000000000023e-07     evaluation reward: 5.77\n",
      "episode: 2924   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 388    lr: 4.0960000000000023e-07     evaluation reward: 5.79\n",
      "episode: 2925   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 368    lr: 4.0960000000000023e-07     evaluation reward: 5.78\n",
      "episode: 2926   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.79\n",
      "episode: 2927   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.79\n",
      "episode: 2928   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 302    lr: 4.0960000000000023e-07     evaluation reward: 5.77\n",
      "episode: 2929   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 4.0960000000000023e-07     evaluation reward: 5.76\n",
      "episode: 2930   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 422    lr: 4.0960000000000023e-07     evaluation reward: 5.8\n",
      "episode: 2931   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 286    lr: 4.0960000000000023e-07     evaluation reward: 5.76\n",
      "episode: 2932   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 371    lr: 4.0960000000000023e-07     evaluation reward: 5.79\n",
      "episode: 2933   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 336    lr: 4.0960000000000023e-07     evaluation reward: 5.75\n",
      "episode: 2934   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 428    lr: 4.0960000000000023e-07     evaluation reward: 5.78\n",
      "episode: 2935   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 240    lr: 4.0960000000000023e-07     evaluation reward: 5.74\n",
      "episode: 2936   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 435    lr: 4.0960000000000023e-07     evaluation reward: 5.72\n",
      "episode: 2937   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 400    lr: 4.0960000000000023e-07     evaluation reward: 5.75\n",
      "episode: 2938   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 457    lr: 4.0960000000000023e-07     evaluation reward: 5.76\n",
      "episode: 2939   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.76\n",
      "episode: 2940   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 298    lr: 4.0960000000000023e-07     evaluation reward: 5.75\n",
      "episode: 2941   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 286    lr: 4.0960000000000023e-07     evaluation reward: 5.72\n",
      "episode: 2942   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.71\n",
      "episode: 2943   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 320    lr: 4.0960000000000023e-07     evaluation reward: 5.72\n",
      "episode: 2944   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 382    lr: 4.0960000000000023e-07     evaluation reward: 5.72\n",
      "episode: 2945   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.7\n",
      "episode: 2946   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 404    lr: 4.0960000000000023e-07     evaluation reward: 5.72\n",
      "episode: 2947   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 298    lr: 4.0960000000000023e-07     evaluation reward: 5.71\n",
      "episode: 2948   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.72\n",
      "episode: 2949   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 494    lr: 4.0960000000000023e-07     evaluation reward: 5.75\n",
      "episode: 2950   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 402    lr: 4.0960000000000023e-07     evaluation reward: 5.77\n",
      "episode: 2951   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 448    lr: 4.0960000000000023e-07     evaluation reward: 5.8\n",
      "episode: 2952   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 298    lr: 4.0960000000000023e-07     evaluation reward: 5.79\n",
      "episode: 2953   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 348    lr: 4.0960000000000023e-07     evaluation reward: 5.79\n",
      "episode: 2954   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 259    lr: 4.0960000000000023e-07     evaluation reward: 5.79\n",
      "episode: 2955   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 351    lr: 4.0960000000000023e-07     evaluation reward: 5.79\n",
      "episode: 2956   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 230    lr: 4.0960000000000023e-07     evaluation reward: 5.77\n",
      "episode: 2957   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.77\n",
      "episode: 2958   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 413    lr: 4.0960000000000023e-07     evaluation reward: 5.81\n",
      "episode: 2959   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 410    lr: 4.0960000000000023e-07     evaluation reward: 5.84\n",
      "episode: 2960   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 357    lr: 4.0960000000000023e-07     evaluation reward: 5.86\n",
      "episode: 2961   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 298    lr: 4.0960000000000023e-07     evaluation reward: 5.85\n",
      "episode: 2962   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 298    lr: 4.0960000000000023e-07     evaluation reward: 5.83\n",
      "episode: 2963   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 336    lr: 4.0960000000000023e-07     evaluation reward: 5.83\n",
      "episode: 2964   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 382    lr: 4.0960000000000023e-07     evaluation reward: 5.86\n",
      "episode: 2965   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 298    lr: 4.0960000000000023e-07     evaluation reward: 5.85\n",
      "episode: 2966   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 330    lr: 4.0960000000000023e-07     evaluation reward: 5.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2967   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 289    lr: 4.0960000000000023e-07     evaluation reward: 5.84\n",
      "episode: 2968   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 302    lr: 4.0960000000000023e-07     evaluation reward: 5.83\n",
      "episode: 2969   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 250    lr: 4.0960000000000023e-07     evaluation reward: 5.81\n",
      "episode: 2970   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 397    lr: 4.0960000000000023e-07     evaluation reward: 5.83\n",
      "episode: 2971   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 280    lr: 4.0960000000000023e-07     evaluation reward: 5.82\n",
      "episode: 2972   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 352    lr: 4.0960000000000023e-07     evaluation reward: 5.84\n",
      "episode: 2973   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 311    lr: 4.0960000000000023e-07     evaluation reward: 5.84\n",
      "episode: 2974   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 330    lr: 4.0960000000000023e-07     evaluation reward: 5.84\n",
      "episode: 2975   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 337    lr: 4.0960000000000023e-07     evaluation reward: 5.85\n",
      "episode: 2976   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 329    lr: 4.0960000000000023e-07     evaluation reward: 5.84\n",
      "episode: 2977   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 358    lr: 4.0960000000000023e-07     evaluation reward: 5.83\n",
      "episode: 2978   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 331    lr: 4.0960000000000023e-07     evaluation reward: 5.8\n",
      "episode: 2979   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 298    lr: 4.0960000000000023e-07     evaluation reward: 5.8\n",
      "episode: 2980   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 298    lr: 4.0960000000000023e-07     evaluation reward: 5.79\n",
      "episode: 2981   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 307    lr: 4.0960000000000023e-07     evaluation reward: 5.78\n",
      "episode: 2982   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 403    lr: 4.0960000000000023e-07     evaluation reward: 5.79\n",
      "episode: 2983   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 356    lr: 4.0960000000000023e-07     evaluation reward: 5.8\n",
      "episode: 2984   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 378    lr: 4.0960000000000023e-07     evaluation reward: 5.82\n",
      "episode: 2985   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 327    lr: 4.0960000000000023e-07     evaluation reward: 5.83\n",
      "episode: 2986   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 403    lr: 4.0960000000000023e-07     evaluation reward: 5.84\n",
      "episode: 2987   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 4.0960000000000023e-07     evaluation reward: 5.83\n",
      "episode: 2988   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 302    lr: 4.0960000000000023e-07     evaluation reward: 5.83\n",
      "episode: 2989   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 448    lr: 4.0960000000000023e-07     evaluation reward: 5.86\n",
      "episode: 2990   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 454    lr: 4.0960000000000023e-07     evaluation reward: 5.89\n",
      "episode: 2991   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 307    lr: 4.0960000000000023e-07     evaluation reward: 5.88\n",
      "episode: 2992   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 330    lr: 4.0960000000000023e-07     evaluation reward: 5.84\n",
      "episode: 2993   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 357    lr: 4.0960000000000023e-07     evaluation reward: 5.86\n",
      "episode: 2994   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 446    lr: 4.0960000000000023e-07     evaluation reward: 5.89\n",
      "episode: 2995   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 422    lr: 4.0960000000000023e-07     evaluation reward: 5.9\n",
      "episode: 2996   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 313    lr: 4.0960000000000023e-07     evaluation reward: 5.89\n",
      "episode: 2997   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 381    lr: 4.0960000000000023e-07     evaluation reward: 5.9\n",
      "episode: 2998   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 482    lr: 4.0960000000000023e-07     evaluation reward: 5.96\n",
      "episode: 2999   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 302    lr: 4.0960000000000023e-07     evaluation reward: 5.96\n",
      "episode: 3000   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 466    lr: 4.0960000000000023e-07     evaluation reward: 6.0\n",
      "episode: 3001   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 387    lr: 4.0960000000000023e-07     evaluation reward: 6.03\n",
      "episode: 3002   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 483    lr: 4.0960000000000023e-07     evaluation reward: 6.08\n",
      "episode: 3003   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 340    lr: 4.0960000000000023e-07     evaluation reward: 6.08\n",
      "episode: 3004   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 363    lr: 4.0960000000000023e-07     evaluation reward: 6.08\n",
      "episode: 3005   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 312    lr: 4.0960000000000023e-07     evaluation reward: 6.08\n",
      "episode: 3006   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 381    lr: 4.0960000000000023e-07     evaluation reward: 6.01\n",
      "episode: 3007   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 337    lr: 4.0960000000000023e-07     evaluation reward: 6.03\n",
      "episode: 3008   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 336    lr: 4.0960000000000023e-07     evaluation reward: 6.01\n",
      "episode: 3009   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 282    lr: 4.0960000000000023e-07     evaluation reward: 6.0\n",
      "episode: 3010   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 363    lr: 4.0960000000000023e-07     evaluation reward: 6.01\n",
      "episode: 3011   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 330    lr: 4.0960000000000023e-07     evaluation reward: 6.0\n",
      "episode: 3012   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 309    lr: 4.0960000000000023e-07     evaluation reward: 5.94\n",
      "episode: 3013   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 428    lr: 4.0960000000000023e-07     evaluation reward: 5.96\n",
      "episode: 3014   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 364    lr: 4.0960000000000023e-07     evaluation reward: 5.97\n",
      "episode: 3015   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 259    lr: 4.0960000000000023e-07     evaluation reward: 5.96\n",
      "episode: 3016   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 500    lr: 4.0960000000000023e-07     evaluation reward: 6.0\n",
      "episode: 3017   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 214    lr: 4.0960000000000023e-07     evaluation reward: 5.95\n",
      "episode: 3018   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 471    lr: 4.0960000000000023e-07     evaluation reward: 6.0\n",
      "episode: 3019   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 337    lr: 4.0960000000000023e-07     evaluation reward: 6.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3020   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 362    lr: 4.0960000000000023e-07     evaluation reward: 6.01\n",
      "episode: 3021   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 339    lr: 4.0960000000000023e-07     evaluation reward: 5.98\n",
      "episode: 3022   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 291    lr: 4.0960000000000023e-07     evaluation reward: 5.95\n",
      "episode: 3023   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 401    lr: 4.0960000000000023e-07     evaluation reward: 5.95\n",
      "episode: 3024   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 378    lr: 4.0960000000000023e-07     evaluation reward: 5.95\n",
      "episode: 3025   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 506    lr: 4.0960000000000023e-07     evaluation reward: 5.97\n",
      "episode: 3026   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 409    lr: 4.0960000000000023e-07     evaluation reward: 5.99\n",
      "episode: 3027   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 301    lr: 4.0960000000000023e-07     evaluation reward: 5.98\n",
      "episode: 3028   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 310    lr: 4.0960000000000023e-07     evaluation reward: 5.99\n",
      "episode: 3029   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 259    lr: 4.0960000000000023e-07     evaluation reward: 5.98\n",
      "episode: 3030   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 381    lr: 4.0960000000000023e-07     evaluation reward: 5.97\n",
      "episode: 3031   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 386    lr: 4.0960000000000023e-07     evaluation reward: 5.99\n",
      "episode: 3032   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 279    lr: 4.0960000000000023e-07     evaluation reward: 5.96\n",
      "episode: 3033   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 357    lr: 4.0960000000000023e-07     evaluation reward: 5.96\n",
      "episode: 3034   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 387    lr: 4.0960000000000023e-07     evaluation reward: 5.95\n",
      "episode: 3035   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 260    lr: 4.0960000000000023e-07     evaluation reward: 5.95\n",
      "episode: 3036   score: 3.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 212    lr: 4.0960000000000023e-07     evaluation reward: 5.9\n",
      "episode: 3037   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 378    lr: 4.0960000000000023e-07     evaluation reward: 5.89\n",
      "episode: 3038   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 356    lr: 4.0960000000000023e-07     evaluation reward: 5.86\n",
      "episode: 3039   score: 8.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 453    lr: 4.0960000000000023e-07     evaluation reward: 5.89\n",
      "episode: 3040   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 260    lr: 4.0960000000000023e-07     evaluation reward: 5.89\n",
      "episode: 3041   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 336    lr: 4.0960000000000023e-07     evaluation reward: 5.9\n",
      "episode: 3042   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 257    lr: 4.0960000000000023e-07     evaluation reward: 5.89\n",
      "episode: 3043   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 260    lr: 4.0960000000000023e-07     evaluation reward: 5.87\n",
      "episode: 3044   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 359    lr: 4.0960000000000023e-07     evaluation reward: 5.86\n",
      "episode: 3045   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 339    lr: 4.0960000000000023e-07     evaluation reward: 5.87\n",
      "episode: 3046   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 278    lr: 4.0960000000000023e-07     evaluation reward: 5.84\n",
      "episode: 3047   score: 16.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 598    lr: 4.0960000000000023e-07     evaluation reward: 5.96\n",
      "episode: 3048   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 537    lr: 4.0960000000000023e-07     evaluation reward: 6.01\n",
      "episode: 3049   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 388    lr: 4.0960000000000023e-07     evaluation reward: 5.98\n",
      "episode: 3050   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 349    lr: 4.0960000000000023e-07     evaluation reward: 5.96\n",
      "episode: 3051   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 277    lr: 4.0960000000000023e-07     evaluation reward: 5.92\n",
      "episode: 3052   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 349    lr: 4.0960000000000023e-07     evaluation reward: 5.93\n",
      "episode: 3053   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 384    lr: 4.0960000000000023e-07     evaluation reward: 5.95\n",
      "episode: 3054   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 395    lr: 4.0960000000000023e-07     evaluation reward: 5.98\n",
      "episode: 3055   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 282    lr: 4.0960000000000023e-07     evaluation reward: 5.96\n",
      "episode: 3056   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 428    lr: 4.0960000000000023e-07     evaluation reward: 6.0\n",
      "episode: 3057   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 339    lr: 4.0960000000000023e-07     evaluation reward: 6.01\n",
      "episode: 3058   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 337    lr: 4.0960000000000023e-07     evaluation reward: 5.98\n",
      "episode: 3059   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 387    lr: 4.0960000000000023e-07     evaluation reward: 5.97\n",
      "episode: 3060   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 360    lr: 4.0960000000000023e-07     evaluation reward: 5.97\n",
      "episode: 3061   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 409    lr: 4.0960000000000023e-07     evaluation reward: 6.0\n",
      "episode: 3062   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 336    lr: 4.0960000000000023e-07     evaluation reward: 6.02\n",
      "episode: 3063   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 360    lr: 4.0960000000000023e-07     evaluation reward: 6.02\n",
      "episode: 3064   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 357    lr: 4.0960000000000023e-07     evaluation reward: 6.02\n",
      "episode: 3065   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 282    lr: 4.0960000000000023e-07     evaluation reward: 6.02\n",
      "episode: 3066   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 358    lr: 4.0960000000000023e-07     evaluation reward: 6.03\n",
      "episode: 3067   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 429    lr: 4.0960000000000023e-07     evaluation reward: 6.05\n",
      "episode: 3068   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 329    lr: 4.0960000000000023e-07     evaluation reward: 6.06\n",
      "episode: 3069   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 307    lr: 4.0960000000000023e-07     evaluation reward: 6.08\n",
      "episode: 3070   score: 9.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 466    lr: 4.0960000000000023e-07     evaluation reward: 6.11\n",
      "episode: 3071   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 336    lr: 4.0960000000000023e-07     evaluation reward: 6.13\n",
      "episode: 3072   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 362    lr: 4.0960000000000023e-07     evaluation reward: 6.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3073   score: 11.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 420    lr: 4.0960000000000023e-07     evaluation reward: 6.18\n",
      "episode: 3074   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 326    lr: 4.0960000000000023e-07     evaluation reward: 6.18\n",
      "episode: 3075   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 340    lr: 4.0960000000000023e-07     evaluation reward: 6.18\n",
      "episode: 3076   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 339    lr: 4.0960000000000023e-07     evaluation reward: 6.19\n",
      "episode: 3077   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 355    lr: 4.0960000000000023e-07     evaluation reward: 6.19\n",
      "episode: 3078   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 347    lr: 4.0960000000000023e-07     evaluation reward: 6.21\n",
      "episode: 3079   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 243    lr: 4.0960000000000023e-07     evaluation reward: 6.21\n",
      "episode: 3080   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 304    lr: 4.0960000000000023e-07     evaluation reward: 6.22\n",
      "episode: 3081   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 356    lr: 4.0960000000000023e-07     evaluation reward: 6.24\n",
      "episode: 3082   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 339    lr: 4.0960000000000023e-07     evaluation reward: 6.22\n",
      "episode: 3083   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 330    lr: 4.0960000000000023e-07     evaluation reward: 6.21\n",
      "episode: 3084   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 272    lr: 4.0960000000000023e-07     evaluation reward: 6.2\n",
      "episode: 3085   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 378    lr: 4.0960000000000023e-07     evaluation reward: 6.21\n",
      "episode: 3086   score: 4.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 260    lr: 4.0960000000000023e-07     evaluation reward: 6.18\n",
      "episode: 3087   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 325    lr: 4.0960000000000023e-07     evaluation reward: 6.18\n",
      "episode: 3088   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 377    lr: 4.0960000000000023e-07     evaluation reward: 6.2\n",
      "episode: 3089   score: 5.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 330    lr: 4.0960000000000023e-07     evaluation reward: 6.17\n",
      "episode: 3090   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 367    lr: 4.0960000000000023e-07     evaluation reward: 6.16\n",
      "episode: 3091   score: 10.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 545    lr: 4.0960000000000023e-07     evaluation reward: 6.21\n",
      "episode: 3092   score: 7.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 385    lr: 4.0960000000000023e-07     evaluation reward: 6.23\n",
      "episode: 3093   score: 6.0   memory length: 100000   epsilon: 0.009998020008555413    steps: 360    lr: 4.0960000000000023e-07     evaluation reward: 6.23\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_200/3498000806.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# Start training after random sample generation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mtrain_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_policy_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[1;31m# Update the target network only for Double DQN only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdouble_dqn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mupdate_target_network_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\assignment5\\assignment5_materials\\agent.py\u001b[0m in \u001b[0;36mtrain_policy_net\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;31m### CODE ####\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for e in range(start, EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        \n",
    "        # notes: memory management for preventing overuse of memory\n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "#         agent.memory.half_length()\n",
    "        \n",
    "\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "#                 torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                torch.save(agent.policy_net.state_dict(), f\"./breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(agent.memory)\n",
    "# params = dict()\n",
    "# # params[\"epsilon\"] = agent.epsilon\n",
    "# # params[\"agent_memory\"]= agent.memory\n",
    "# params[\"rewards\"] = rewards \n",
    "# params[\"episodes\"] = episodes\n",
    "# params[\"best_eval_reward\"] = best_eval_reward\n",
    "# joblib.dump(agent,\"prev_agent\")\n",
    "# joblib.dump(params,\"prev_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net.state_dict(), f\"./save_model/breakout_DQN_3093_episodes.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a DQN LSTM Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a DQN agent that uses LSTM rather than past frames as history. We augment the experience replay to contain previous few (state, action, reward, next state) tuples rather than just one (state, action, reward, next state) tuple so it can work with LSTMs. Use the previous tuples to generate the current hidden and context vector for LSTM. \n",
    "Esentially, when you get a sample from replay buffer during training, start with the first tuple and generate hidden and context vector from this and pass it onto the next tuple. Do so consequitively till you reach the last tuple, where you will make Q value predictions.\n",
    "Training loop remains nearly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import LSTM_Agent\n",
    "agent = LSTM_Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0\n",
    "\n",
    "HISTORY_SIZE = 1\n",
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([HISTORY_SIZE + 1, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "    hidden = None\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action, hidden = agent.get_action(np.float32(history[:1, :, :]) / 255., hidden)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[1, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:1, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn_lstm.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net.state_dict(), f\"./save_model/breakout_DQN_900_episodes.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gym.wrappers import Monitor # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "from gym.wrappers import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "# agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
