{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "\n",
    "from src.resnet_yolo import resnet50\n",
    "from yolo_loss import YoloLoss\n",
    "from src.dataset import VocDetectorDataset\n",
    "from src.eval_voc import evaluate\n",
    "from src.predict import predict_image\n",
    "from src.config import VOC_CLASSES, COLORS\n",
    "from kaggle_submission import output_submission_csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOLO network hyperparameters\n",
    "B = 2  # number of bounding box predictions per cell\n",
    "S = 14  # width/height of network output grid (larger than 7x7 from paper since we use a different network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement Yolo we will rely on a pretrained classifier as the backbone for our detection network. PyTorch offers a variety of models which are pretrained on ImageNet in the [`torchvision.models`](https://pytorch.org/docs/stable/torchvision/models.html) package. In particular, we will use the ResNet50 architecture as a base for our detector. This is different from the base architecture in the Yolo paper and also results in a different output grid size (14x14 instead of 7x7).\n",
    "\n",
    "Models are typically pretrained on ImageNet since the dataset is very large (> 1 million images) and widely used. The pretrained model provides a very useful weight initialization for our detector, so that the network is able to learn quickly and effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-trained model\n"
     ]
    }
   ],
   "source": [
    "load_network_path = None #'checkpoints/best_detector.pth' \n",
    "pretrained = True\n",
    "\n",
    "# use to load a previously trained network\n",
    "if load_network_path is not None:\n",
    "    print('Loading saved network from {}'.format(load_network_path))\n",
    "    net = resnet50().to(device)\n",
    "    net.load_state_dict(torch.load(load_network_path))\n",
    "else:\n",
    "    print('Load pre-trained model')\n",
    "    net = resnet50(pretrained=pretrained).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "batch_size = 8\n",
    "\n",
    "# Yolo loss component coefficients (as given in Yolo v1 paper)\n",
    "lambda_coord = 5\n",
    "lambda_noobj = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Pascal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Pascal is a small dataset (5000 in train+val) we have combined the train and val splits to train our detector. This is not typically a good practice, but we will make an exception in this case to be able to get reasonable detection results with a comparatively small object detection dataset.\n",
    "\n",
    "The train dataset loader also using a variety of data augmentation techniques including random shift, scaling, crop, and flips. Data augmentation is slightly more complicated for detection datasets since the bounding box annotations must be kept consistent throughout the transformations.\n",
    "\n",
    "Since the output of the detector network we train is an SxSx(B*5+C), we use an encoder to convert the original bounding box coordinates into relative grid bounding box coordinates corresponding to the expected output. We also use a decoder which allows us to convert the opposite direction into image coordinate bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset\n",
      "Loaded 5011 train images\n"
     ]
    }
   ],
   "source": [
    "file_root_train = 'data/VOCdevkit_2007/VOC2007/JPEGImages/'\n",
    "annotation_file_train = 'data/voc2007.txt'\n",
    "\n",
    "train_dataset = VocDetectorDataset(root_img_dir=file_root_train,dataset_file=annotation_file_train,train=True, S=S)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=2)\n",
    "print('Loaded %d train images' % len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset\n",
      "Loaded 4950 test images\n"
     ]
    }
   ],
   "source": [
    "file_root_test = 'data/VOCdevkit_2007/VOC2007test/JPEGImages/'\n",
    "annotation_file_test = 'data/voc2007test.txt'\n",
    "\n",
    "test_dataset = VocDetectorDataset(root_img_dir=file_root_test,dataset_file=annotation_file_test,train=False, S=S)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=2)\n",
    "print('Loaded %d test images' % len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2].\n",
    "    Args:\n",
    "      box1: (tensor) bounding boxes, sized [N,4].\n",
    "      box2: (tensor) bounding boxes, sized [M,4].\n",
    "    Return:\n",
    "      (tensor) iou, sized [N,M].\n",
    "    \"\"\"\n",
    "    N = box1.size(0)\n",
    "    M = box2.size(0)\n",
    "\n",
    "    lt = torch.max(\n",
    "        box1[:, :2].unsqueeze(1).expand(N, M, 2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "        box2[:, :2].unsqueeze(0).expand(N, M, 2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "    )\n",
    "\n",
    "    rb = torch.min(\n",
    "        box1[:, 2:].unsqueeze(1).expand(N, M, 2),  # [N,2] -> [N,1,2] -> [N,M,2]\n",
    "        box2[:, 2:].unsqueeze(0).expand(N, M, 2),  # [M,2] -> [1,M,2] -> [N,M,2]\n",
    "    )\n",
    "\n",
    "    wh = rb - lt  # [N,M,2]\n",
    "    wh[wh < 0] = 0  # clip at 0\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "    area1 = (box1[:, 2] - box1[:, 0]) * (box1[:, 3] - box1[:, 1])  # [N,]\n",
    "    area2 = (box2[:, 2] - box2[:, 0]) * (box2[:, 3] - box2[:, 1])  # [M,]\n",
    "    area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\n",
    "    area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\n",
    "\n",
    "    iou = inter / (area1 + area2 - inter)\n",
    "    return iou\n",
    "\n",
    "\n",
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S, B, l_coord, l_noobj):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.l_coord = l_coord\n",
    "        self.l_noobj = l_noobj\n",
    "\n",
    "    def xywh2xyxy(self, boxes):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        boxes: (N,4) representing by x,y,w,h\n",
    "\n",
    "        Returns:\n",
    "        boxes: (N,4) representing by x1,y1,x2,y2\n",
    "\n",
    "        if for a Box b the coordinates are represented by [x, y, w, h] then\n",
    "        x1, y1 = x/S - 0.5*w, y/S - 0.5*h ; x2,y2 = x/S + 0.5*w, y/S + 0.5*h\n",
    "        Note: Over here initially x, y are the center of the box and w,h are width and height.\n",
    "        \"\"\"\n",
    "        ### CODE ###\n",
    "        # Your code here\n",
    "        ret = torch.empty_like(boxes)\n",
    "        N = boxes.shape[0]\n",
    "        for i in range(N):\n",
    "            \n",
    "            x1 = boxes[i][...,0]/self.S - 0.5*boxes[i][...,2]\n",
    "            y1 = boxes[i][...,1]/self.S - 0.5*boxes[i][...,3]\n",
    "            x2 = boxes[i][...,0]/self.S + 0.5*boxes[i][...,2]\n",
    "            y2 = boxes[i][...,1]/self.S + 0.5*boxes[i][...,3]\n",
    "            \n",
    "            ret[...,0] = x1\n",
    "            ret[...,1] = y1\n",
    "            ret[...,2] = x2\n",
    "            ret[...,3] = y2\n",
    "            \n",
    "#         print(\"0\",boxes)\n",
    "        return ret\n",
    "\n",
    "    def find_best_iou_boxes(self, pred_box_list, box_target):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        # Typo here\n",
    "        box_pred_list : [(tensor) size (-1, 5) ...]\n",
    "        box_target : (tensor)  size (-1, 4)\n",
    "\n",
    "        Returns:\n",
    "        best_iou: (tensor) size (-1, 1)\n",
    "        best_boxes : (tensor) size (-1, 5), containing the boxes which give the best iou among the two (self.B) predictions\n",
    "\n",
    "        Hints:\n",
    "        1) Find the iou's of each of the 2 bounding boxes of each grid cell of each image.\n",
    "        2) For finding iou's use the compute_iou function\n",
    "        3) use xywh2xyxy to convert bbox format if necessary,\n",
    "        Note: Over here initially x, y are the center of the box and w,h are width and height.\n",
    "        We perform this transformation to convert the correct coordinates into bounding box coordinates.\n",
    "        \"\"\"\n",
    "\n",
    "        ### CODE ###\n",
    "        # Your code here\n",
    "        b0_xyxy = self.xywh2xyxy(pred_box_list[0][..., :4])\n",
    "        b1_xyxy = self.xywh2xyxy(pred_box_list[1][..., :4])\n",
    "        t_xyxy = self.xywh2xyxy(box_target)\n",
    "\n",
    "        b0_iou = torch.diagonal(compute_iou(b0_xyxy, t_xyxy))\n",
    "        b1_iou = torch.diagonal(compute_iou(b1_xyxy, t_xyxy))\n",
    "\n",
    "        mask = b0_iou > b1_iou\n",
    "        best_ious = torch.where(mask, b0_iou, b1_iou)\n",
    "        best_boxes = torch.where(\n",
    "            mask.unsqueeze(-1).expand_as(pred_box_list[0]),\n",
    "            pred_box_list[0],\n",
    "            pred_box_list[1],\n",
    "        )\n",
    "\n",
    "        return best_ious, best_boxes\n",
    "\n",
    "    def get_class_prediction_loss(self, classes_pred, classes_target, has_object_map):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        classes_pred : (tensor) size (batch_size, S, S, 20)\n",
    "        classes_target : (tensor) size (batch_size, S, S, 20)\n",
    "        has_object_map: (tensor) size (batch_size, S, S)\n",
    "\n",
    "        Returns:\n",
    "        class_loss : scalar\n",
    "        \"\"\"\n",
    "        ### CODE ###\n",
    "        # Your code here\n",
    "\n",
    "        loss = torch.sum(\n",
    "            has_object_map\n",
    "            * torch.sum(torch.pow(classes_pred - classes_target, 2), dim=-1)\n",
    "        )\n",
    "\n",
    "        # they do the item for you\n",
    "        return loss# .item()\n",
    "\n",
    "    def get_no_object_loss(self, pred_boxes_list, has_object_map):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        pred_boxes_list: (list) [(tensor) size (N, S, S, 5)  for B pred_boxes]\n",
    "        has_object_map: (tensor) size (N, S, S)\n",
    "\n",
    "        Returns:\n",
    "        loss : scalar\n",
    "\n",
    "        Hints:\n",
    "        1) Only compute loss for cell which doesn't contain object\n",
    "        2) compute loss for all predictions in the pred_boxes_list list\n",
    "        3) You can assume the ground truth confidence of non-object cells is 0\n",
    "        \"\"\"\n",
    "        ### CODE ###\n",
    "        # Your code here\n",
    "        # unroll it by hand\n",
    "        no_object_loss = lambda x: torch.sum(\n",
    "            torch.logical_not(has_object_map) * torch.pow(x, 2)\n",
    "        )\n",
    "        loss = no_object_loss(pred_boxes_list[0][..., -1])\n",
    "        loss += no_object_loss(pred_boxes_list[1][..., -1])\n",
    "\n",
    "        # they do the item for you\n",
    "        return self.l_noobj * loss # .item()\n",
    "\n",
    "    def get_contain_conf_loss(self, box_pred_conf, box_target_conf):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        box_pred_conf : (tensor) size (-1,1)\n",
    "        box_target_conf: (tensor) size (-1,1)\n",
    "\n",
    "        Returns:\n",
    "        contain_loss : scalar\n",
    "\n",
    "        Hints:\n",
    "        The box_target_conf should be treated as ground truth, i.e., no gradient\n",
    "\n",
    "        \"\"\"\n",
    "        ### CODE\n",
    "        # your code here\n",
    "\n",
    "        # already masked here, so only object containing boxes arrive\n",
    "        loss = torch.sum(torch.pow(box_pred_conf - box_target_conf.detach(), 2))\n",
    "\n",
    "        # they do the item for you\n",
    "        return loss # .item()\n",
    "\n",
    "    def get_regression_loss(self, box_pred_response, box_target_response):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        box_pred_response : (tensor) size (-1, 4)\n",
    "        box_target_response : (tensor) size (-1, 4)\n",
    "        Note : -1 corresponds to ravels the tensor into the dimension specified\n",
    "        See : https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view_as\n",
    "\n",
    "        Returns:\n",
    "        reg_loss : scalar\n",
    "\n",
    "        \"\"\"\n",
    "        ### CODE\n",
    "        # your code here\n",
    "\n",
    "        predicted_corners = box_pred_response[..., :2]\n",
    "        target_corners = box_target_response[..., :2]\n",
    "\n",
    "        predicted_sizes = box_pred_response[..., 2:] + 1e-16\n",
    "        target_sizes = box_target_response[..., 2:] + 1e-16\n",
    "\n",
    "        # already masked here, so only object containing boxes arrive\n",
    "        reg_loss = torch.sum(torch.pow(predicted_corners - target_corners, 2))\n",
    "        reg_loss += torch.sum(\n",
    "            torch.pow(torch.sqrt(predicted_sizes) - torch.sqrt(target_sizes), 2)\n",
    "        )\n",
    "\n",
    "        # they do the item for you\n",
    "        return self.l_coord * reg_loss # .item()\n",
    "\n",
    "    def forward(self, pred_tensor, target_boxes, target_cls, has_object_map):\n",
    "        \"\"\"\n",
    "        pred_tensor: (tensor) size(N,S,S,Bx5+20=30) N:batch_size\n",
    "                      where B - number of bounding boxes this grid cell is a part of = 2\n",
    "                            5 - number of bounding box values corresponding to [x, y, w, h, c]\n",
    "                                where x - x_coord, y - y_coord, w - width, h - height, c - confidence of having an object\n",
    "                            20 - number of classes\n",
    "\n",
    "        target_boxes: (tensor) size (N, S, S, 4): the ground truth bounding boxes\n",
    "        target_cls: (tensor) size (N, S, S, 20): the ground truth class\n",
    "        has_object_map: (tensor, bool) size (N, S, S): the ground truth for whether each cell contains an object (True/False)\n",
    "\n",
    "        Returns:\n",
    "        loss_dict (dict): with key value stored for total_loss, reg_loss, containing_obj_loss, no_obj_loss and cls_loss\n",
    "        \"\"\"\n",
    "        N = pred_tensor.size(0)\n",
    "        inv_N = 1.0 / N\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # split the pred tensor from an entity to separate tensors:\n",
    "        # -- pred_boxes_list: a list containing all bbox prediction (list) [(tensor) size (N, S, S, 5)  for B pred_boxes]\n",
    "        # -- pred_cls (containing all classification prediction)\n",
    "        pred_boxes_list = [pred_tensor[..., :5], pred_tensor[..., 5:10]]\n",
    "        # (N, S, S, 20)\n",
    "        pred_cls = pred_tensor[..., 10:]\n",
    "\n",
    "        # compcute classification loss\n",
    "        classification_loss = inv_N * self.get_class_prediction_loss(\n",
    "            pred_cls, target_cls, has_object_map\n",
    "        )\n",
    "\n",
    "        # compute no-object loss\n",
    "        no_object_loss = inv_N * self.get_no_object_loss(\n",
    "            pred_boxes_list, has_object_map\n",
    "        )\n",
    "\n",
    "        # Re-shape boxes in pred_boxes_list and target_boxes to meet the following desires\n",
    "        # 1) only keep having-object cells\n",
    "        # 2) vectorize all dimensions except for the last one for faster computation\n",
    "        # piazza, @503\n",
    "        object_map_mask = has_object_map.reshape(-1)\n",
    "        # (-1, 5)\n",
    "        masked_pred_boxes_list = [\n",
    "            pred_boxes_list[b].reshape(-1, 5)[object_map_mask, :] for b in range(2)\n",
    "        ]\n",
    "        # (-1, 4)\n",
    "        masked_target_boxes = target_boxes.reshape(-1, 4)[object_map_mask, :]\n",
    "\n",
    "        # find the best boxes among the 2 (or self.B) predicted boxes and the corresponding iou\n",
    "        masked_best_ious, masked_best_boxes = self.find_best_iou_boxes(\n",
    "            masked_pred_boxes_list, masked_target_boxes\n",
    "        )\n",
    "\n",
    "        # compute regression loss between the found best bbox and GT bbox for all the cell containing objects\n",
    "        regression_loss = inv_N * self.get_regression_loss(\n",
    "            masked_best_boxes[..., :-1], masked_target_boxes\n",
    "        )\n",
    "\n",
    "        # compute contain_object_loss\n",
    "        # target confidence is IOU of the best box\n",
    "        contain_object_loss = inv_N * self.get_contain_conf_loss(\n",
    "            masked_best_boxes[..., -1], masked_best_ious\n",
    "        )\n",
    "\n",
    "        # compute final loss\n",
    "        total_loss = (\n",
    "            classification_loss + no_object_loss + regression_loss + contain_object_loss\n",
    "        )\n",
    "\n",
    "        # construct return loss_dict\n",
    "        loss_dict = dict(\n",
    "            total_loss=total_loss,\n",
    "            reg_loss=regression_loss,\n",
    "            containing_obj_loss=contain_object_loss,\n",
    "            no_obj_loss=no_object_loss,\n",
    "            cls_loss=classification_loss,\n",
    "        )\n",
    "        return loss_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = YoloLoss(S, B, lambda_coord, lambda_noobj)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting epoch 1 / 50\n",
      "Learning Rate for this epoch: 0.001\n",
      "Epoch [1/50], Iter [50/627], Loss: total=26.255, reg=4.379, containing_obj=0.374, no_obj=12.464, cls=9.038\n",
      "Epoch [1/50], Iter [100/627], Loss: total=18.026, reg=3.933, containing_obj=0.488, no_obj=6.458, cls=7.147\n",
      "Epoch [1/50], Iter [150/627], Loss: total=14.568, reg=3.566, containing_obj=0.507, no_obj=4.410, cls=6.085\n",
      "Epoch [1/50], Iter [200/627], Loss: total=13.003, reg=3.469, containing_obj=0.575, no_obj=3.369, cls=5.590\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26536/1571408739.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_cls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_object_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mloss_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_cls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhas_object_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26536/2762860108.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, pred_tensor, target_boxes, target_cls, has_object_map)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mobject_map_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhas_object_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;31m# (-1, 5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         masked_pred_boxes_list = [\n\u001b[0m\u001b[0;32m    260\u001b[0m             \u001b[0mpred_boxes_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobject_map_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         ]\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26536/2762860108.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;31m# (-1, 5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         masked_pred_boxes_list = [\n\u001b[1;32m--> 260\u001b[1;33m             \u001b[0mpred_boxes_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mobject_map_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m         ]\n\u001b[0;32m    262\u001b[0m         \u001b[1;31m# (-1, 4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_test_loss = np.inf\n",
    "learning_rate = 1e-3\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    \n",
    "    # Update learning rate late in training\n",
    "    if epoch == 30 or epoch == 40:\n",
    "        learning_rate /= 10.0\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "    \n",
    "    print('\\n\\nStarting epoch %d / %d' % (epoch + 1, num_epochs))\n",
    "    print('Learning Rate for this epoch: {}'.format(learning_rate))\n",
    "    \n",
    "    total_loss = collections.defaultdict(int)\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        data = (item.to(device) for item in data)\n",
    "        images, target_boxes, target_cls, has_object_map = data\n",
    "        pred = net(images)\n",
    "        loss_dict = criterion(pred, target_boxes, target_cls, has_object_map)\n",
    "        for key in loss_dict:\n",
    "            total_loss[key] += loss_dict[key].item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_dict['total_loss'].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 50 == 0:\n",
    "            outstring = 'Epoch [%d/%d], Iter [%d/%d], Loss: ' % ((epoch+1, num_epochs, i+1, len(train_loader)))\n",
    "            outstring += ', '.join( \"%s=%.3f\" % (key[:-5], val / (i+1)) for key, val in total_loss.items() )\n",
    "            print(outstring)\n",
    "    \n",
    "    # evaluate the network on the test data\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        test_aps = evaluate(net, test_dataset_file=annotation_file_test, img_root=file_root_test)\n",
    "        print(epoch, test_aps)\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        net.eval()\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = (item.to(device) for item in data)\n",
    "            images, target_boxes, target_cls, has_object_map = data\n",
    "            \n",
    "            pred = net(images)\n",
    "            loss_dict = criterion(pred, target_boxes, target_cls, has_object_map)\n",
    "            test_loss += loss_dict['total_loss'].item()\n",
    "        test_loss /= len(test_loader)\n",
    "    \n",
    "    if best_test_loss > test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        print('Updating best test loss: %.5f' % best_test_loss)\n",
    "        torch.save(net.state_dict(),'checkpoints/best_detector.pth')\n",
    "    \n",
    "    if (epoch+1) in [5, 10, 20, 30, 40]:\n",
    "        torch.save(net.state_dict(),'checkpoints/detector_epoch_%d.pth' % (epoch+1))\n",
    "\n",
    "    torch.save(net.state_dict(),'checkpoints/detector.pth')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),'detector_epoch_5.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "\n",
    "# select random image from test set\n",
    "image_name = random.choice(test_dataset.fnames)\n",
    "image = cv2.imread(os.path.join(file_root_test, image_name))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print('predicting...')\n",
    "result = predict_image(net, image_name, root_img_directory=file_root_test)\n",
    "for left_up, right_bottom, class_name, _, prob in result:\n",
    "    color = COLORS[VOC_CLASSES.index(class_name)]\n",
    "    cv2.rectangle(image, left_up, right_bottom, color, 2)\n",
    "    label = class_name + str(round(prob, 2))\n",
    "    text_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n",
    "    p1 = (left_up[0], left_up[1] - text_size[1])\n",
    "    cv2.rectangle(image, (p1[0] - 2 // 2, p1[1] - 2 - baseline), (p1[0] + text_size[0], p1[1] + text_size[1]),\n",
    "                  color, -1)\n",
    "    cv2.putText(image, label, (p1[0], p1[1] + baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, 8)\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluate on Test\n",
    "\n",
    "To evaluate detection results we use mAP (mean of average precision over each class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate() missing 1 required positional argument: 'img_root'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26536/2602872541.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_aps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mannotation_file_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: evaluate() missing 1 required positional argument: 'img_root'"
     ]
    }
   ],
   "source": [
    "test_aps = evaluate(net, test_dataset_file=annotation_file_test, img_root=file_root_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell added to get intermediate mAP values for students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved network from detector_epoch_5.pth\n",
      "---Evaluate model on test samples---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4950/4950 [02:02<00:00, 40.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---class aeroplane ap 0.0---\n",
      "---class bicycle ap 0.0--- (no predictions for this class)\n",
      "---class bird ap 0.0--- (no predictions for this class)\n",
      "---class boat ap 0.0--- (no predictions for this class)\n",
      "---class bottle ap 0.0--- (no predictions for this class)\n",
      "---class bus ap 0.0--- (no predictions for this class)\n",
      "---class car ap 0.0--- (no predictions for this class)\n",
      "---class cat ap 0.0--- (no predictions for this class)\n",
      "---class chair ap 0.0--- (no predictions for this class)\n",
      "---class cow ap 0.0--- (no predictions for this class)\n",
      "---class diningtable ap 0.0--- (no predictions for this class)\n",
      "---class dog ap 0.0--- (no predictions for this class)\n",
      "---class horse ap 0.0--- (no predictions for this class)\n",
      "---class motorbike ap 0.0--- (no predictions for this class)\n",
      "---class person ap 0.0--- (no predictions for this class)\n",
      "---class pottedplant ap 0.0--- (no predictions for this class)\n",
      "---class sheep ap 0.0--- (no predictions for this class)\n",
      "---class sofa ap 0.0--- (no predictions for this class)\n",
      "---class train ap 0.0--- (no predictions for this class)\n",
      "---class tvmonitor ap 0.0--- (no predictions for this class)\n",
      "---map 0.0---\n",
      "Loading saved network from detector_epoch_10.pth\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'detector_epoch_10.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26536/3162865258.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loading saved network from {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_network_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnet_loaded\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mresnet50\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mnet_loaded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_network_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_loaded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mannotation_file_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_root_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'detector_epoch_10.pth'"
     ]
    }
   ],
   "source": [
    "network_paths = ['detector_epoch_%d.pth' % epoch for epoch in [5, 10, 20, 30, 40]]+['detector.pth']\n",
    "for load_network_path in network_paths:\n",
    "    print('Loading saved network from {}'.format(load_network_path))\n",
    "    net_loaded =  resnet50().to(device)\n",
    "    net_loaded.load_state_dict(torch.load(load_network_path))\n",
    "    evaluate(net_loaded, test_dataset_file=annotation_file_test, img_root=file_root_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('my_new_solution.csv', test_aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
